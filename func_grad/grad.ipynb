{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44be75af",
   "metadata": {},
   "source": [
    "# 生成非线性项梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9236cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# --- 你的原始函数 ---\n",
    "def equation(q: jnp.array, q_t: jnp.array, params: jnp.array):\n",
    "    \"\"\" Mathematical function for lagrangian in a one-dimensional physical system\n",
    "    Args:\n",
    "        q (jnp.array): observation of current generalized coordinate.\n",
    "                         Expected shape (..., 1) or (...,) if already squeezed.\n",
    "        q_t (jnp.array): observation of generalized velocity.\n",
    "                         Expected shape (..., 1) or (...,) if already squeezed.\n",
    "        params (jnp.array): List of numeric constants or parameters. Shape (N,).\n",
    "    Returns:\n",
    "        jnp.array: lagrangian as the result of applying the mathematical function.\n",
    "                   Shape will match the leading dimensions of q/q_t.\n",
    "    \"\"\"\n",
    "    # Ensure we are working with the core dimension\n",
    "    if q.ndim > 0 and q.shape[-1] == 1:\n",
    "        q = q[..., 0]\n",
    "    if q_t.ndim > 0 and q_t.shape[-1] == 1:\n",
    "        q_t = q_t[..., 0]\n",
    "\n",
    "    # Original Calculation (assuming param indices match the description)\n",
    "    # T = p[0]*(1-q_t^2)^(-1) + p[1] + p[2]*q_t^2 - p[4]\n",
    "    # V = -p[3]*q + p[5]\n",
    "    # L = T - V\n",
    "    # It seems the original function might have missed some powers or had typos?\n",
    "    # Let's refine based on common forms or stick strictly to the provided text:\n",
    "    # T = params[0]*jnp.power(1-q_t**2,-1)+params[1] + params[2]*jnp.power(q_t,2) - params[4] # Term -params[4] seems unusual in T, maybe constant shift?\n",
    "    # V = -params[3]*q + params[5] # Term +params[5] seems unusual in V, maybe constant shift?\n",
    "\n",
    "    # Re-interpreting based on potential physics (e.g., relativistic-like kinetic term, linear potential)\n",
    "    # Let's assume a simplified form for clarity, adjust if needed based on the *exact* intended physics.\n",
    "    # Example: T = 0.5 * m * q_t^2 (using params[0] for mass 'm')\n",
    "    # Example: V = 0.5 * k * q^2 (using params[1] for spring constant 'k')\n",
    "    # L = T - V\n",
    "    # --- Using the EXACT provided function for demonstration ---\n",
    "    # Ensure params has enough elements if indices 0-5 are used.\n",
    "    # Add safety checks or padding if params might be shorter.\n",
    "    safe_params = jnp.pad(params, (0, max(0, 6 - len(params)))) # Pad if less than 6 params\n",
    "\n",
    "    term0 = safe_params[0] * jnp.power(1 - q_t**2, -1) # Potential singularity if q_t approaches 1\n",
    "    term1 = safe_params[1]\n",
    "    term2 = safe_params[2] * jnp.power(q_t, 2)\n",
    "    term3 = -safe_params[3] * q\n",
    "    term4 = -safe_params[4] # Constant shift from T\n",
    "    term5 = safe_params[5]  # Constant shift from V\n",
    "\n",
    "    T = term0 + term1 + term2 + term4\n",
    "    V = term3 - term5 # V = -p[3]*q + p[5] => -V = p[3]*q - p[5]\n",
    "\n",
    "    result = T - V # L = (term0 + term1 + term2 + term4) - (term3 - term5)\n",
    "                 # L = term0 + term1 + term2 - term3 + term4 + term5\n",
    "    return result\n",
    "\n",
    "# --- Function to calculate influence ---\n",
    "\n",
    "def calculate_param_influence(q: jnp.array, q_t: jnp.array, params: jnp.array):\n",
    "    \"\"\"\n",
    "    Calculates the influence of each parameter on the equation's output.\n",
    "\n",
    "    Args:\n",
    "        q (jnp.array): observation of current generalized coordinate.\n",
    "        q_t (jnp.array): observation of generalized velocity.\n",
    "        params (jnp.array): The original parameters. Shape (N,).\n",
    "\n",
    "    Returns:\n",
    "        jnp.array: The calculated 'influence' for each parameter.\n",
    "                   Shape (N, ...) matching leading dimensions of q/q_t.\n",
    "    \"\"\"\n",
    "    num_params = params.shape[0]\n",
    "    indices = jnp.arange(num_params)\n",
    "\n",
    "    # Calculate the original output\n",
    "    original_output = equation(q, q_t, params)\n",
    "\n",
    "    # Define a function that calculates the influence for a *single* index 'i'\n",
    "    def influence_for_index(i, q, q_t, params, original_output):\n",
    "        # Create parameters with the i-th element set to 0\n",
    "        modified_params = params.at[i].set(0.0)\n",
    "        # Calculate the output with the modified parameters\n",
    "        modified_output = equation(q, q_t, modified_params)\n",
    "        # Calculate the absolute difference\n",
    "        diff = jnp.abs(original_output - modified_output)\n",
    "        # Get the absolute value of the original parameter\n",
    "        original_param_val = jnp.abs(params[i])\n",
    "        # Calculate influence, handle division by zero if param was originally 0\n",
    "        # If original_param_val is 0, setting it to 0 causes no change, so influence is 0.\n",
    "        influence = jnp.where(original_param_val == 0.0,\n",
    "                              0.0,\n",
    "                              diff / original_param_val)\n",
    "        return influence\n",
    "\n",
    "    # Use vmap to apply 'influence_for_index' across all indices\n",
    "    # Inputs to vmap:\n",
    "    # - The function to vectorize: influence_for_index\n",
    "    # - in_axes: Specifies how arguments map to the vectorized dimension\n",
    "    #   - 0: Vectorize over the first argument (indices)\n",
    "    #   - None: Broadcast the argument (q, q_t, params, original_output don't change per index)\n",
    "    vectorized_influence_calc = jax.vmap(\n",
    "        influence_for_index,\n",
    "        in_axes=(0, None, None, None, None)\n",
    "    )\n",
    "\n",
    "    # Run the vectorized calculation\n",
    "    all_influences = vectorized_influence_calc(indices, q, q_t, params, original_output)\n",
    "\n",
    "    return all_influences\n",
    "\n",
    "# --- Example Usage ---\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Example inputs (adjust shapes as needed, e.g., add a batch dimension)\n",
    "# Assume q and q_t represent observations for a single instance (or averaged)\n",
    "# Add batch dim: q = jax.random.normal(key, (batch_size, 1))\n",
    "q_example = jnp.array([0.5]) # Example coordinate shape (1,) -> becomes scalar in equation()\n",
    "q_t_example = jnp.array([0.1]) # Example velocity shape (1,) -> becomes scalar in equation()\n",
    "\n",
    "# Initial parameters (10 as requested)\n",
    "params_example = jnp.arange(1, 11, dtype=jnp.float32) # Example: [1.0, 2.0, ..., 10.0]\n",
    "# If your equation truly only uses params 0-5, you might adjust this:\n",
    "# params_example = jnp.arange(1, 7, dtype=jnp.float32) # Example: [1.0, ..., 6.0]\n",
    "\n",
    "\n",
    "print(\"Example q:\", q_example)\n",
    "print(\"Example q_t:\", q_t_example)\n",
    "print(\"Example Parameters:\", params_example)\n",
    "\n",
    "# Calculate influences\n",
    "influences = calculate_param_influence(q_example, q_t_example, params_example)\n",
    "\n",
    "print(\"\\nCalculated Influences (per parameter):\")\n",
    "print(influences)\n",
    "\n",
    "# Optional: If q/q_t had batch/time dimensions, you might want to average\n",
    "# influences_mean = influences.mean(axis=tuple(range(1, influences.ndim))) # Average over all non-parameter axes\n",
    "# print(\"\\nMean Influences (averaged over input dimensions):\")\n",
    "# print(influences_mean)\n",
    "\n",
    "# Optional: JIT compile for potential speedup on repeated calls with same shapes\n",
    "jit_calculate_param_influence = jax.jit(calculate_param_influence)\n",
    "\n",
    "print(\"\\nRunning JIT compiled version:\")\n",
    "influences_jit = jit_calculate_param_influence(q_example, q_t_example, params_example)\n",
    "print(influences_jit)\n",
    "\n",
    "# Verify results are the same (within float tolerance)\n",
    "assert jnp.allclose(influences, influences_jit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c5f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np # Still needed for data loading and potentially scipy interaction\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Initialize parameters (as standard Python list or NumPy array is fine here)\n",
    "MAX_NPARAMS = 10\n",
    "# Initial guess for optimization (use NumPy array for scipy)\n",
    "initial_params_np = np.ones(MAX_NPARAMS, dtype=np.float64) * 1.0\n",
    "\n",
    "@jax.jit\n",
    "def equation_jax(q: jnp.array, q_t: jnp.array, params: jnp.array) -> tuple[jnp.array, jnp.array]:\n",
    "    \"\"\"\n",
    "    JAX version: Mathematical function for lagrangian and energy.\n",
    "    Args:\n",
    "        q (jnp.array): Observations of generalized coordinates. Shape (batch, 2).\n",
    "        q_t (jnp.array): Observations of generalized velocities. Shape (batch, 2).\n",
    "        params (jnp.array): List of numeric constants or parameters. Shape (MAX_NPARAMS,).\n",
    "    Returns:\n",
    "        tuple[jnp.array, jnp.array]: Lagrangian (T-V), Energy (T+V). Shape (batch,).\n",
    "    \"\"\"\n",
    "    q1, q2 = q[:, 0], q[:, 1]\n",
    "    q1_t, q2_t = q_t[:, 0], q_t[:, 1]\n",
    "\n",
    "    # Ensure params has enough elements, padding with zero if necessary\n",
    "    safe_params = jnp.pad(params, (0, max(0, MAX_NPARAMS - len(params))))\n",
    "\n",
    "    # Kinetic energy (T)\n",
    "    # Assuming params[0] for q1_t^2 and params[1] for q2_t^2 is more standard\n",
    "    # T = safe_params[0] * q1_t**2 + safe_params[1] * q1_t**2 # Original had typo?\n",
    "    T = safe_params[0] * q1_t**2 + safe_params[1] * q2_t**2\n",
    "    # Note: params[2] is currently unused in this formula.\n",
    "\n",
    "    # Potential energy (V)\n",
    "    V = safe_params[3] * q1 + safe_params[4] * q2 + safe_params[5] *jnp.cos(q1)\n",
    "    # Note: params[5] through params[9] are currently unused.\n",
    "\n",
    "    lagrangian = T - V\n",
    "    energy = T + V\n",
    "    return lagrangian, energy\n",
    "\n",
    "@jax.jit\n",
    "def calculate_loss_jax(params: jnp.array, q: jnp.array, q_t: jnp.array, outputs1: jnp.array, outputs2: jnp.array) -> jnp.array:\n",
    "    \"\"\"Calculates the loss using JAX operations.\"\"\"\n",
    "    y_pred, energy_pred = equation_jax(q, q_t, params)\n",
    "    loss1 = jnp.mean((y_pred - outputs1) ** 2)\n",
    "    loss2 = 2 * jnp.mean((energy_pred - outputs2) ** 2)\n",
    "    total_loss = loss1 + loss2\n",
    "    return total_loss\n",
    "\n",
    "@jax.jit\n",
    "def calculate_param_influence_on_loss(\n",
    "    params: jnp.array, # Typically the *optimized* parameters\n",
    "    q: jnp.array,\n",
    "    q_t: jnp.array,\n",
    "    outputs1: jnp.array,\n",
    "    outputs2: jnp.array\n",
    ") -> jnp.array:\n",
    "    \"\"\"\n",
    "    Calculates the influence of each parameter on the final loss value.\n",
    "\n",
    "    Args:\n",
    "        params (jnp.array): The parameters for which to calculate influence (usually optimized).\n",
    "        q, q_t, outputs1, outputs2: Data arrays.\n",
    "\n",
    "    Returns:\n",
    "        jnp.array: The calculated 'influence' for each parameter. Shape (MAX_NPARAMS,).\n",
    "    \"\"\"\n",
    "    num_params = params.shape[0]\n",
    "    indices = jnp.arange(num_params)\n",
    "\n",
    "    # Calculate the original loss with the given parameters\n",
    "    original_loss = calculate_loss_jax(params, q, q_t, outputs1, outputs2)\n",
    "\n",
    "    # Define a function that calculates the loss difference for a *single* index 'i'\n",
    "    def loss_influence_for_index(i, current_params, q, q_t, outputs1, outputs2, original_loss_val):\n",
    "        # Create parameters with the i-th element set to 0\n",
    "        modified_params = current_params.at[i].set(0.0)\n",
    "        # Calculate the loss with the modified parameters\n",
    "        modified_loss = calculate_loss_jax(modified_params, q, q_t, outputs1, outputs2)\n",
    "        # Calculate the absolute difference in loss\n",
    "        loss_diff = jnp.abs(original_loss_val - modified_loss)\n",
    "        # Get the absolute value of the original parameter\n",
    "        original_param_val = jnp.abs(current_params[i])\n",
    "        # Calculate influence, handle division by zero if param was originally 0\n",
    "        influence = jnp.where(original_param_val == 0.0,\n",
    "                              0.0,\n",
    "                              loss_diff / original_param_val)\n",
    "        return influence\n",
    "\n",
    "    # Use vmap to apply 'loss_influence_for_index' across all indices\n",
    "    vectorized_influence_calc = jax.vmap(\n",
    "        loss_influence_for_index,\n",
    "        in_axes=(0, None, None, None, None, None, None) # i changes, others are constant\n",
    "    )\n",
    "\n",
    "    # Run the vectorized calculation\n",
    "    all_influences = vectorized_influence_calc(indices, params, q, q_t, outputs1, outputs2, original_loss)\n",
    "\n",
    "    return all_influences\n",
    "\n",
    "# Removed the @evaluate.run decorator\n",
    "def evaluate_jax(data: dict) -> tuple[float | None, np.ndarray | None]:\n",
    "    \"\"\" Evaluate the equation, optimize, and calculate parameter influences using JAX.\"\"\"\n",
    "\n",
    "    # Load data observations (as NumPy arrays initially)\n",
    "    inputs_np, outputs_np = data['inputs'], data['outputs']\n",
    "    q_np = inputs_np[:, :2]\n",
    "    q_t_np = inputs_np[:, 2:4]\n",
    "    outputs1_np = outputs_np[:, 0]\n",
    "    outputs2_np = outputs_np[:, 1]\n",
    "\n",
    "    # --- Convert data to JAX arrays for JAX functions ---\n",
    "    q = jnp.asarray(q_np)\n",
    "    q_t = jnp.asarray(q_t_np)\n",
    "    outputs1 = jnp.asarray(outputs1_np)\n",
    "    outputs2 = jnp.asarray(outputs2_np)\n",
    "    # ---\n",
    "\n",
    "    # Define the loss function wrapper for scipy.optimize.minimize\n",
    "    # It takes NumPy array (from scipy), converts to JAX, calls JAX loss, returns float\n",
    "    def loss_for_scipy(params_np):\n",
    "        params_jax = jnp.asarray(params_np)\n",
    "        loss_val = calculate_loss_jax(params_jax, q, q_t, outputs1, outputs2)\n",
    "        return float(loss_val) # Return standard float for scipy\n",
    "\n",
    "    # Optimize parameters using scipy.optimize.minimize\n",
    "    # We still use the NumPy-based initial guess\n",
    "    result = minimize(loss_for_scipy, initial_params_np, method='BFGS')\n",
    "\n",
    "    optimized_params_np = result.x\n",
    "    final_loss = result.fun # This is the scalar loss value\n",
    "\n",
    "    print('Optimized parameters (NumPy array):', optimized_params_np)\n",
    "    print('Final loss:', final_loss)\n",
    "\n",
    "    if np.isnan(final_loss) or np.isinf(final_loss):\n",
    "        return None, None\n",
    "\n",
    "    # --- Calculate Parameter Influences on the Final Loss ---\n",
    "    # Convert optimized params to JAX array for the influence calculation\n",
    "    optimized_params_jax = jnp.asarray(optimized_params_np)\n",
    "\n",
    "    # Calculate influences using the JAX function\n",
    "    param_influences_jax = calculate_param_influence_on_loss(\n",
    "        optimized_params_jax, q, q_t, outputs1, outputs2\n",
    "    )\n",
    "\n",
    "    # Convert influences back to NumPy array for return, if desired, or keep as JAX array\n",
    "    param_influences_np = np.asarray(param_influences_jax)\n",
    "    print('Parameter influences on loss:', param_influences_np)\n",
    "    # ---\n",
    "\n",
    "    # Return evaluation score (negative loss) and parameter influences\n",
    "    return -final_loss, param_influences_np\n",
    "\n",
    "# --- Example Usage (requires a data dictionary) ---\n",
    "if __name__ == '__main__':\n",
    "    # Create some dummy data for demonstration\n",
    "    import pandas as pd\n",
    "    # 读取 CSV 文件并转换为 NumPy 数组\n",
    "    data0 = pd.read_csv('./train.csv')#\n",
    "    tae = data0.to_numpy()\n",
    "\n",
    "    # 使用 JAX 的数组操作替换 PyTorch 的操作\n",
    "    state = jnp.array(tae[:, :4], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "    true_q_ddot = jnp.array(tae[:, 4:-1], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "    energy = jnp.array(tae[:, -1], dtype=jnp.float64)\n",
    "    print(true_q_ddot)\n",
    "    # 将数据存储在字典中\n",
    "    data = {\n",
    "        'inputs': state,\n",
    "        'outputs': true_q_ddot,  # 真实的加速度\n",
    "        'energy': energy\n",
    "    }\n",
    "\n",
    "\n",
    "    print(\"--- Running JAX Evaluation ---\")\n",
    "    neg_loss, influences = evaluate_jax(data)\n",
    "\n",
    "    if neg_loss is not None:\n",
    "        print(\"\\n--- Results ---\")\n",
    "        print(f\"Negative Loss (Score): {neg_loss}\")\n",
    "        print(f\"Parameter Influences: {influences}\")\n",
    "\n",
    "        # You can now analyze the 'influences' array. Higher absolute values\n",
    "        # indicate parameters that, when set to zero, caused a larger change\n",
    "        # in the optimized loss (normalized by the parameter's optimized value).\n",
    "        # Parameters with very low influence might be candidates for removal.\n",
    "        # Also note which parameters were unused in the equation (e.g., index 2, 5-9).\n",
    "        # Their influence should ideally be zero or very close to it due to floating point noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5296c9ad",
   "metadata": {},
   "source": [
    "# 为简单起见，我们先检验使用拉格朗日量和能量作为损失的框架来测试效果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a8c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config, jit, vmap\n",
    "from jax.scipy.optimize import minimize\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def evaluate(data: dict) -> float:\n",
    "    \"\"\" Evaluate the equation on data observations.\"\"\"\n",
    "\n",
    "    # Load data observations\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    q = inputs[:,:2]\n",
    "    q_t = inputs[:,2:4]\n",
    "    outputs1 = outputs[:,0]\n",
    "    outputs2 = outputs[:,1]\n",
    "\n",
    "    def loss_fn(params):\n",
    "        y_pred, energy = equation(q, q_t, params)\n",
    "        # 使用 jnp.mean 计算损失\n",
    "        return jnp.mean((y_pred - outputs1) ** 2) + 2*jnp.mean((energy - outputs2) ** 2)\n",
    "\n",
    "    # 使用 loss_fn 定义 loss_partial\n",
    "    loss_partial = lambda params: loss_fn(params)\n",
    "    loss_partial = jit(loss_fn)  # 确保JIT编译\n",
    "    # 使用 initial_params 初始化优化参数\n",
    "    optimized_params = initial_params\n",
    "    result = minimize(loss_partial, optimized_params, method='BFGS', options={'maxiter': 1000})\n",
    "    # 从优化结果中获取优化后的参数和损失\n",
    "    optimized_params = result.x\n",
    "    loss = result.fun\n",
    "\n",
    "    print('optimized_params:',optimized_params)\n",
    "\n",
    "    # 使用 jnp 检查损失值是否有效\n",
    "    if jnp.isnan(loss) or jnp.isinf(loss):\n",
    "        return None\n",
    "    else:\n",
    "        return -loss\n",
    "\n",
    "\n",
    "def equation(q: jnp.array, q_t: jnp.array, params: jnp.array) -> jnp.array:\n",
    "    \"\"\" Mathematical function for lagrangian in a physical system,which is a conservative system.\n",
    "    Args:\n",
    "        q (jnp.array): observations of current generalized coordinate,which dimension is rad.\n",
    "        q_t (jnp.array): observations of generalized velocity.\n",
    "        params (jnp.array): List of numeric constants or parameters to be optimized.\n",
    "    Returns:\n",
    "        jnp.array: lagrangian (T-V) and total energy (T+V).\n",
    "    \"\"\"\n",
    "\n",
    "    q1, q2 = q[:,0],q[:,1]\n",
    "    q1_t, q2_t = q_t[:,0],q_t[:,1]\n",
    "\n",
    "    # Kinetic energy (T)\n",
    "    T =  params[0]*q1_t**2 + params[1]*q2_t**2\n",
    "    # potential energy (V)\n",
    "    V =  params[3]*q1 +params[4]*q2 + params[5]*jnp.cos(q1) +  params[6]*jnp.cos(q2) + params[7]*jnp.sin(q1) #+ params[8]*jnp.sin(q2) \n",
    "    return T - V, T + V\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a8cff33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.80206776e-15 -1.80206776e-15]\n",
      " [ 7.70196859e-04 -1.14977777e-11]\n",
      " [ 3.08078740e-03 -1.21308289e-11]\n",
      " ...\n",
      " [ 4.98795246e+01 -2.30539589e-06]\n",
      " [ 4.99611381e+01 -2.79509812e-06]\n",
      " [ 5.00323659e+01 -3.35014171e-06]]\n",
      "[[-9.81000000e+00  0.00000000e+00]\n",
      " [-9.80999999e+00 -2.26759624e-08]\n",
      " [-9.80999979e+00 -3.62815393e-07]\n",
      " ...\n",
      " [ 1.96515787e+01 -1.87375710e+01]\n",
      " [ 1.54711058e+01 -1.33573195e+01]\n",
      " [ 1.11144949e+01 -7.73028138e+00]]\n",
      "优化参数：[  0.61962555   1.          -1.74516287  -0.99169285 -16.06962547\n",
      "  -9.46751544  -0.03475219   4.1065319    1.        ]\n",
      "优化参数：[  1.20214258   1.          -1.07756867  -0.30950188 -16.72852577\n",
      "  -7.17620188  -0.08846496   2.146836     1.        ]\n",
      "优化参数：[  0.92578317   0.51176874  -2.27120662  -1.05742179 -18.3589969\n",
      "  -9.44787378  -0.09683702   5.04498894   1.        ]\n",
      "优化参数：[  0.92532286   0.51136107   1.          -0.84686675 -18.43831607\n",
      "  -9.37900276  -0.32259573   1.81515464   1.        ]\n",
      "优化参数：[  0.92554156   0.51069574   1.          -1.38673051 -18.96281453\n",
      "  -8.68037638  -1.42744993   3.48446455   1.        ]\n",
      "优化参数：[  0.78565836   0.47282078   1.          -7.81896388 -11.1112449\n",
      " -22.62935592  12.38896429  15.66038198   1.        ]\n",
      "优化参数：[  0.92622959   0.49161958   1.          -0.48260645   3.6876375\n",
      " -23.25344589  -6.77325932   2.07725234   1.        ]\n",
      "优化参数：[  0.92580771   0.51175198   1.          -2.2871857   -1.09922968\n",
      " -18.33636459  -9.48344011   5.04159073   1.        ]\n",
      "优化参数：[  0.92495046   0.51076506   1.           1.00389194  -0.68072443\n",
      " -18.51289605  -9.33199935  -0.24012      1.        ]\n",
      "优化参数：[  0.92578317   0.51176874   1.          -2.27120662  -1.05742179\n",
      " -18.3589969   -9.44787378  -0.09683702   5.04498894]\n",
      "{'optimized_params': Array([  0.92578317,   0.51176874,   1.        ,  -2.27120662,\n",
      "        -1.05742179, -18.3589969 ,  -9.44787378,  -0.09683702,\n",
      "         5.04498894,   1.        ], dtype=float64), 'final_loss': Array(105.6324943, dtype=float64), 'negative_loss': Array(-105.6324943, dtype=float64), 'sensitivities': {0: Array(1.84394218, dtype=float64), 1: Array(2.61179105, dtype=float64), 2: Array(0., dtype=float64), 3: Array(0.00395862, dtype=float64), 4: Array(0.0077421, dtype=float64), 5: Array(2.23812998, dtype=float64), 6: Array(0.4276709, dtype=float64), 7: Array(2.36861228e-05, dtype=float64), 8: Array(0.00891643, dtype=float64), 9: Array(0., dtype=float64)}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# 读取 CSV 文件并转换为 NumPy 数组\n",
    "data0 = pd.read_csv('./train.csv')#\n",
    "tae = data0.to_numpy()\n",
    "\n",
    "# 使用 JAX 的数组操作替换 PyTorch 的操作\n",
    "state = jnp.array(tae[:, :4], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "true_q_ddot = jnp.array(tae[:, 4:6], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "energy = jnp.array(tae[:, 6:], dtype=jnp.float64)\n",
    "print(energy)\n",
    "print(true_q_ddot)\n",
    "# 将数据存储在字典中\n",
    "data = {\n",
    "    'inputs': state,\n",
    "    'outputs': energy\n",
    "}\n",
    "\n",
    "\n",
    "f = evaluate(data)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7de2de4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4352e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config, jit, vmap\n",
    "from jax.scipy.optimize import minimize\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "def evaluate(data: dict, initial_params_override: jnp.array = None) -> dict | None:\n",
    "    \"\"\"\n",
    "    优化参数，评估损失，并计算所有参数的敏感性。\n",
    "    将损失函数和敏感性分析逻辑封装在此函数内部。\n",
    "\n",
    "    Args:\n",
    "        data (dict): 包含 'inputs' 和 'outputs' 的数据字典。\n",
    "                     outputs 预期有两列：目标 T-V 和 目标 T+V。\n",
    "        initial_params_override (jnp.array, optional): 用于覆盖全局 initial_params 的初始参数。\n",
    "\n",
    "    Returns:\n",
    "        dict | None: 如果成功，返回包含以下键的字典：\n",
    "                     'optimized_params': 优化后的参数数组。\n",
    "                     'final_loss': 优化后的标量损失值。\n",
    "                     'negative_loss': -final_loss (用于与旧版输出比较)。\n",
    "                     'sensitivities': 包含所有参数索引到其敏感性得分的字典。\n",
    "                     如果优化失败或损失无效，则返回 None。\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 损失函数定义 (嵌套在 evaluate 内部) ---\n",
    "    @jit\n",
    "    def loss_fn_internal(params: jnp.array, current_data: dict) -> float:\n",
    "        \"\"\" 计算给定参数和数据的损失 (evaluate 内部版本)。\"\"\"\n",
    "        inputs, outputs = current_data['inputs'], current_data['outputs']\n",
    "        q = inputs[:,:2]\n",
    "        q_t = inputs[:,2:4]\n",
    "        outputs_lagrangian = outputs[:, 0] # 对应 T-V\n",
    "        outputs_energy = outputs[:, 1]     # 对应 T+V\n",
    "        y_pred, energy_pred = equation(q, q_t, params)\n",
    "        loss_lagrangian = jnp.mean((y_pred - outputs_lagrangian) ** 2)\n",
    "        loss_energy = 2 * jnp.mean((energy_pred - outputs_energy) ** 2)\n",
    "        return loss_lagrangian + loss_energy\n",
    "\n",
    "    # --- 内部辅助函数：敏感性计算 (嵌套在 evaluate 内部) ---\n",
    "    def _calculate_sensitivity_internal(optimized_params: jnp.array, original_loss: float, current_data: dict, param_indices_to_test: list[int]) -> dict:\n",
    "        \"\"\" 内部辅助函数：计算损失对每个指定参数的敏感性。\"\"\"\n",
    "        sensitivities = {}\n",
    "        print(\"\\n--- (内部) 开始敏感性分析 ---\")\n",
    "        print(f\"用于分析的原始损失: {original_loss:.6f}\")\n",
    "\n",
    "        for i in param_indices_to_test:\n",
    "            original_param_value = optimized_params[i]\n",
    "            print(f\"\\n测试参数索引 {i} (原始值: {original_param_value:.6f})\")\n",
    "\n",
    "            # 创建一个屏蔽函数，强制将特定参数保持为0\n",
    "            def masked_loss_fn(params_subset, data, mask_idx):\n",
    "                # 创建完整参数集，但将被屏蔽的参数设为0\n",
    "                full_params = jnp.zeros_like(optimized_params)\n",
    "                \n",
    "                # 将子集参数放入正确的位置，跳过被屏蔽的参数\n",
    "                idx = 0\n",
    "                for j in range(len(optimized_params)):\n",
    "                    if j != mask_idx:\n",
    "                        full_params = full_params.at[j].set(params_subset[idx])\n",
    "                        idx += 1\n",
    "                \n",
    "                return loss_fn_internal(full_params, data)\n",
    "\n",
    "            # 创建不包含被测试参数的初始参数子集\n",
    "            initial_params_subset = jnp.array([optimized_params[j] for j in range(len(optimized_params)) if j != i])\n",
    "            \n",
    "            # 为当前屏蔽的参数创建部分损失函数\n",
    "            loss_partial_masked = jit(lambda p: masked_loss_fn(p, current_data, i))\n",
    "            \n",
    "            print(f\"开始重新优化（忽略参数 {i}）...\")\n",
    "            try:\n",
    "                # 尝试不同的优化方法（如果BFGS有问题）\n",
    "                result_masked = minimize(loss_partial_masked,initial_params_subset,method='BFGS',options={'maxiter': 500})\n",
    "\n",
    "                # 打印更详细的优化结果信息\n",
    "                print(f\"优化状态: {'成功' if getattr(result_masked, 'success', False) else '未成功'}\")\n",
    "                if hasattr(result_masked, 'status'):\n",
    "                    print(f\"状态码: {result_masked.status}\")\n",
    "                if hasattr(result_masked, 'message'):\n",
    "                    print(f\"消息: {result_masked.message}\")\n",
    "                if hasattr(result_masked, 'nfev'):\n",
    "                    print(f\"函数评估次数: {result_masked.nfev}\")\n",
    "                if hasattr(result_masked, 'nit'):\n",
    "                    print(f\"迭代次数: {result_masked.nit}\")\n",
    "                \n",
    "                # 构建完整的参数集（屏蔽的参数为0）\n",
    "                optimized_params_with_mask = jnp.zeros_like(optimized_params)\n",
    "                idx = 0\n",
    "                for j in range(len(optimized_params)):\n",
    "                    if j != i:\n",
    "                        optimized_params_with_mask = optimized_params_with_mask.at[j].set(result_masked.x[idx])\n",
    "                        idx += 1\n",
    "                \n",
    "                # 计算新的最优损失\n",
    "                modified_loss = loss_fn_internal(optimized_params_with_mask, current_data)\n",
    "                \n",
    "                # 验证损失是否有效\n",
    "                if jnp.isnan(modified_loss) or jnp.isinf(modified_loss):\n",
    "                    print(f\"警告: 参数 {i} 的重新优化结果包含无效损失值 (NaN 或 Inf)\")\n",
    "                    sensitivities[i] = float('nan')  # 标记为无效\n",
    "                else:\n",
    "                    print(f\"参数 {i} 忽略后重新优化的损失: {modified_loss:.6f}\")\n",
    "                    \n",
    "                    loss_difference = modified_loss - original_loss\n",
    "                    sensitivity = loss_difference / original_loss\n",
    "                    sensitivities[i] = sensitivity\n",
    "                    \n",
    "                    print(f\"损失差异: {loss_difference:.6f}\")\n",
    "                    print(f\"敏感性得分: {sensitivity:.6f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"对参数 {i} 进行重新优化时发生错误: {e}\")\n",
    "                sensitivities[i] = float('nan')  # 标记失败的敏感性计算\n",
    "        \n",
    "        return sensitivities\n",
    "\n",
    "    # --- evaluate 函数主体逻辑 ---\n",
    "    # ... (确定 current_initial_params 的逻辑不变) ...\n",
    "    current_initial_params = initial_params_override if initial_params_override is not None else initial_params\n",
    "    print(f\"使用的初始参数: {current_initial_params}\")\n",
    "\n",
    "    # 1. 参数优化\n",
    "    loss_partial = jit(lambda p: loss_fn_internal(p, data))\n",
    "    print(\"开始优化...\")\n",
    "    try:\n",
    "        result = minimize(loss_partial, current_initial_params, method='BFGS', options={'maxiter': 500})\n",
    "    except Exception as e:\n",
    "        print(f\"优化过程中发生错误: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not result.success:\n",
    "        print(f\"优化失败: {result.message}\")\n",
    "        # pass # 允许继续，但结果可能不可靠\n",
    "\n",
    "    optimized_params = result.x\n",
    "    final_loss = loss_fn_internal(optimized_params, data)\n",
    "\n",
    "    print('优化后的参数:', optimized_params)\n",
    "    # print('最终损失 (来自优化器):', result.fun) # 可以取消注释以进行比较\n",
    "    print('最终损失 (重新计算):', final_loss)\n",
    "\n",
    "    if jnp.isnan(final_loss) or jnp.isinf(final_loss):\n",
    "        print(\"警告: 优化结果包含无效 (NaN 或 Inf) 的损失值。\")\n",
    "        return None\n",
    "\n",
    "    # 2. 敏感性分析 (如果优化成功且损失有效)\n",
    "    # ***更正***: 定义要测试的所有参数索引\n",
    "    # 使用 range(len(optimized_params)) 来获取从 0 到 n-1 的所有索引\n",
    "    all_param_indices = list(range(len(optimized_params)))\n",
    "\n",
    "    print(f\"\\n将对以下参数索引进行敏感性分析: {all_param_indices}\")\n",
    "\n",
    "    if not all_param_indices: # 检查列表是否为空 (虽然不太可能，除非优化参数为空)\n",
    "         print(\"警告：没有有效的参数索引用于敏感性分析。\")\n",
    "         sensitivities = {}\n",
    "    else:\n",
    "        # 调用内部定义的敏感性分析函数，传入所有索引\n",
    "        sensitivities = _calculate_sensitivity_internal(\n",
    "            optimized_params,\n",
    "            final_loss,\n",
    "            data,\n",
    "            all_param_indices # <--- 使用包含所有索引的列表\n",
    "        )\n",
    "\n",
    "    # 3. 返回结果\n",
    "    return {\n",
    "        'optimized_params': optimized_params,\n",
    "        'final_loss': final_loss,\n",
    "        'negative_loss': -final_loss,\n",
    "        'sensitivities': sensitivities # 现在包含所有测试过的参数的敏感性\n",
    "    }\n",
    "\n",
    "def equation(q: jnp.array, q_t: jnp.array, params: jnp.array) -> jnp.array:\n",
    "    \"\"\" Mathematical function for lagrangian in a physical system,which is a conservative system.\n",
    "    Args:\n",
    "        q (jnp.array): observations of current generalized coordinate,which dimension is rad.\n",
    "        q_t (jnp.array): observations of generalized velocity.\n",
    "        params (jnp.array): List of numeric constants or parameters to be optimized.\n",
    "    Returns:\n",
    "        jnp.array: lagrangian (T-V) and total energy (T+V).\n",
    "    \"\"\"\n",
    "\n",
    "    q1, q2 = q[:,0],q[:,1]\n",
    "    q1_t, q2_t = q_t[:,0],q_t[:,1]\n",
    "\n",
    "    # Kinetic energy (T)\n",
    "    T =  params[0]*q1_t**2 + params[1]*q2_t**2\n",
    "    # potential energy (V)\n",
    "    V = params[3]*q1 + params[4]*q2 + params[5]*jnp.cos(q1) +   params[6]*jnp.cos(q2) + params[8]*jnp.sin(q1) + params[7]*jnp.sin(q2) \n",
    "    return T - V, T + V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f083239",
   "metadata": {},
   "source": [
    "### 循环计算每一个参数的伪梯度，效果良好，最终优化的代码，最简洁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fc546c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config, jit, vmap\n",
    "from jax.scipy.optimize import minimize\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def evaluate(data: dict, initial_params_override: jnp.array = None) -> dict | None:\n",
    "    @jit\n",
    "    def loss_fn_internal(params: jnp.array, current_data: dict) -> float:\n",
    "        inputs, outputs = current_data['inputs'], current_data['outputs']\n",
    "        q = inputs[:,:2]\n",
    "        q_t = inputs[:,2:4]\n",
    "        outputs_lagrangian = outputs[:, 0]\n",
    "        outputs_energy = outputs[:, 1]\n",
    "        y_pred, energy_pred = equation(q, q_t, params)\n",
    "        loss_lagrangian = jnp.mean((y_pred - outputs_lagrangian) ** 2)\n",
    "        loss_energy = 2 * jnp.mean((energy_pred - outputs_energy) ** 2)\n",
    "        return loss_lagrangian + loss_energy\n",
    "\n",
    "    def _calculate_sensitivity_internal(optimized_params: jnp.array, original_loss: float, \n",
    "                                   current_data: dict, param_indices_to_test: list[int]) -> dict:\n",
    "        sensitivities = {}\n",
    "        \n",
    "        for i in param_indices_to_test:\n",
    "            mask = jnp.ones_like(optimized_params, dtype=bool)\n",
    "            mask = mask.at[i].set(False)\n",
    "            initial_params_subset = optimized_params[mask]\n",
    "    \n",
    "            @jit\n",
    "            def masked_loss_fn(params_subset):\n",
    "                full_params = jnp.zeros_like(optimized_params)\n",
    "                full_params = full_params.at[mask].set(params_subset)\n",
    "                return loss_fn_internal(full_params, current_data)\n",
    "            \n",
    "            try:\n",
    "                result_masked = minimize(masked_loss_fn, initial_params_subset, method='BFGS', options={'maxiter': 500})\n",
    "                print(f\"优化参数：{result_masked.x}\")\n",
    "                optimized_params_with_mask = jnp.zeros_like(optimized_params)\n",
    "                optimized_params_with_mask = optimized_params_with_mask.at[mask].set(result_masked.x)\n",
    "                \n",
    "                modified_loss = loss_fn_internal(optimized_params_with_mask, current_data)\n",
    "                \n",
    "                sensitivities[i] = jnp.where(\n",
    "                    jnp.isnan(modified_loss) | jnp.isinf(modified_loss),\n",
    "                    jnp.nan,\n",
    "                    (modified_loss - original_loss) / original_loss\n",
    "                )\n",
    "            except Exception as e:\n",
    "                sensitivities[i] = float('nan')\n",
    "        \n",
    "        return sensitivities\n",
    "\n",
    "    current_initial_params = initial_params_override if initial_params_override is not None else initial_params\n",
    "\n",
    "    loss_partial = jit(lambda p: loss_fn_internal(p, data))\n",
    "    try:\n",
    "        result = minimize(loss_partial, current_initial_params, method='BFGS', options={'maxiter': 500})\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "    optimized_params = result.x\n",
    "    final_loss = loss_fn_internal(optimized_params, data)\n",
    "\n",
    "    if jnp.isnan(final_loss) or jnp.isinf(final_loss):\n",
    "        return None\n",
    "\n",
    "    all_param_indices = list(range(len(optimized_params)))\n",
    "    sensitivities = _calculate_sensitivity_internal(optimized_params, final_loss, data, all_param_indices)\n",
    "\n",
    "    return {\n",
    "        'optimized_params': optimized_params,\n",
    "        'final_loss': final_loss,\n",
    "        'negative_loss': -final_loss,\n",
    "        'sensitivities': sensitivities\n",
    "    }\n",
    "\n",
    "\n",
    "def equation(q: jnp.array, q_t: jnp.array, params: jnp.array) -> jnp.array:\n",
    "    \"\"\" Mathematical function for lagrangian in a physical system,which is a conservative system.\n",
    "    Args:\n",
    "        q (jnp.array): observations of current generalized coordinate,which dimension is rad.\n",
    "        q_t (jnp.array): observations of generalized velocity.\n",
    "        params (jnp.array): List of numeric constants or parameters to be optimized.\n",
    "    Returns:\n",
    "        jnp.array: lagrangian (T-V) and total energy (T+V).\n",
    "    \"\"\"\n",
    "\n",
    "    q1, q2 = q[:,0],q[:,1]\n",
    "    q1_t, q2_t = q_t[:,0],q_t[:,1]\n",
    "\n",
    "    # Kinetic energy (T)\n",
    "    T =  params[0]*q1_t**2 + params[1]*q2_t**2\n",
    "    # potential energy (V)\n",
    "    V = params[3]*q1 + params[4]*q2 + params[5]*jnp.cos(q1) +   params[6]*jnp.cos(q2) + params[8]*jnp.sin(q1) + params[7]*jnp.sin(q2) \n",
    "    return T - V, T + V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee9064",
   "metadata": {},
   "source": [
    "# 以上代码已经能够运行，我们尝试能否优化代码，加速计算.\n",
    "优化前：12.4s，10.0S   \n",
    "对应的参数的敏感度：{0: Array(1.84394218, dtype=float64), 1: Array(2.61179105, dtype=float64), 2: Array(0., dtype=float64), 3: Array(0.00395862, dtype=float64), 4: Array(0.0077421, dtype=float64), 5: Array(2.23812998, dtype=float64), 6: Array(0.4276709, dtype=float64), 7: Array(2.36861228e-05, dtype=float64), 8: Array(0.00891643, dtype=float64), 9: Array(0., dtype=float64)}   \n",
    "优化后：2.3s   \n",
    "对应参数的敏感度：   \n",
    "'sensitivities': {0: 1.8530223881547223, 1: 2.627426792901825, 2: -1.6278262024711505e-14, 3: 0.2583657524769522, 4: 0.07432524506118177, 5: 2.6259165000759235, 6: 0.5372272257631501, 7: 5.758876017124561e-05, 8: 0.029088609674932472, 9: -1.6278262024711505e-14}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a64ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config, jit, vmap, grad\n",
    "from jax.scipy.optimize import minimize\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def evaluate(data: dict, initial_params_override: jnp.array = None) -> dict | None:\n",
    "    @jit\n",
    "    def loss_fn_internal(params: jnp.array, current_data: dict) -> float:\n",
    "        inputs, outputs = current_data['inputs'], current_data['outputs']\n",
    "        q = inputs[:,:2]\n",
    "        q_t = inputs[:,2:4]\n",
    "        outputs_lagrangian = outputs[:, 0]\n",
    "        outputs_energy = outputs[:, 1]\n",
    "        y_pred, energy_pred = equation(q, q_t, params)\n",
    "        loss_lagrangian = jnp.mean((y_pred - outputs_lagrangian) ** 2)\n",
    "        loss_energy = 2 * jnp.mean((energy_pred - outputs_energy) ** 2)\n",
    "        return loss_lagrangian + loss_energy\n",
    "    \n",
    "    def calculate_masked_sensitivities_batch(optimized_params, original_loss, current_data):\n",
    "        # 创建掩码参数矩阵 - 每行代表一组去掉一个参数的配置\n",
    "        # 初始化为与optimized_params相同的值\n",
    "        masked_params_matrix = jnp.tile(optimized_params, (MAX_NPARAMS, 1))\n",
    "        \n",
    "        # 对每行设置对应位置的参数为0（即掩码）\n",
    "        for i in range(MAX_NPARAMS):\n",
    "            masked_params_matrix = masked_params_matrix.at[i, i].set(0.0)\n",
    "        \n",
    "        # 定义批量损失函数 - 计算每个掩码配置的损失\n",
    "        @jit\n",
    "        def batch_masked_loss_fn(params_matrix):\n",
    "            # 计算每行参数配置的损失\n",
    "            def compute_loss_for_row(row_params):\n",
    "                return loss_fn_internal(row_params, current_data)\n",
    "            \n",
    "            # 使用vmap并行计算所有行的损失\n",
    "            losses = jax.vmap(compute_loss_for_row)(params_matrix)\n",
    "            \n",
    "            # 返回所有损失的总和 - 这是我们要优化的目标\n",
    "            return jnp.sum(losses)\n",
    "        \n",
    "        # 对batch_masked_loss_fn进行优化\n",
    "        # 注意：我们需要定义一个函数来维持特定位置的掩码（即保持对应位置为0）\n",
    "        def optimize_with_masks():\n",
    "            # 创建初始掩码矩阵的平坦版本（展平为一维数组）\n",
    "            initial_flat = masked_params_matrix.flatten()\n",
    "            \n",
    "            # 定义优化函数，保持掩码位置为0\n",
    "            def masked_opt_fn(flat_params):\n",
    "                # 重塑为矩阵\n",
    "                reshaped = flat_params.reshape(MAX_NPARAMS, MAX_NPARAMS)\n",
    "                # 确保掩码位置为0\n",
    "                for i in range(MAX_NPARAMS):\n",
    "                    reshaped = reshaped.at[i, i].set(0.0)\n",
    "                # 计算损失\n",
    "                return batch_masked_loss_fn(reshaped)\n",
    "            \n",
    "            # 执行优化\n",
    "            try:\n",
    "                result = minimize(masked_opt_fn, initial_flat, method='BFGS', options={'maxiter': 500})\n",
    "                # 将结果重塑为矩阵，并确保掩码位置为0\n",
    "                optimized_matrix = result.x.reshape(MAX_NPARAMS, MAX_NPARAMS)\n",
    "                for i in range(MAX_NPARAMS):\n",
    "                    optimized_matrix = optimized_matrix.at[i, i].set(0.0)\n",
    "                return optimized_matrix\n",
    "            except Exception as e:\n",
    "                print(f\"Optimization error: {str(e)}\")\n",
    "                return masked_params_matrix  # 返回初始矩阵作为后备\n",
    "        \n",
    "        # 执行优化并获取优化后的掩码参数矩阵\n",
    "        optimized_matrix = optimize_with_masks()\n",
    "        \n",
    "        # 计算每个掩码配置的损失和敏感度\n",
    "        sensitivities = {}\n",
    "        \n",
    "        for i in range(MAX_NPARAMS):\n",
    "            # 获取第i行的掩码参数配置\n",
    "            masked_params = optimized_matrix[i]\n",
    "            \n",
    "            # 计算该配置下的损失\n",
    "            masked_loss = loss_fn_internal(masked_params, current_data)\n",
    "            \n",
    "            # 计算敏感度\n",
    "            sensitivity = jnp.where(\n",
    "                jnp.isnan(masked_loss) | jnp.isinf(masked_loss),\n",
    "                jnp.nan,\n",
    "                (masked_loss - original_loss) / original_loss\n",
    "            )\n",
    "            \n",
    "            sensitivities[i] = float(sensitivity)\n",
    "        \n",
    "        return sensitivities\n",
    "\n",
    "    current_initial_params = initial_params_override if initial_params_override is not None else initial_params\n",
    "\n",
    "    loss_partial = jit(lambda p: loss_fn_internal(p, data))\n",
    "    try:\n",
    "        result = minimize(loss_partial, current_initial_params, method='BFGS', options={'maxiter': 500})\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "    optimized_params = result.x\n",
    "    final_loss = loss_fn_internal(optimized_params, data)\n",
    "\n",
    "    if jnp.isnan(final_loss) or jnp.isinf(final_loss):\n",
    "        return None\n",
    "\n",
    "    # 计算各个参数的敏感度\n",
    "    sensitivities = calculate_masked_sensitivities_batch(optimized_params, final_loss, data)\n",
    "\n",
    "    return {\n",
    "        'optimized_params': optimized_params,\n",
    "        'final_loss': final_loss,\n",
    "        'negative_loss': -final_loss,\n",
    "        'sensitivities': sensitivities\n",
    "    }\n",
    "\n",
    "\n",
    "def equation(q: jnp.array, q_t: jnp.array, params: jnp.array) -> jnp.array:\n",
    "    \"\"\" Mathematical function for lagrangian in a physical system,which is a conservative system.\n",
    "    Args:\n",
    "        q (jnp.array): observations of current generalized coordinate,which dimension is rad.\n",
    "        q_t (jnp.array): observations of generalized velocity.\n",
    "        params (jnp.array): List of numeric constants or parameters to be optimized.\n",
    "    Returns:\n",
    "        jnp.array: lagrangian (T-V) and total energy (T+V).\n",
    "    \"\"\"\n",
    "\n",
    "    q1, q2 = q[:,0],q[:,1]\n",
    "    q1_t, q2_t = q_t[:,0],q_t[:,1]\n",
    "\n",
    "    # Kinetic energy (T)\n",
    "    T = params[0]*q1_t**2 + params[1]*q2_t**2 \n",
    "    # potential energy (V)\n",
    "    V = params[3]*q1 + params[4]*q2 + params[5]*jnp.cos(q1) + params[6]*jnp.cos(q2) + \\\n",
    "        params[8]*jnp.sin(q1) + params[7]*jnp.sin(q2) \n",
    "    \n",
    "    return T - V, T + V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40dee4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.80206776e-15 -1.80206776e-15]\n",
      " [ 7.70196859e-04 -1.14977777e-11]\n",
      " [ 3.08078740e-03 -1.21308289e-11]\n",
      " ...\n",
      " [ 4.98795246e+01 -2.30539589e-06]\n",
      " [ 4.99611381e+01 -2.79509812e-06]\n",
      " [ 5.00323659e+01 -3.35014171e-06]]\n",
      "[[-9.81000000e+00  0.00000000e+00]\n",
      " [-9.80999999e+00 -2.26759624e-08]\n",
      " [-9.80999979e+00 -3.62815393e-07]\n",
      " ...\n",
      " [ 1.96515787e+01 -1.87375710e+01]\n",
      " [ 1.54711058e+01 -1.33573195e+01]\n",
      " [ 1.11144949e+01 -7.73028138e+00]]\n",
      "{'optimized_params': Array([  0.92578317,   0.51176874,   1.        ,  -2.27120662,\n",
      "        -1.05742179, -18.3589969 ,  -9.44787378,  -0.09683702,\n",
      "         5.04498894,   1.        ], dtype=float64), 'final_loss': Array(105.6324943, dtype=float64), 'negative_loss': Array(-105.6324943, dtype=float64), 'sensitivities': {0: 1.8530223881547223, 1: 2.627426792901825, 2: -1.6278262024711505e-14, 3: 0.2583657524769522, 4: 0.07432524506118177, 5: 2.6259165000759235, 6: 0.5372272257631501, 7: 5.758876017124561e-05, 8: 0.029088609674932472, 9: -1.6278262024711505e-14}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# 读取 CSV 文件并转换为 NumPy 数组\n",
    "data0 = pd.read_csv('./train.csv')#\n",
    "tae = data0.to_numpy()\n",
    "\n",
    "# 使用 JAX 的数组操作替换 PyTorch 的操作\n",
    "state = jnp.array(tae[:, :4], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "true_q_ddot = jnp.array(tae[:, 4:6], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "energy = jnp.array(tae[:, 6:], dtype=jnp.float64)\n",
    "print(energy)\n",
    "print(true_q_ddot)\n",
    "# 将数据存储在字典中\n",
    "data = {\n",
    "    'inputs': state,\n",
    "    'outputs': energy\n",
    "}\n",
    "\n",
    "\n",
    "f = evaluate(data)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d59160bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config, jit, vmap, grad\n",
    "from jax.scipy.optimize import minimize\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def evaluate(data: dict, initial_params_override: jnp.array = None) -> dict | None:\n",
    "    @jit\n",
    "    def loss_fn_internal(params: jnp.array, current_data: dict) -> float:\n",
    "        inputs, outputs = current_data['inputs'], current_data['outputs']\n",
    "        q = inputs[:,:2]\n",
    "        q_t = inputs[:,2:4]\n",
    "        outputs_lagrangian = outputs[:, 0]\n",
    "        outputs_energy = outputs[:, 1]\n",
    "        y_pred, energy_pred = equation(q, q_t, params)\n",
    "        loss_lagrangian = jnp.mean((y_pred - outputs_lagrangian) ** 2)\n",
    "        loss_energy = 2 * jnp.mean((energy_pred - outputs_energy) ** 2)\n",
    "        return loss_lagrangian + loss_energy\n",
    "    \n",
    "    def calculate_masked_sensitivities_batch(optimized_params, original_loss, current_data):\n",
    "        # 使用矩阵运算创建掩码参数矩阵\n",
    "        mask = 1 - jnp.eye(MAX_NPARAMS, dtype=optimized_params.dtype)\n",
    "        masked_params_matrix = optimized_params * mask\n",
    "\n",
    "        @jit\n",
    "        def batch_masked_loss(params_matrix):\n",
    "            return vmap(loss_fn_internal, in_axes=(0, None))(params_matrix, current_data)\n",
    "\n",
    "        # 优化后的掩码优化函数\n",
    "        def optimize_with_masks():\n",
    "            def masked_opt_fn(flat_params):\n",
    "                reshaped = flat_params.reshape(MAX_NPARAMS, MAX_NPARAMS) * mask\n",
    "                return jnp.sum(batch_masked_loss(reshaped))\n",
    "            \n",
    "            result = minimize(masked_opt_fn, masked_params_matrix.flatten(),\n",
    "                            method='BFGS', options={'maxiter': 500})\n",
    "            return result.x.reshape(MAX_NPARAMS, MAX_NPARAMS) * mask\n",
    "\n",
    "        try:\n",
    "            optimized_matrix = optimize_with_masks()\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization error: {str(e)}\")\n",
    "            return {i: float('nan') for i in range(MAX_NPARAMS)}\n",
    "\n",
    "        # 向量化计算敏感度\n",
    "        losses = batch_masked_loss(optimized_matrix)\n",
    "        relative_loss = (losses - original_loss) / original_loss\n",
    "        return {i: float(jnp.nan_to_num(relative_loss[i])) for i in range(MAX_NPARAMS)}\n",
    "\n",
    "    \n",
    "    current_initial_params = initial_params_override if initial_params_override is not None else initial_params\n",
    "\n",
    "    loss_partial = jit(lambda p: loss_fn_internal(p, data))\n",
    "    try:\n",
    "        result = minimize(loss_partial, current_initial_params, method='BFGS', options={'maxiter': 500})\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "    optimized_params = result.x\n",
    "    final_loss = loss_fn_internal(optimized_params, data)\n",
    "\n",
    "    if jnp.isnan(final_loss) or jnp.isinf(final_loss):\n",
    "        return None\n",
    "\n",
    "    # 计算各个参数的敏感度\n",
    "    sensitivities = calculate_masked_sensitivities_batch(optimized_params, final_loss, data)\n",
    "\n",
    "    return {\n",
    "        'optimized_params': optimized_params,\n",
    "        'final_loss': final_loss,\n",
    "        'negative_loss': -final_loss,\n",
    "        'sensitivities': sensitivities\n",
    "    }\n",
    "\n",
    "\n",
    "def equation(q: jnp.array, q_t: jnp.array, params: jnp.array) -> jnp.array:\n",
    "    \"\"\" Mathematical function for lagrangian in a physical system,which is a conservative system.\n",
    "    Args:\n",
    "        q (jnp.array): observations of current generalized coordinate,which dimension is rad.\n",
    "        q_t (jnp.array): observations of generalized velocity.\n",
    "        params (jnp.array): List of numeric constants or parameters to be optimized.\n",
    "    Returns:\n",
    "        jnp.array: lagrangian (T-V) and total energy (T+V).\n",
    "    \"\"\"\n",
    "\n",
    "    q1, q2 = q[:,0],q[:,1]\n",
    "    q1_t, q2_t = q_t[:,0],q_t[:,1]\n",
    "\n",
    "    # Kinetic energy (T)\n",
    "    T = params[0]*q1_t**2 + params[1]*q2_t**2 \n",
    "    # potential energy (V)\n",
    "    V = params[3]*q1 + params[4]*q2 + params[5]*jnp.cos(q1) + params[6]*jnp.cos(q2) + \\\n",
    "        params[8]*jnp.sin(q1) + params[7]*jnp.sin(q2) \n",
    "    \n",
    "    return T - V, T + V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ea44170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.80206776e-15 -1.80206776e-15]\n",
      " [ 7.70196859e-04 -1.14977777e-11]\n",
      " [ 3.08078740e-03 -1.21308289e-11]\n",
      " ...\n",
      " [ 4.98795246e+01 -2.30539589e-06]\n",
      " [ 4.99611381e+01 -2.79509812e-06]\n",
      " [ 5.00323659e+01 -3.35014171e-06]]\n",
      "[[-9.81000000e+00  0.00000000e+00]\n",
      " [-9.80999999e+00 -2.26759624e-08]\n",
      " [-9.80999979e+00 -3.62815393e-07]\n",
      " ...\n",
      " [ 1.96515787e+01 -1.87375710e+01]\n",
      " [ 1.54711058e+01 -1.33573195e+01]\n",
      " [ 1.11144949e+01 -7.73028138e+00]]\n",
      "{'optimized_params': Array([  0.92578317,   0.51176874,   1.        ,  -2.27120662,\n",
      "        -1.05742179, -18.3589969 ,  -9.44787378,  -0.09683702,\n",
      "         5.04498894,   1.        ], dtype=float64), 'final_loss': Array(105.6324943, dtype=float64), 'negative_loss': Array(-105.6324943, dtype=float64), 'sensitivities': {0: 1.8530223881597834, 1: 2.6274267929103012, 2: -1.6143730933598184e-14, 3: 0.2583657521740584, 4: 0.07432524495188332, 5: 2.625916500241012, 6: 0.5372272258175684, 7: 5.758875910575937e-05, 8: 0.029088609628322156, 9: -1.6143730933598184e-14}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# 读取 CSV 文件并转换为 NumPy 数组\n",
    "data0 = pd.read_csv('./train.csv')#\n",
    "tae = data0.to_numpy()\n",
    "\n",
    "# 使用 JAX 的数组操作替换 PyTorch 的操作\n",
    "state = jnp.array(tae[:, :4], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "true_q_ddot = jnp.array(tae[:, 4:6], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "energy = jnp.array(tae[:, 6:], dtype=jnp.float64)\n",
    "print(energy)\n",
    "print(true_q_ddot)\n",
    "# 将数据存储在字典中\n",
    "data = {\n",
    "    'inputs': state,\n",
    "    'outputs': energy\n",
    "}\n",
    "\n",
    "\n",
    "f = evaluate(data)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a4cf6f",
   "metadata": {},
   "source": [
    "### 包含pso优化的双摆例子进行梯度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f567e699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config, grad, jit, vmap\n",
    "import numpy as np\n",
    "from pyswarm import pso\n",
    "from jax.scipy.optimize import minimize\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = [jnp.array(1.0)]*MAX_NPARAMS\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(data: dict, params=initial_params) -> float:\n",
    "\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    n_dim = inputs.shape[1] // 2\n",
    "    q = inputs[:, :n_dim]\n",
    "    q_t = inputs[:, n_dim:]\n",
    "    true_accelerations = outputs\n",
    "\n",
    "\n",
    "    # 修改：将lagrangian直接嵌入compute_acceleration中，避免将函数作为参数传递\n",
    "    @jit\n",
    "    def compute_acceleration(q, q_t, params):\n",
    "        # 直接使用equation函数而不是传递lagrangian函数\n",
    "        \n",
    "        # 预先计算梯度\n",
    "        def lag(q, q_t, params):\n",
    "            return equation(q, q_t, params)\n",
    "            \n",
    "        # 计算二阶导数\n",
    "        hessian_q_t = jax.hessian(lag, 1)(q, q_t, params)\n",
    "        grad_q = jax.grad(lag, 0)(q, q_t, params)\n",
    "        jacobian_q_q_t = jax.jacobian(jax.grad(lag, 1), 0)(q, q_t, params)\n",
    "        q_tt = jnp.linalg.pinv(hessian_q_t) @ (grad_q - jacobian_q_q_t @ q_t)\n",
    "        return q_tt\n",
    "\n",
    "    # 使用vmap批处理\n",
    "    batch_compute_acceleration = jit(vmap(compute_acceleration, in_axes=(0, 0, None)))\n",
    "\n",
    "    # 使用jit装饰器优化损失函数\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        predicted_accelerations = batch_compute_acceleration(q, q_t, params)\n",
    "        return jnp.mean(jnp.square(predicted_accelerations - true_accelerations))\n",
    "\n",
    "    # 针对numpy数组的包装函数\n",
    "    def objective(params):\n",
    "        try:\n",
    "            params = jnp.array(params)  # 确保转换为JAX数组\n",
    "            loss_value = loss_fn(params)\n",
    "            return float(loss_value)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in objective function: {e}\")\n",
    "            # 返回一个大的损失值，避免优化器选择这个点\n",
    "            return 1e10\n",
    "\n",
    "    # 粒子群优化的参数\n",
    "    lb = [-1.0] * len(initial_params)  # 参数下限\n",
    "    ub = [10.0] * len(initial_params)   # 参数上限\n",
    "\n",
    "    # 调用 pso 函数进行优化\n",
    "    optimized_params, optimized_loss = pso(objective, lb, ub, swarmsize=30, maxiter=500)\n",
    "\n",
    "    print(\"pso Optimized parameters:\", optimized_params)\n",
    "    print(\"pso Optimized loss:\", optimized_loss)\n",
    "\n",
    "    # 使用JAX的优化器进一步优化\n",
    "    loss_partial = jit(loss_fn)  # 确保JIT编译\n",
    "    result = minimize(loss_partial, optimized_params, method='BFGS', options={'maxiter': 1000})\n",
    "    optimized_params = result.x\n",
    "    loss = result.fun\n",
    "\n",
    "    if jnp.isnan(loss) or jnp.isinf(loss):\n",
    "        return None\n",
    "    else:\n",
    "        print(optimized_params)\n",
    "        return -loss.item()\n",
    "\n",
    "\n",
    "@jit  # 添加JIT装饰器\n",
    "def equation(q: jnp.array, q_t: jnp.array, params: jnp.array) -> jnp.array:\n",
    "    \"\"\" Mathematical function for lagrangian in a physical system\n",
    "    Args:\n",
    "        q (jnp.array): observations of current generalized coordinate.\n",
    "        q_t (jnp.array): observations of generalized velocity.\n",
    "        params (jnp.array): List of numeric constants or parameters to be optimized.\n",
    "    Returns:\n",
    "        jnp.array: lagrangian as the result of applying the mathematical function to the inputs.\n",
    "    \"\"\"\n",
    "    \"\"\"Improved version of `equation_v1` with additional parameters for damping and external forces.\"\"\"\n",
    "    q1, q2 = q[...,0], q[..., 1]\n",
    "    q1_t, q2_t = q_t[..., 0], q_t[..., 1]\n",
    "\n",
    "    m1, m2, l1, l2, g = params[0], params[1], params[2], params[3], params[4]\n",
    "    c1, c2 = params[5], params[6]  # damping coefficients for each pendulum\n",
    "    F1, F2 = params[7], params[8]  # external forces for each pendulum\n",
    "    k = params[9]  # coefficient for interaction term between q1 and q2\n",
    "\n",
    "    # Kinetic energy terms (considering interactions between coordinates)\n",
    "    T = (0.5 * m1 * l1**2 * q1_t**2 +\n",
    "         0.5 * m2 * (l1**2 * q1_t**2 + l2**2 * q2_t**2 + 2 * l1 * l2 * q1_t * q2_t * jnp.cos(q1 - q2)))\n",
    "\n",
    "    # Damping terms\n",
    "    D = (c1 * q1_t +\n",
    "         c2 * q2_t)\n",
    "\n",
    "    # Potential energy terms (gravity and external forces)\n",
    "    V = -g * (m1 * l1 * jnp.cos(q1) + m2 * (l1 * jnp.cos(q1) + l2 * jnp.cos(q2))) + \\\n",
    "        F1 * q1 + F2 * q2\n",
    "\n",
    "\n",
    "    # Lagrangian including potential, kinetic, damping, and interaction terms\n",
    "    return T - V - D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9943c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config, grad, jit, vmap\n",
    "from pyswarm import pso\n",
    "from jax.scipy.optimize import minimize\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "def evaluate(data: dict, params=initial_params) -> dict:\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    n_dim = inputs.shape[1] // 2\n",
    "    q, q_t = inputs[:, :n_dim], inputs[:, n_dim:]\n",
    "    true_accelerations = outputs\n",
    "\n",
    "    @jit\n",
    "    def compute_acceleration(q, q_t, params):\n",
    "        def lag(q, q_t, params):\n",
    "            return equation(q, q_t, params)\n",
    "        hessian_q_t = jax.hessian(lag, 1)(q, q_t, params)\n",
    "        grad_q = jax.grad(lag, 0)(q, q_t, params)\n",
    "        jacobian_q_q_t = jax.jacobian(jax.grad(lag, 1), 0)(q, q_t, params)\n",
    "        return jnp.linalg.pinv(hessian_q_t) @ (grad_q - jacobian_q_q_t @ q_t)\n",
    "\n",
    "    batch_compute_acceleration = jit(vmap(compute_acceleration, (0, 0, None)))\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        pred = batch_compute_acceleration(q, q_t, params)\n",
    "        return jnp.mean(jnp.square(pred - true_accelerations))\n",
    "\n",
    "    def run_optimization(objective_fn, initial_guess):\n",
    "        if initial_guess.size > MAX_NPARAMS:\n",
    "            result = minimize(objective_fn, initial_guess,\n",
    "                            method='BFGS', options={'maxiter': 500})\n",
    "            return result.x\n",
    "        else:\n",
    "            def pso_wrapper(x):\n",
    "                return objective_fn(jnp.array(x))\n",
    "            \n",
    "            lb = [-1.0]*initial_guess.size\n",
    "            ub = [10.0]*initial_guess.size\n",
    "            \n",
    "            pso_params, _ = pso(pso_wrapper, lb, ub, \n",
    "                            swarmsize=30, maxiter=200)\n",
    "            \n",
    "            result = minimize(objective_fn, jnp.array(pso_params),\n",
    "                            method='BFGS', options={'maxiter': 500})\n",
    "            return result.x\n",
    "\n",
    "\n",
    "    # 敏感度分析模块\n",
    "    def calculate_sensitivities(opt_params, base_loss):\n",
    "        mask = 1 - jnp.eye(MAX_NPARAMS)\n",
    "        \n",
    "        @jit\n",
    "        def batch_loss(params_matrix):\n",
    "            return vmap(loss_fn)(params_matrix * mask)\n",
    "\n",
    "        def sensitivity_objective(flat_params):\n",
    "            matrix_params = flat_params.reshape(MAX_NPARAMS, MAX_NPARAMS)\n",
    "            return jnp.sum(batch_loss(matrix_params))\n",
    "\n",
    "        try:\n",
    "            initial_flat = (opt_params * mask).flatten()\n",
    "            optimized_flat = run_optimization(sensitivity_objective, initial_flat)\n",
    "            optimized_matrix = optimized_flat.reshape(MAX_NPARAMS, MAX_NPARAMS)\n",
    "            losses = batch_loss(optimized_matrix)\n",
    "            relative_loss = (losses - base_loss) / base_loss\n",
    "            return {i: float(jnp.nan_to_num(relative_loss[i])) for i in range(MAX_NPARAMS)}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Sensitivity analysis error: {str(e)}\")\n",
    "            return {i: 0.0 for i in range(MAX_NPARAMS)}\n",
    "\n",
    "\n",
    "    # 主流程\n",
    "    try:\n",
    "        optimized_params = run_optimization(loss_fn, params)\n",
    "        final_loss = loss_fn(optimized_params)\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not jnp.isfinite(final_loss):\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'params': optimized_params,\n",
    "        'loss': final_loss,\n",
    "        'sensitivities': calculate_sensitivities(optimized_params, final_loss)\n",
    "    }\n",
    "\n",
    "@jit\n",
    "def equation(q: jnp.array, q_t: jnp.array, params: jnp.array) -> jnp.array:\n",
    "    q1, q2 = q[...,0], q[...,1]\n",
    "    q1_t, q2_t = q_t[...,0], q_t[...,1]\n",
    "    \n",
    "    m1, m2, l1, l2, g = params[0], params[1], params[2], params[3], params[4]\n",
    "    c1, c2, F1, F2, k = params[5], params[6], params[7], params[8], params[9]\n",
    "    \n",
    "    T = 0.5*(m1*l1**2*q1_t**2 + m2*(l1**2*q1_t**2 + l2**2*q2_t**2 + 2*l1*l2*q1_t*q2_t*jnp.cos(q1-q2)))\n",
    "    V = -g*(m1*l1*jnp.cos(q1) + m2*(l1*jnp.cos(q1)+l2*jnp.cos(q2))) + F1*q1 + F2*q2\n",
    "    D = c1*q1_t + c2*q2_t\n",
    "    return T - V - D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6373be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.81000000e+00  0.00000000e+00]\n",
      " [-9.80999997e+00 -5.53723182e-08]\n",
      " [-9.80999948e+00 -8.85956966e-07]\n",
      " ...\n",
      " [ 1.52940364e+01 -1.63022950e+01]\n",
      " [ 1.48602131e+01 -1.55387182e+01]\n",
      " [ 1.43807650e+01 -1.47005543e+01]]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Stopping search: Swarm best objective change less than 1e-08\n",
      "最终损失值 (MSE): {'params': Array([1.03445108e+01, 1.03445105e+01, 2.01300011e+00, 2.01300034e+00,\n",
      "       1.97475313e+01, 9.84050994e-01, 4.90573269e+00, 1.18473294e-05,\n",
      "       9.51485829e-06, 2.57651355e-01], dtype=float64), 'loss': Array(9.69115154e-13, dtype=float64), 'sensitivities': {0: 3.20130858136045e+19, 1: 139222546490502.81, 2: 119653707896404.27, 3: 139222547621080.39, 4: 298745128231405.3, 5: 2676.4397483168295, 6: 2676.4397483168295, 7: 2658.04703368037, 8: 2721.4478237104254, 9: 2676.4397483168295}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 读取 CSV 文件并转换为 NumPy 数组\n",
    "data0 = pd.read_csv('./double_pendulum_data_with_energy.csv')#\n",
    "tae = data0.to_numpy()\n",
    "\n",
    "# 使用 JAX 的数组操作替换 PyTorch 的操作\n",
    "state = jnp.array(tae[:, :4], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "true_q_ddot = jnp.array(tae[:, 4:-1], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "energy = jnp.array(tae[:, -1], dtype=jnp.float64)\n",
    "print(true_q_ddot)\n",
    "# 将数据存储在字典中\n",
    "data = {\n",
    "    'inputs': state,\n",
    "    'outputs': true_q_ddot,  # 真实的加速度\n",
    "    'energy': energy\n",
    "}\n",
    "\n",
    "''''inputs, outputs = data['inputs'], data['outputs']\n",
    "print(inputs.shape, outputs.shape)\n",
    "n_dim = inputs.shape[1] // 2\n",
    "q = inputs[:, :n_dim]\n",
    "q_t = inputs[:, n_dim:]\n",
    "true_accelerations = outputs\n",
    "\n",
    "q1, q2 = q[...,0], q[..., 1]\n",
    "q1_t, q2_t = q_t[..., 0], q_t[..., 1]\n",
    "print(q1.shape, q2.shape, q1_t.shape, q2_t.shape)\n",
    "\n",
    "print(q.shape, q_t.shape, true_accelerations.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(equation(q,q_t,initial_params))'''\n",
    "\n",
    "\n",
    "print(initial_params)\n",
    "# 评估并优化参数\n",
    "final_loss = evaluate(data, initial_params)\n",
    "print(\"最终损失值 (MSE):\", final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235c4ca",
   "metadata": {},
   "source": [
    "#### 结构化优化的过程中，我们发现，对于复杂式子的优化和伪梯度计算，会出现不该有的项敏感度为负值，这是我们所希望的结果，但偶尔也会出现敏感度为几千，但该有的项敏感度为8，9，甚至10几次方量级，故而，可以使用概率归一化来缩小数值，以便让大模型更好的分析。\n",
    "不行，数值相差太大是，其他值都为0，一个数为1，绝对值归一化后也会导致过于大的值占主导，其余值基本保持一致。\n",
    "好吧，尝试各种方法，对于绝对大的敏感度好像无法量化衡量，其余相比之下都为0.\n",
    "\n",
    "使得中位数为敏感度为1,不去归一化，某一项绝对大   \n",
    "$$ sensetive = e^{\\frac{x-x_d}{x_d}}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca1fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config, grad, jit, vmap\n",
    "from pyswarm import pso\n",
    "from jax.scipy.optimize import minimize\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "def evaluate(data: dict, params=initial_params) -> dict:\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    n_dim = inputs.shape[1] // 2\n",
    "    q, q_t = inputs[:, :n_dim], inputs[:, n_dim:]\n",
    "    true_accelerations = outputs\n",
    "\n",
    "    @jit\n",
    "    def compute_acceleration(q, q_t, params):\n",
    "        def lag(q, q_t, params):\n",
    "            return equation(q, q_t, params)\n",
    "        hessian_q_t = jax.hessian(lag, 1)(q, q_t, params)\n",
    "        grad_q = jax.grad(lag, 0)(q, q_t, params)\n",
    "        jacobian_q_q_t = jax.jacobian(jax.grad(lag, 1), 0)(q, q_t, params)\n",
    "        return jnp.linalg.pinv(hessian_q_t) @ (grad_q - jacobian_q_q_t @ q_t)\n",
    "\n",
    "    batch_compute_acceleration = jit(vmap(compute_acceleration, (0, 0, None)))\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        pred = batch_compute_acceleration(q, q_t, params)\n",
    "        return jnp.mean(jnp.square(pred - true_accelerations))\n",
    "\n",
    "    def run_optimization(objective_fn, initial_guess):\n",
    "        \"\"\"处理任意维度的参数优化\"\"\"\n",
    "        # PSO阶段需要特殊处理高维参数\n",
    "        if initial_guess.size > MAX_NPARAMS:\n",
    "            result = minimize(objective_fn, initial_guess,\n",
    "                            method='BFGS', options={'maxiter': 500})\n",
    "            return result.x\n",
    "        else:\n",
    "            # 原始混合优化逻辑\n",
    "            def pso_wrapper(x):\n",
    "                return objective_fn(jnp.array(x))\n",
    "            \n",
    "            lb = [-1.0]*initial_guess.size\n",
    "            ub = [10.0]*initial_guess.size\n",
    "            \n",
    "            pso_params, _ = pso(pso_wrapper, lb, ub, \n",
    "                            swarmsize=30, maxiter=200)\n",
    "            \n",
    "            result = minimize(objective_fn, jnp.array(pso_params),\n",
    "                            method='BFGS', options={'maxiter': 500})\n",
    "            return result.x\n",
    "\n",
    "\n",
    "    # 敏感度分析模块\n",
    "    def calculate_sensitivities(opt_params, base_loss):\n",
    "        mask = 1 - jnp.eye(MAX_NPARAMS)\n",
    "        \n",
    "        @jit\n",
    "        def batch_loss(params_matrix):\n",
    "            # 确保输入是正确形状的矩阵\n",
    "            return vmap(loss_fn)(params_matrix * mask)\n",
    "\n",
    "        def sensitivity_objective(flat_params):\n",
    "            # 显式重塑参数形状\n",
    "            matrix_params = flat_params.reshape(MAX_NPARAMS, MAX_NPARAMS)\n",
    "            return jnp.sum(batch_loss(matrix_params))\n",
    "\n",
    "        try:\n",
    "            # 生成正确的初始参数（复制参数矩阵并应用掩码）\n",
    "            initial_flat = (opt_params * mask).flatten()\n",
    "            \n",
    "            # 执行优化（确保处理100个参数）\n",
    "            optimized_flat = run_optimization(sensitivity_objective, initial_flat)\n",
    "            optimized_matrix = optimized_flat.reshape(MAX_NPARAMS, MAX_NPARAMS)\n",
    "            \n",
    "            # 计算损失差异\n",
    "            losses = batch_loss(optimized_matrix)\n",
    "            raw_relative = (losses - base_loss) / base_loss\n",
    "            scaled = jnp.nan_to_num(raw_relative, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            \n",
    "            # 新增加的核心计算逻辑\n",
    "            x_d = jnp.median(scaled)\n",
    "            epsilon = 1e-8  # 防止除以零\n",
    "            adjusted_xd = x_d \n",
    "            \n",
    "            # 应用指数公式\n",
    "            relative_to_median = (scaled - x_d) / adjusted_xd\n",
    "            weights = jnp.exp(relative_to_median)\n",
    "            \n",
    "            # 保持符号（相对于中位数的变化方向）\n",
    "            final_sensitivities = weights * jnp.sign(scaled)\n",
    "\n",
    "            return {i: float(final_sensitivities[i]) for i in range(MAX_NPARAMS)}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Sensitivity analysis error: {str(e)}\")\n",
    "            return {i: 0.0 for i in range(MAX_NPARAMS)}\n",
    "\n",
    "\n",
    "    # 主流程\n",
    "    try:\n",
    "        optimized_params = run_optimization(loss_fn, params)\n",
    "        final_loss = loss_fn(optimized_params)\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not jnp.isfinite(final_loss):\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'params': optimized_params,\n",
    "        'loss': final_loss,\n",
    "        'sensitivities': calculate_sensitivities(optimized_params, final_loss)\n",
    "    }\n",
    "\n",
    "@jit\n",
    "def equation(q: jnp.array, q_t: jnp.array, params: jnp.array) -> jnp.array:\n",
    "    q1, q2 = q[...,0], q[...,1]\n",
    "    q1_t, q2_t = q_t[...,0], q_t[...,1]\n",
    "    \n",
    "    m1, m2, l1, l2, g = params[0], params[1], params[2], params[3], params[4]\n",
    "    c1, c2, F1, F2, k = params[5], params[6], params[7], params[8], params[9]\n",
    "    \n",
    "    T = 0.5*(m1*l1**2*q1_t**2 + m2*(l1**2*q1_t**2 + l2**2*q2_t**2 + 2*l1*l2*q1_t*q2_t*jnp.cos(q1-q2)))\n",
    "    V = -g*(m1*l1*jnp.cos(q1) + m2*(l1*jnp.cos(q1)+l2*jnp.cos(q2))) #+ F1*q1 + F2*q2\n",
    "    D = c1*q1_t + c2*q2_t\n",
    "    return T - V - D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64f4edc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.81000000e+00  0.00000000e+00]\n",
      " [-9.80999997e+00 -5.53723182e-08]\n",
      " [-9.80999948e+00 -8.85956966e-07]\n",
      " ...\n",
      " [ 1.52940364e+01 -1.63022950e+01]\n",
      " [ 1.48602131e+01 -1.55387182e+01]\n",
      " [ 1.43807650e+01 -1.47005543e+01]]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Stopping search: Swarm best position change less than 1e-08\n",
      "最终损失值 (MSE): {'params': Array([ 5.728103  ,  5.72810327,  1.0260507 ,  1.02605066, 10.06555711,\n",
      "        5.79314061,  6.02788142,  9.16491405,  5.19117204, 10.        ],      dtype=float64), 'loss': Array(1.41403035e-13, dtype=float64), 'sensitivities': {0: inf, 1: 3.3242815168480804, 2: 2.718281828323312, 3: 3.3242815168480804, 4: 366.9478979839086, 5: 0.3678794411898118, 6: 0.3678794411898118, 7: 0.3678794411898118, 8: 0.3678794411898118, 9: 0.3678794411898118}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 读取 CSV 文件并转换为 NumPy 数组\n",
    "data0 = pd.read_csv('./double_pendulum_data_with_energy.csv')#\n",
    "tae = data0.to_numpy()\n",
    "\n",
    "# 使用 JAX 的数组操作替换 PyTorch 的操作\n",
    "state = jnp.array(tae[:, :4], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "true_q_ddot = jnp.array(tae[:, 4:-1], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "energy = jnp.array(tae[:, -1], dtype=jnp.float64)\n",
    "print(true_q_ddot)\n",
    "# 将数据存储在字典中\n",
    "data = {\n",
    "    'inputs': state,\n",
    "    'outputs': true_q_ddot,  # 真实的加速度\n",
    "    'energy': energy\n",
    "}\n",
    "\n",
    "\n",
    "print(initial_params)\n",
    "# 评估并优化参数\n",
    "final_loss = evaluate(data, initial_params)\n",
    "print(\"最终损失值 (MSE):\", final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dbde2d",
   "metadata": {},
   "source": [
    "## 来测试一下其他的优化器：\n",
    "### 1.量子隧穿优化，有一次优化到-19次方，牛逼，运行16S左右，和粒子群差不多,但在参数未全部使用的情况下，无效   \n",
    "优化损失: [3.10366193e+07 1.42791394e+02 1.36837775e+02 1.42791518e+02\n",
    " 1.73510532e+07 1.66615206e-14 1.66615206e-14 1.74181776e-14\n",
    " 1.55317123e-14 1.66615206e-14]\n",
    "最终损失值 (MSE): {'params': Array([ 1.38116467e+00,  1.38116467e+00,  5.92812112e-01,  5.92812112e-01,\n",
    "        5.81548682e+00, -2.47551587e-02,  4.32660101e-01,  1.83811304e-10,\n",
    "        9.94207883e-11,  7.87441955e-01], dtype=float64), 'loss': Array(1.10887512e-19, dtype=float64), 'sensitivities': {0: 2.7989282841833538e+26, 1: 1.2877139359385067e+21, 2: 1.2340233210216482e+21, 3: 1.287715049505326e+21, 4: 1.5647436669708444e+26, 5: 150255.05974498438, 6: 150255.05974498438, 7: 157078.7052214516, 8: 140066.28182291775, 9: 150255.05974498438}}\n",
    "### 2.神经引导优化，7.6s就能优化到-13次方，，同量子隧穿，参数未全部使用则无效。但是对于掩码后的参数优化，不能有效适配,不同网络不稳定，使用了稳定的网络到-15，但是要1分半钟，优化到22秒，但是调了几个超参，发现影响很大，这说明对于不同问题，还需要调超惨，这种方法不太行。   \n",
    "优化损失: [1.93514032e+07 1.43640117e+02 1.43390734e+02 1.54258418e+02\n",
    " 3.66413656e+03 5.23901447e+07 1.34418381e+07 3.54652590e+04\n",
    " 2.75959884e+02 2.89812589e+02]\n",
    "最终损失值 (MSE): {'params': Array([ 1.26675148e+00,  1.26675151e+00,  7.75105047e-01,  7.75105064e-01,\n",
    "        7.60378114e+00,  7.11072584e-01,  7.69675069e-01, -3.48068037e-07,\n",
    "       -1.08772388e-07,  7.48417784e-01], dtype=float64), 'loss': Array(3.22607551e-13, dtype=float64), 'sensitivities': {0: 5.99843465581859e+19, 1: 445247225168795.9, 2: 444474201985906.1, 3: 478161213559623.4, 4: 1.1357875980563062e+16, 5: 1.6239590331551914e+20, 6: 4.166622282788226e+19, 7: 1.0993313352637762e+17, 8: 855404293090404.2, 9: 898344097753334.1}}\n",
    "### 3.混沌优化效果也不错，但是需要1分钟甚至2分钟的运行时间\n",
    "优化损失: [3.10520114e+07 1.30095089e+02 1.12380677e+02 1.30095090e+02\n",
    " 5.15585807e+02 2.23281006e-13 2.23281006e-13 2.33281008e-13\n",
    " 2.25769055e-13 2.23281006e-13]\n",
    "最终损失值 (MSE): {'params': Array([ 1.07311760e+01,  1.07311760e+01,  3.13674746e+00,  3.13674747e+00,\n",
    "        3.07714928e+01,  8.77470277e-01,  9.34985895e+00, -6.56614019e-06,\n",
    "        4.65663855e-07,  8.99687204e+00], dtype=float64), 'loss': Array(5.16725456e-15, dtype=float64), 'sensitivities': {0: 6.009382954031375e+21, 1: 2.5176829932118064e+16, 2: 2.1748624192127092e+16, 3: 2.517683001464005e+16, 4: 9.977944796813381e+16, 5: 42.210761888552184, 6: 42.210761888552184, 7: 44.14602586173936, 8: 42.692265023123255, 9: 42.210761888552184}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76e8502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random\n",
    "import optax\n",
    "from flax import linen as nn\n",
    "from scipy.optimize import differential_evolution\n",
    "from flax.training import train_state \n",
    "import jax.numpy as jnp\n",
    "from jax import config, grad, jit, vmap\n",
    "from pyswarm import pso\n",
    "from jax.scipy.optimize import minimize\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "def evaluate(data: dict, params=initial_params) -> dict:\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    n_dim = inputs.shape[1] // 2\n",
    "    q, q_t = inputs[:, :n_dim], inputs[:, n_dim:]\n",
    "    true_accelerations = outputs\n",
    "\n",
    "    @jit\n",
    "    def compute_acceleration(q, q_t, params):\n",
    "        def lag(q, q_t, params):\n",
    "            return equation(q, q_t, params)\n",
    "        hessian_q_t = jax.hessian(lag, 1)(q, q_t, params)\n",
    "        grad_q = jax.grad(lag, 0)(q, q_t, params)\n",
    "        jacobian_q_q_t = jax.jacobian(jax.grad(lag, 1), 0)(q, q_t, params)\n",
    "        return jnp.linalg.pinv(hessian_q_t) @ (grad_q - jacobian_q_q_t @ q_t)\n",
    "\n",
    "    batch_compute_acceleration = jit(vmap(compute_acceleration, (0, 0, None)))\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        pred = batch_compute_acceleration(q, q_t, params)\n",
    "        return jnp.mean(jnp.square(pred - true_accelerations))\n",
    "\n",
    "\n",
    "        # 量子隧穿优化\n",
    "    def run_optimization(objective_fn, initial_guess, method='quantum', key=random.PRNGKey(0)):\n",
    "        \"\"\"\n",
    "        Optimized function to minimize objective_fn using quantum tunneling optimization.\n",
    "        \n",
    "        Args:\n",
    "            objective_fn: Function to minimize\n",
    "            initial_guess: Initial parameters (jnp.array)\n",
    "            method: Optimization method ('quantum', 'bfgs')\n",
    "            key: JAX random key\n",
    "        \n",
    "        Returns:\n",
    "            Optimized parameters (jnp.array)\n",
    "        \"\"\"\n",
    "        @jit\n",
    "        def bfgs_optimize(params):\n",
    "            result = minimize(objective_fn, params, method='BFGS', \n",
    "                            options={'maxiter': 200, 'gtol': 1e-6})\n",
    "            return result.x\n",
    "\n",
    "        if initial_guess.size > MAX_NPARAMS or method == 'bfgs':\n",
    "            return bfgs_optimize(initial_guess)\n",
    "        \n",
    "        # Quantum tunneling optimization parameters\n",
    "        TUNNEL_PROB = 0.05  # Reduced tunneling probability for better exploration\n",
    "        SWARM_SIZE = 30     # Increased swarm size for diversity\n",
    "        MAX_ITER = 150      # More iterations for convergence\n",
    "        DIM = initial_guess.size  # Parameter dimension (10)\n",
    "        \n",
    "        # Initialize swarm\n",
    "        keys = random.split(key, 5)\n",
    "        positions = initial_guess + random.normal(keys[0], (SWARM_SIZE, DIM)) * 0.2\n",
    "        velocities = random.normal(keys[1], (SWARM_SIZE, DIM)) * 0.02\n",
    "        pbest = jnp.copy(positions)\n",
    "        pbest_losses = vmap(objective_fn)(positions)\n",
    "        gbest_idx = jnp.argmin(pbest_losses)\n",
    "        gbest = positions[gbest_idx]\n",
    "        potential_well = jnp.copy(gbest)\n",
    "        \n",
    "        def update_particle(carry, _):\n",
    "            pos, vel, pbest, pbest_losses, gbest, potential_well = carry\n",
    "            \n",
    "            # Compute losses\n",
    "            losses = vmap(objective_fn)(pos)\n",
    "            \n",
    "            # Quantum tunneling mask\n",
    "            tunnel_mask = random.bernoulli(keys[2], TUNNEL_PROB, (SWARM_SIZE,))\n",
    "            \n",
    "            # Velocity update with adaptive quantum term\n",
    "            r1 = random.uniform(keys[3], (SWARM_SIZE, DIM))\n",
    "            r2 = random.uniform(keys[4], (SWARM_SIZE, DIM))\n",
    "            cognitive = 1.0 * r1 * (pbest - pos)  # Emphasize personal best\n",
    "            social = 1.0 * r2 * (gbest - pos)     # Emphasize global best\n",
    "            quantum_term = 0.05 * random.normal(keys[3], (SWARM_SIZE, DIM))\n",
    "            vel = 0.8 * vel + cognitive + social + quantum_term\n",
    "            \n",
    "            # Position update with selective tunneling\n",
    "            tunnel_pos = potential_well + 0.1 * random.normal(keys[4], (SWARM_SIZE, DIM))\n",
    "            pos = jnp.where(tunnel_mask[:, None], tunnel_pos, pos + vel)\n",
    "            \n",
    "            # Update personal and global best\n",
    "            pbest = jnp.where(losses[:, None] < pbest_losses[:, None], pos, pbest)\n",
    "            pbest_losses = jnp.minimum(losses, pbest_losses)\n",
    "            gbest_idx = jnp.argmin(losses)\n",
    "            gbest = pos[gbest_idx]\n",
    "            \n",
    "            # Update potential well adaptively\n",
    "            well_update_factor = 0.05 / (1 + jnp.exp(-jnp.min(losses)))  # Adaptive based on loss\n",
    "            potential_well = (1 - well_update_factor) * potential_well + well_update_factor * gbest\n",
    "            \n",
    "            return (pos, vel, pbest, pbest_losses, gbest, potential_well), None\n",
    "        \n",
    "        # Run quantum optimization\n",
    "        (positions, _, pbest, pbest_losses, gbest, _), _ = jax.lax.scan(\n",
    "            update_particle,\n",
    "            (positions, velocities, pbest, pbest_losses, gbest, potential_well),\n",
    "            None,\n",
    "            length=MAX_ITER\n",
    "        )\n",
    "        \n",
    "        # BFGS refinement\n",
    "        quantum_params = gbest\n",
    "        return bfgs_optimize(quantum_params)\n",
    "    \n",
    "    # 神经引导优化算法\n",
    "    '''%autoawaitclass GuideNN(nn.Module):\n",
    "        hidden_dim: int\n",
    "        output_dim: int\n",
    "\n",
    "        @nn.compact\n",
    "        def __call__(self, x):\n",
    "            x = nn.Dense(features=self.hidden_dim)(x)\n",
    "            x = nn.relu(x)\n",
    "            x = nn.Dense(features=self.output_dim)(x) # Output matches parameter dimension\n",
    "            return x\n",
    "\n",
    "    # 神经引导优化算法\n",
    "    def run_optimization(loss_fn_to_optimize, init_params_opt, num_steps=1000, learning_rate=1e-3, guide_strength=0.05, seed=42):\n",
    "        \"\"\"\n",
    "        Runs optimization using a fixed neuro-guided approach.\n",
    "\n",
    "        Args:\n",
    "            loss_fn_to_optimize: The loss function to minimize.\n",
    "            init_params_opt: Initial parameters for optimization.\n",
    "            num_steps: Number of optimization steps.\n",
    "            learning_rate: Learning rate for the Adam optimizer.\n",
    "            guide_strength: Factor to scale the guide network's output.\n",
    "            seed: PRNG seed for reproducibility.\n",
    "\n",
    "        Returns:\n",
    "            Optimized parameters.\n",
    "        \"\"\"\n",
    "        key = random.PRNGKey(seed)\n",
    "        guide_key, opt_key = random.split(key)\n",
    "\n",
    "        param_dim = init_params_opt.shape[0] # Dimension of parameters being optimized\n",
    "\n",
    "        # Initialize Guide Network (defined above within evaluate)\n",
    "        guide_model = GuideNN(hidden_dim=32, output_dim=param_dim)\n",
    "        # Use a dummy input of the correct shape for initialization\n",
    "        try:\n",
    "            guide_nn_params = guide_model.init(guide_key, jnp.zeros_like(init_params_opt))['params']\n",
    "        except Exception as e:\n",
    "             print(f\"Error initializing GuideNN: {e}. Input shape: {jnp.zeros_like(init_params_opt).shape}\")\n",
    "             raise # Re-raise the error after logging\n",
    "\n",
    "        # JIT the apply function for the guide network with static parameters\n",
    "        guide_apply_jit = jit(lambda p, nn_params: guide_model.apply({'params': nn_params}, p))\n",
    "\n",
    "        # Initialize Optimizer (e.g., Adam)\n",
    "        optimizer = optax.adam(learning_rate)\n",
    "        opt_state = optimizer.init(init_params_opt)\n",
    "\n",
    "        # Gradient function\n",
    "        value_and_grad_fn = jit(jax.value_and_grad(loss_fn_to_optimize))\n",
    "\n",
    "        current_params = init_params_opt\n",
    "\n",
    "        # Define the optimization step function\n",
    "        @jit\n",
    "        def step(params_step, opt_state_step, guide_nn_params_frozen):\n",
    "            loss_val, grads = value_and_grad_fn(params_step)\n",
    "            # Get guidance direction (using the frozen NN parameters)\n",
    "            guidance = guide_apply_jit(params_step, guide_nn_params_frozen)\n",
    "\n",
    "            # --- Core NGO Logic ---\n",
    "            # Combine gradient and guidance.\n",
    "            combined_update_direction = grads + guide_strength * guidance\n",
    "            # ---------------------\n",
    "\n",
    "            # Compute updates using the optimizer\n",
    "            updates, new_opt_state = optimizer.update(combined_update_direction, opt_state_step, params_step)\n",
    "            # Apply updates\n",
    "            new_params = optax.apply_updates(params_step, updates)\n",
    "            return new_params, new_opt_state, loss_val\n",
    "\n",
    "        # Optimization loop\n",
    "        print(f\"Starting NGO: {param_dim} params, {num_steps} steps, lr={learning_rate}, guide_strength={guide_strength}\")\n",
    "        for i in range(num_steps):\n",
    "            # Pass the static guide network parameters to the step function\n",
    "            current_params, opt_state, loss_val = step(current_params, opt_state, guide_nn_params)\n",
    "            if i % (num_steps // 10) == 0 or i == num_steps - 1:\n",
    "                 # Check for NaNs in loss or params during optimization\n",
    "                 if not jnp.isfinite(loss_val) or not jnp.all(jnp.isfinite(current_params)):\n",
    "                     print(f\"Warning: Non-finite value encountered at step {i+1}. Loss: {loss_val}, Params Finite: {jnp.all(jnp.isfinite(current_params))}\")\n",
    "                     # Optionally break or handle the divergence\n",
    "                     # break\n",
    "                 print(f\"  Step {i+1}/{num_steps}, Loss: {loss_val:.6f}\")\n",
    "\n",
    "\n",
    "        final_loss_check = loss_fn_to_optimize(current_params)\n",
    "        print(f\"NGO finished. Final Loss: {final_loss_check:.6f}\")\n",
    "        if not jnp.isfinite(final_loss_check):\n",
    "            print(f\"Warning: Final loss is non-finite: {final_loss_check}\")\n",
    "        if not jnp.all(jnp.isfinite(current_params)):\n",
    "            print(f\"Warning: Final parameters contain non-finite values.\")\n",
    "\n",
    "        return current_params'''\n",
    "    \n",
    "    #差分进化算法\n",
    "    '''def run_optimization(objective_fn, initial_guess):\n",
    "        \"\"\"处理任意维度的参数优化\"\"\"\n",
    "        # 高维参数（通常在敏感度分析中遇到）使用纯BFGS优化\n",
    "        if initial_guess.size > MAX_NPARAMS:\n",
    "            result = minimize(objective_fn, initial_guess,\n",
    "                              method='BFGS', options={'maxiter': 500})\n",
    "            if not result.success:\n",
    "                 print(f\"High-dim BFGS optimization failed: {result.message}\")\n",
    "                 # 可以考虑返回 initial_guess 或 raise error\n",
    "                 return initial_guess\n",
    "            return result.x\n",
    "        else:\n",
    "            # 低维参数（主优化流程）使用差分进化 + BFGS\n",
    "            # 定义参数边界\n",
    "            bounds = [(-1.0, 10.0)] * initial_guess.size\n",
    "\n",
    "            # 差分进化（DE）阶段\n",
    "            de_result = differential_evolution(objective_fn, bounds,\n",
    "                                               maxiter=100, # 可以调整迭代次数\n",
    "                                               popsize=15,   # 可以调整种群大小\n",
    "                                               tol=0.01,     # 可以调整容忍度\n",
    "                                               mutation=(0.5, 1), # 变异因子范围\n",
    "                                               recombination=0.7, # 交叉概率\n",
    "                                               seed=None)     # 可设置随机种子以便复现\n",
    "\n",
    "            if not de_result.success:\n",
    "                print(f\"Differential Evolution failed: {de_result.message}\")\n",
    "                # 如果DE失败，可以考虑直接使用初始猜测进行BFGS或返回DE的最佳结果\n",
    "                de_best_params = de_result.x\n",
    "            else:\n",
    "                de_best_params = de_result.x\n",
    "\n",
    "            # BFGS 精炼阶段\n",
    "            # 使用 DE 找到的最优参数作为 BFGS 的初始点\n",
    "            bfgs_result = minimize(objective_fn, jnp.array(de_best_params),\n",
    "                                   method='BFGS', options={'maxiter': 500}) # 增加BFGS迭代次数\n",
    "\n",
    "            if not bfgs_result.success:\n",
    "                 print(f\"BFGS refinement failed: {bfgs_result.message}\")\n",
    "                 # 如果BFGS失败，返回DE的结果\n",
    "                 return de_best_params\n",
    "\n",
    "            return bfgs_result.x'''\n",
    "    \n",
    "    #神经引导优化\n",
    "    '''def run_optimization(objective_fn, initial_guess):\n",
    "        \"\"\"使用简化的神经引导优化算法处理参数优化，提高计算速度\"\"\"\n",
    "        import flax.linen as nn\n",
    "        from flax.training import train_state\n",
    "        import optax\n",
    "        \n",
    "        param_dim = initial_guess.size\n",
    "        \n",
    "        # 简化网络结构 - 减少层数和神经元数量\n",
    "        class PredictorNetwork(nn.Module):\n",
    "            @nn.compact\n",
    "            def __call__(self, x):\n",
    "                x = nn.Dense(features=16)(x)\n",
    "                x = nn.relu(x)\n",
    "                x = nn.Dense(features=param_dim)(x)\n",
    "                return x\n",
    "        \n",
    "        # 初始化神经网络\n",
    "        predictor = PredictorNetwork()\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        params = predictor.init(key, jnp.ones((1, param_dim)))\n",
    "        \n",
    "        # 使用更快的优化器，增大学习率\n",
    "        optimizer = optax.adam(learning_rate=0.03)\n",
    "        state = train_state.TrainState.create(\n",
    "            apply_fn=predictor.apply,\n",
    "            params=params,\n",
    "            tx=optimizer\n",
    "        )\n",
    "        \n",
    "        # 使用JIT加速训练过程\n",
    "        @jit\n",
    "        def train_step(state, x, y):\n",
    "            def loss_fn(params):\n",
    "                pred = state.apply_fn(params, x)\n",
    "                return jnp.mean(jnp.square(pred - y))\n",
    "            \n",
    "            grad_fn = jax.value_and_grad(loss_fn)\n",
    "            loss, grads = grad_fn(state.params)\n",
    "            return state.apply_gradients(grads=grads), loss\n",
    "        \n",
    "        # 简化数据收集 - 减少样本数量\n",
    "        def collect_data(current_params, n_samples=10):  # 减少样本量\n",
    "            key = jax.random.PRNGKey(int(jnp.sum(current_params) * 1000) % 2**32)\n",
    "            \n",
    "            # 一次性生成所有随机噪声，避免循环\n",
    "            key, subkey = jax.random.split(key)\n",
    "            noises = jax.random.normal(subkey, shape=(n_samples, param_dim)) * 0.1\n",
    "            samples = current_params + noises\n",
    "            \n",
    "            # 批量计算梯度\n",
    "            grad_fn = jax.grad(objective_fn)\n",
    "            batch_grad_fn = jax.vmap(grad_fn)\n",
    "            gradients = batch_grad_fn(samples)\n",
    "            \n",
    "            return samples, gradients\n",
    "        \n",
    "        # 优化神经引导主循环 - 减少迭代次数\n",
    "        current_params = initial_guess\n",
    "        best_params = initial_guess\n",
    "        best_loss = objective_fn(initial_guess)\n",
    "        \n",
    "        # 减少总迭代次数\n",
    "        for epoch in range(15):  # 降低迭代次数\n",
    "            # 收集数据并训练预测器\n",
    "            x_data, y_data = collect_data(current_params)\n",
    "            \n",
    "            # 减少训练步数\n",
    "            for _ in range(30):  # 降低训练步数\n",
    "                state, loss = train_step(state, x_data, y_data)\n",
    "            \n",
    "            # 使用神经网络预测梯度\n",
    "            predicted_direction = -state.apply_fn(state.params, current_params.reshape(1, -1))[0]\n",
    "            normalized_direction = predicted_direction / (jnp.linalg.norm(predicted_direction) + 1e-8)\n",
    "            \n",
    "            # 简化线搜索 - 减少步长尝试\n",
    "            step_sizes = jnp.geomspace(0.02, 0.5, num=5)  # 减少步长选项\n",
    "            best_step = 0.1  # 默认步长\n",
    "            best_step_loss = objective_fn(current_params)\n",
    "            \n",
    "            for step_size in step_sizes:\n",
    "                new_params = current_params + step_size * normalized_direction\n",
    "                new_loss = objective_fn(new_params)\n",
    "                \n",
    "                if new_loss < best_step_loss:\n",
    "                    best_step = step_size\n",
    "                    best_step_loss = new_loss\n",
    "            \n",
    "            # 更新参数\n",
    "            current_params = current_params + best_step * normalized_direction\n",
    "            \n",
    "            # 降低BFGS频率\n",
    "            if epoch % 5 == 0:  # 每5次神经引导后进行一次BFGS优化\n",
    "                result = minimize(objective_fn, current_params, \n",
    "                                method='BFGS', \n",
    "                                options={'maxiter': 30})  # 减少最大迭代次数\n",
    "                current_params = result.x\n",
    "            \n",
    "            # 记录最佳结果\n",
    "            current_loss = objective_fn(current_params)\n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                best_params = current_params\n",
    "        \n",
    "        # 最后进行一次完整的BFGS优化\n",
    "        result = minimize(objective_fn, best_params, \n",
    "                        method='BFGS', \n",
    "                        options={'maxiter': 100})  # 减少最终优化的迭代次数\n",
    "        \n",
    "        return result.x'''\n",
    "\n",
    "\n",
    "    # 混沌优化\n",
    "    '''def run_optimization(objective_fn, initial_guess, key=random.PRNGKey(0)):\n",
    "\n",
    "        @jit\n",
    "        def bfgs_optimize(params):\n",
    "            result = minimize(objective_fn, params, method='BFGS', \n",
    "                            options={'maxiter': 200, 'gtol': 1e-6})\n",
    "            return result.x\n",
    "\n",
    "        DIM = initial_guess.size  # 10\n",
    "        N_POINTS = 20\n",
    "        MAX_ITER = 100\n",
    "        \n",
    "        # 初始化混沌点\n",
    "        keys = random.split(key, 2)\n",
    "        chaos = random.uniform(keys[0], (N_POINTS,)) * 0.9 + 0.1  # [0.1, 1]\n",
    "        r = 3.9  # 混沌区\n",
    "        bounds = jnp.array([-10., 10.])  # 参数范围\n",
    "        \n",
    "        def chaos_step(carry, _):\n",
    "            chaos, points, best_point, best_loss = carry\n",
    "            # 逻辑映射\n",
    "            chaos = r * chaos * (1 - chaos)\n",
    "            # 映射到参数空间\n",
    "            scaled = bounds[0] + (bounds[1] - bounds[0]) * chaos\n",
    "            new_points = points.at[:, 0].set(scaled)\n",
    "            for i in range(1, DIM):\n",
    "                chaos = r * chaos * (1 - chaos)\n",
    "                scaled = bounds[0] + (bounds[1] - bounds[0]) * chaos\n",
    "                new_points = new_points.at[:, i].set(scaled)\n",
    "            \n",
    "            # 评估损失\n",
    "            losses = vmap(objective_fn)(new_points)\n",
    "            min_idx = jnp.argmin(losses)\n",
    "            new_best_point = new_points[min_idx]\n",
    "            new_best_loss = losses[min_idx]\n",
    "            \n",
    "            # 更新最佳点\n",
    "            best_point = jnp.where(new_best_loss < best_loss, new_best_point, best_point)\n",
    "            best_loss = jnp.minimum(best_loss, new_best_loss)\n",
    "            return (chaos, new_points, best_point, best_loss), None\n",
    "        \n",
    "        # 运行混沌优化\n",
    "        points = initial_guess + random.normal(keys[1], (N_POINTS, DIM)) * 0.2\n",
    "        best_loss = objective_fn(initial_guess)\n",
    "        (chaos, points, best_point, best_loss), _ = jax.lax.scan(\n",
    "            chaos_step, (chaos, points, initial_guess, best_loss), None, length=MAX_ITER)\n",
    "        \n",
    "        # BFGS精修\n",
    "        return bfgs_optimize(best_point)'''\n",
    "    \n",
    "\n",
    "\n",
    "    # 敏感度分析模块\n",
    "    def calculate_sensitivities(opt_params, base_loss):\n",
    "        mask = 1 - jnp.eye(MAX_NPARAMS)\n",
    "        \n",
    "        @jit\n",
    "        def batch_loss(params_matrix):\n",
    "            # 确保输入是正确形状的矩阵\n",
    "            return vmap(loss_fn)(params_matrix * mask)\n",
    "\n",
    "        def sensitivity_objective(flat_params):\n",
    "            # 显式重塑参数形状\n",
    "            matrix_params = flat_params.reshape(MAX_NPARAMS, MAX_NPARAMS)\n",
    "            return jnp.sum(batch_loss(matrix_params))\n",
    "\n",
    "        try:\n",
    "            # 生成正确的初始参数（复制参数矩阵并应用掩码）\n",
    "            initial_flat = (opt_params * mask).flatten()\n",
    "            \n",
    "            # 执行优化（确保处理100个参数）\n",
    "            optimized_flat = run_optimization(sensitivity_objective, initial_flat)\n",
    "            optimized_matrix = optimized_flat.reshape(MAX_NPARAMS, MAX_NPARAMS)\n",
    "            \n",
    "            # 计算损失差异\n",
    "            losses = batch_loss(optimized_matrix)\n",
    "            print('优化损失:',losses)\n",
    "            raw_relative = (losses - base_loss) / base_loss\n",
    "            '''scaled = jnp.nan_to_num(raw_relative, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            \n",
    "            # 新增加的核心计算逻辑\n",
    "            x_d = jnp.median(scaled)\n",
    "            epsilon = 1e-8  # 防止除以零\n",
    "            adjusted_xd = x_d \n",
    "            \n",
    "            # 应用指数公式\n",
    "            relative_to_median = (scaled - x_d) / adjusted_xd\n",
    "            weights = jnp.exp(relative_to_median)\n",
    "            \n",
    "            # 保持符号（相对于中位数的变化方向）\n",
    "            final_sensitivities = weights * jnp.sign(scaled)'''\n",
    "\n",
    "            return {i: float(raw_relative[i]) for i in range(MAX_NPARAMS)}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Sensitivity analysis error: {str(e)}\")\n",
    "            return {i: 0.0 for i in range(MAX_NPARAMS)}\n",
    "\n",
    "\n",
    "    # 主流程\n",
    "    try:\n",
    "        optimized_params = run_optimization(loss_fn, params)\n",
    "        final_loss = loss_fn(optimized_params)\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not jnp.isfinite(final_loss):\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'params': optimized_params,\n",
    "        'loss': final_loss,\n",
    "        'sensitivities': calculate_sensitivities(optimized_params, final_loss)\n",
    "    }\n",
    "\n",
    "@jit\n",
    "def equation(q: jnp.array, q_t: jnp.array, params: jnp.array) -> jnp.array:\n",
    "    q1, q2 = q[...,0], q[...,1]\n",
    "    q1_t, q2_t = q_t[...,0], q_t[...,1]\n",
    "    \n",
    "    m1, m2, l1, l2, g = params[0], params[1], params[2], params[3], params[4]\n",
    "    c1, c2, F1, F2, k = params[5], params[6], params[7], params[8], params[9]\n",
    "    \n",
    "    T = 0.5*(m1*l1**2*q1_t**2 + m2*(l1**2*q1_t**2 + l2**2*q2_t**2 + 2*l1*l2*q1_t*q2_t*jnp.cos(q1-q2)))\n",
    "    V = -g*(m1*l1*jnp.cos(q1) + m2*(l1*jnp.cos(q1)+l2*jnp.cos(q2))) + F1*q1 + F2*q2\n",
    "    D = c1*q1_t + c2*q2_t\n",
    "    return T - V - D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "479221cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.81000000e+00  0.00000000e+00]\n",
      " [-9.80999997e+00 -5.53723182e-08]\n",
      " [-9.80999948e+00 -8.85956966e-07]\n",
      " ...\n",
      " [ 1.52940364e+01 -1.63022950e+01]\n",
      " [ 1.48602131e+01 -1.55387182e+01]\n",
      " [ 1.43807650e+01 -1.47005543e+01]]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "最终损失值 (MSE): {'params': Array([2.92511250e-01, 2.93263427e-01, 5.90578390e-02, 5.90171138e-02,\n",
      "       5.79794518e-01, 1.61471544e-08, 0.00000000e+00, 1.00000000e+00,\n",
      "       1.00000000e+00, 1.00000000e+00], dtype=float64), 'loss': Array(0.0003686, dtype=float64), 'sensitivities': {0: 48669615238.17453, 1: 408578.53417865915, 2: 391923.4500097751, 3: 408706.6548063402, 4: 1096953.6475976415, 5: -0.00039801818556416883, 6: -0.00039801818556416883, 7: -0.271558974297123, 8: -0.271558974297123, 9: -0.271558974297123}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 读取 CSV 文件并转换为 NumPy 数组\n",
    "data0 = pd.read_csv('./double_pendulum_data_with_energy.csv')#\n",
    "tae = data0.to_numpy()\n",
    "\n",
    "# 使用 JAX 的数组操作替换 PyTorch 的操作\n",
    "state = jnp.array(tae[:, :4], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "true_q_ddot = jnp.array(tae[:, 4:-1], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "energy = jnp.array(tae[:, -1], dtype=jnp.float64)\n",
    "print(true_q_ddot)\n",
    "# 将数据存储在字典中\n",
    "data = {\n",
    "    'inputs': state,\n",
    "    'outputs': true_q_ddot,  # 真实的加速度\n",
    "    'energy': energy\n",
    "}\n",
    "\n",
    "\n",
    "print(initial_params)\n",
    "# 评估并优化参数\n",
    "final_loss = evaluate(data, initial_params)\n",
    "print(\"最终损失值 (MSE):\", final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe91be1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
