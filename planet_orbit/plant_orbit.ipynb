{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0631aeee",
   "metadata": {},
   "source": [
    "# 测试一下损失函数中加入积分能不能行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config, grad, jit, vmap\n",
    "from pyswarm import pso\n",
    "from jax.scipy.optimize import minimize\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "def evaluate(data: dict, params=initial_params) -> dict:\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    n_dim = inputs.shape[1] // 2\n",
    "    q, q_t = inputs[:, :n_dim], inputs[:, n_dim:]\n",
    "    true_accelerations = outputs\n",
    "\n",
    "    @jit\n",
    "    def compute_acceleration(q, q_t, params):\n",
    "        def lag(q, q_t, params):\n",
    "            return equation(q, q_t, params)\n",
    "        hessian_q_t = jax.hessian(lag, 1)(q, q_t, params)\n",
    "        grad_q = jax.grad(lag, 0)(q, q_t, params)\n",
    "        jacobian_q_q_t = jax.jacobian(jax.grad(lag, 1), 0)(q, q_t, params)\n",
    "        return jnp.linalg.pinv(hessian_q_t) @ (grad_q - jacobian_q_q_t @ q_t)\n",
    "\n",
    "    batch_compute_acceleration = jit(vmap(compute_acceleration, (0, 0, None)))\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        pred = batch_compute_acceleration(q, q_t, params)\n",
    "        return jnp.mean(jnp.square(pred - true_accelerations))\n",
    "\n",
    "    def run_optimization(objective_fn, initial_guess):\n",
    "        if initial_guess.size > MAX_NPARAMS:\n",
    "            result = minimize(objective_fn, initial_guess,\n",
    "                            method='BFGS', options={'maxiter': 500})\n",
    "            return result.x\n",
    "        else:\n",
    "            def pso_wrapper(x):\n",
    "                return objective_fn(jnp.array(x))\n",
    "            \n",
    "            lb = [-1.0]*initial_guess.size\n",
    "            ub = [10.0]*initial_guess.size\n",
    "            \n",
    "            pso_params, _ = pso(pso_wrapper, lb, ub, \n",
    "                            swarmsize=30, maxiter=200)\n",
    "            \n",
    "            result = minimize(objective_fn, jnp.array(pso_params),\n",
    "                            method='BFGS', options={'maxiter': 500})\n",
    "            return result.x\n",
    "\n",
    "\n",
    "    # 敏感度分析模块\n",
    "    def calculate_sensitivities(opt_params, base_loss):\n",
    "        mask = 1 - jnp.eye(MAX_NPARAMS)\n",
    "        \n",
    "        @jit\n",
    "        def batch_loss(params_matrix):\n",
    "            return vmap(loss_fn)(params_matrix * mask)\n",
    "\n",
    "        def sensitivity_objective(flat_params):\n",
    "            matrix_params = flat_params.reshape(MAX_NPARAMS, MAX_NPARAMS)\n",
    "            return jnp.sum(batch_loss(matrix_params))\n",
    "\n",
    "        try:\n",
    "            initial_flat = (opt_params * mask).flatten()\n",
    "            optimized_flat = run_optimization(sensitivity_objective, initial_flat)\n",
    "            optimized_matrix = optimized_flat.reshape(MAX_NPARAMS, MAX_NPARAMS)\n",
    "            losses = batch_loss(optimized_matrix)\n",
    "            relative_loss = (losses - base_loss) / base_loss\n",
    "            return {i: float(jnp.nan_to_num(relative_loss[i])) for i in range(MAX_NPARAMS)}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Sensitivity analysis error: {str(e)}\")\n",
    "            return {i: 0.0 for i in range(MAX_NPARAMS)}\n",
    "\n",
    "\n",
    "    # 主流程\n",
    "    try:\n",
    "        optimized_params = run_optimization(loss_fn, params)\n",
    "        final_loss = loss_fn(optimized_params)\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not jnp.isfinite(final_loss):\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'params': optimized_params,\n",
    "        'loss': final_loss,\n",
    "        'sensitivities': calculate_sensitivities(optimized_params, final_loss)\n",
    "    }\n",
    "\n",
    "@jit\n",
    "def equation(q: jnp.array, q_t: jnp.array, params: jnp.array) -> jnp.array:\n",
    "    q1, q2 = q[...,0], q[...,1]\n",
    "    q1_t, q2_t = q_t[...,0], q_t[...,1]\n",
    "    \n",
    "    T = params[0]*(q1_t**2 + q2_t**2) \n",
    "    V = - params[1]/jnp.sqrt(q1**2 + q2**2)\n",
    "\n",
    "    return T - V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85544db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "import pandas as pd\n",
    "# 读取 CSV 文件并转换为 NumPy 数组\n",
    "data0 = pd.read_csv('./xy_orbit.csv')#\n",
    "tae = data0.to_numpy()\n",
    "\n",
    "# 使用 JAX 的数组操作替换 PyTorch 的操作\n",
    "t = jnp.array(tae[:, 0], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "state = jnp.array(tae[:, 1:], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "print(state)\n",
    "print(t)\n",
    "# 将数据存储在字典中\n",
    "data = {\n",
    "    'times': t,\n",
    "    'states': state\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(initial_params)\n",
    "# 评估并优化参数\n",
    "final_loss = evaluate(data, initial_params)\n",
    "print(\"最终损失值 (MSE):\", final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config, grad, jit, vmap\n",
    "from jax.experimental.ode import odeint\n",
    "from pyswarm import pso\n",
    "from jax.scipy.optimize import minimize\n",
    "import traceback\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# --- Global Constants ---\n",
    "# Define MAX_NPARAMS, used in run_optimization logic\n",
    "# Adjust this value based on your system's typical parameter count\n",
    "# or the threshold where you want to switch optimization methods.\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "def evaluate(data: dict, params: jnp.array) -> dict:\n",
    "    times = data['times']\n",
    "    target_states = data['states']\n",
    "    initial_state = target_states[0]\n",
    "    n_dim = target_states.shape[1] // 2\n",
    "\n",
    "    def compute_acceleration(q, q_t, params_acc):\n",
    "        def lag(q_lag, q_t_lag, params_lag):\n",
    "            return equation(q_lag, q_t_lag, params_lag)\n",
    "        try:\n",
    "            hessian_q_t = jax.hessian(lag, 1)(q, q_t, params_acc)\n",
    "            grad_q = jax.grad(lag, 0)(q, q_t, params_acc)\n",
    "            jacobian_q_q_t = jax.jacobian(jax.grad(lag, 1), 0)(q, q_t, params_acc)\n",
    "            inv_hessian = jnp.linalg.pinv(hessian_q_t, rtol=1e-10)\n",
    "            accel = inv_hessian @ (grad_q - jacobian_q_q_t @ q_t)\n",
    "        except jnp.linalg.LinAlgError:\n",
    "            print(f\"Warning: Singular matrix in acceleration computation.\")\n",
    "            accel = jnp.full_like(q, jnp.nan)\n",
    "        return accel\n",
    "\n",
    "    @jit\n",
    "    def dynamics_func(y, t, params_dyn):\n",
    "        q = y[:n_dim]\n",
    "        q_t = y[n_dim:]\n",
    "        q_tt = compute_acceleration(q, q_t, params_dyn)\n",
    "        return jnp.concatenate([q_t, q_tt])\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(params_loss):\n",
    "        pred_states = odeint(dynamics_func, initial_state, times, params_loss, rtol=1e-6, atol=1e-6)\n",
    "        loss = jnp.mean(jnp.square(pred_states - target_states))\n",
    "        return loss\n",
    "\n",
    "    def run_optimization(objective_fn, initial_guess):\n",
    "        n_params_opt = initial_guess.size\n",
    "        print(f\"Running optimization for {n_params_opt} parameters.\")\n",
    "\n",
    "        if n_params_opt > MAX_NPARAMS:\n",
    "            print(\"Using BFGS optimizer.\")\n",
    "            try:\n",
    "                result = minimize(objective_fn, initial_guess, method='BFGS', options={'maxiter': 500})\n",
    "                if not result.success: \n",
    "                    print(f\"Warning: BFGS optimization did not converge. Status: {result.status}\")\n",
    "                return result.x\n",
    "            except Exception as e:\n",
    "                print(f\"BFGS optimization failed: {e}\")\n",
    "                # 返回初始猜测值，避免完全失败\n",
    "                return initial_guess\n",
    "        else:\n",
    "            print(\"Using PSO + BFGS optimizer.\")\n",
    "            def pso_wrapper(x):\n",
    "                return float(objective_fn(jnp.array(x, dtype=initial_guess.dtype)))\n",
    "\n",
    "            # 使用合理的搜索范围\n",
    "            lb = jnp.full(n_params_opt, -1.0, dtype=initial_guess.dtype).tolist()\n",
    "            ub = jnp.full(n_params_opt, 10.0, dtype=initial_guess.dtype).tolist()\n",
    "\n",
    "            try:\n",
    "                print(\"Starting PSO optimization...\")\n",
    "                pso_params, pso_loss = pso(pso_wrapper, lb, ub, swarmsize=50, maxiter=200, debug=True)\n",
    "                pso_params = jnp.array(pso_params, dtype=initial_guess.dtype)\n",
    "                print(f\"PSO finished with loss: {pso_loss}. Refining with BFGS...\")\n",
    "                \n",
    "                try:\n",
    "                    result = minimize(objective_fn, pso_params, method='BFGS', options={'maxiter': 500})\n",
    "                    if not result.success:\n",
    "                        print(f\"Warning: BFGS (post-PSO) did not converge. Status: {result.status}\")\n",
    "                    print(f\"BFGS refinement complete. Final loss: {objective_fn(result.x)}\")\n",
    "                    return result.x\n",
    "                except Exception as e:\n",
    "                    print(f\"BFGS refinement failed: {e}, returning PSO results.\")\n",
    "                    return pso_params\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"PSO optimization failed: {e}. Falling back to BFGS.\")\n",
    "                try:\n",
    "                    result = minimize(objective_fn, initial_guess, method='BFGS', options={'maxiter': 500})\n",
    "                    if not result.success:\n",
    "                        print(f\"Warning: Fallback BFGS optimization did not converge. Status: {result.status}\")\n",
    "                    return result.x\n",
    "                except Exception as e:\n",
    "                    print(f\"Fallback BFGS also failed: {e}\")\n",
    "                    return initial_guess  # 返回初始猜测值，避免完全失败\n",
    "\n",
    "    # --- Sensitivity Analysis (Corrected Logic) ---\n",
    "    def calculate_sensitivities(opt_params, base_loss):\n",
    "        n_params_sens = opt_params.size\n",
    "        if n_params_sens == 0:\n",
    "            print(\"Warning: Skipping sensitivity analysis. No parameters found.\")\n",
    "            return {}\n",
    "\n",
    "        print(f\"Calculating sensitivities for {n_params_sens} parameters (re-optimization method)...\")\n",
    "\n",
    "        # Use vmap for batch loss calculation\n",
    "        @jit\n",
    "        def batch_loss(params_matrix):\n",
    "            # Define the single evaluation loss function again, taking params only\n",
    "            def single_loss_eval(p):\n",
    "                pred_states = odeint(dynamics_func, initial_state, times, p, rtol=1e-6, atol=1e-6)\n",
    "                pred_states = jnp.nan_to_num(pred_states, nan=jnp.inf)\n",
    "                return jnp.mean(jnp.square(pred_states - target_states))\n",
    "            # Ensure vmap operates correctly on the parameters dimension\n",
    "            return vmap(single_loss_eval)(params_matrix)\n",
    "\n",
    "        try:\n",
    "            # Create mask based on the actual number of optimized parameters\n",
    "            mask = 1.0 - jnp.eye(n_params_sens, dtype=opt_params.dtype)\n",
    "            # Create matrix where row 'i' has param 'i' = 0\n",
    "            # Broadcast opt_params (shape [n]) with mask (shape [n, n])\n",
    "            masked_params = jnp.expand_dims(opt_params, 0) * mask\n",
    "\n",
    "            # Calculate loss for each zeroed case\n",
    "            losses_if_zero = batch_loss(masked_params)\n",
    "\n",
    "            # Calculate relative loss change, adding epsilon for numerical stability\n",
    "            relative_loss_change = (losses_if_zero - base_loss) / (base_loss + 1e-15)\n",
    "\n",
    "            # Build sensitivity dictionary using the actual number of parameters\n",
    "            sensitivities = {i: float(jnp.nan_to_num(relative_loss_change[i])) for i in range(n_params_sens)}\n",
    "\n",
    "            print(\"Sensitivities calculated based on zeroing each parameter.\")\n",
    "            return sensitivities\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Sensitivity analysis error: {str(e)}\")\n",
    "            print(traceback.format_exc())\n",
    "            # Return zero sensitivities for all actual parameters on error\n",
    "            return {i: 0.0 for i in range(n_params_sens)}\n",
    "\n",
    "    # --- Main Execution Flow ---\n",
    "    print(\"Starting parameter evaluation...\")\n",
    "    try:\n",
    "        # Ensure initial params have a float dtype for optimization\n",
    "        initial_params_float = jnp.asarray(params, dtype=jnp.float64)\n",
    "        optimized_params = run_optimization(loss_fn, initial_params_float)\n",
    "        print(f\"Optimization finished. Optimal parameters: {optimized_params}\")\n",
    "        # Ensure optimized params are used with correct dtype for final loss calculation\n",
    "        final_loss = loss_fn(jnp.asarray(optimized_params, dtype=jnp.float64))\n",
    "        print(f\"Final loss: {final_loss}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization failed: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        return None # Indicate failure\n",
    "\n",
    "    # Check if optimization produced valid results before sensitivity analysis\n",
    "    if optimized_params is None or not jnp.isfinite(final_loss) or final_loss == jnp.inf:\n",
    "        print(f\"Optimization resulted in invalid parameters or non-finite loss: {final_loss}. Skipping sensitivity analysis.\")\n",
    "        # Return results without sensitivities if optimization failed or loss is invalid\n",
    "        return {\n",
    "            'params': optimized_params, # Could be None\n",
    "            'loss': float(final_loss) if jnp.isfinite(final_loss) else float('inf'),\n",
    "            'sensitivities': {} # Return empty sensitivities\n",
    "        }\n",
    "\n",
    "    # Proceed with sensitivity analysis only if optimization was successful\n",
    "    sensitivities = calculate_sensitivities(optimized_params, final_loss)\n",
    "\n",
    "    return {\n",
    "        'params': optimized_params,\n",
    "        'loss': float(final_loss), # Convert scalar JAX array to float\n",
    "        'sensitivities': sensitivities\n",
    "    }\n",
    "\n",
    "# Dummy equation function if not provided elsewhere\n",
    "@jit\n",
    "def equation(q: jnp.array, q_t: jnp.array, params: jnp.array) -> jnp.array:\n",
    "\n",
    "    q1, q2 = q[..., 0], q[..., 1]\n",
    "    q1_t, q2_t = q_t[..., 0], q_t[..., 1]\n",
    "\n",
    "    T = 0.5 * params[0] * (q1_t**2 + q2_t**2) # Corrected Kinetic Energy form (factor of 0.5)\n",
    "    V = - params[1] / jnp.sqrt(q1**2 + q2**2 + 1e-8) # Potential Energy (add epsilon for stability near origin)\n",
    "\n",
    "    return T - V\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed951dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "\n",
    "data0 = pd.read_csv('./xy_orbit.csv') # \n",
    "tae = data0.to_numpy()\n",
    "\n",
    "# 使用 JAX 的数组操作\n",
    "t = jnp.array(tae[:, 0], dtype=jnp.float64)/100\n",
    "state = jnp.array(tae[:, 1:], dtype=jnp.float64)/1e10 # \n",
    "\n",
    "data = {\n",
    "    'times': t,\n",
    "    'states': state # Use the scaled states\n",
    "}\n",
    "\n",
    "initial_params = jnp.array([1.0] * 10, dtype=jnp.float64) \n",
    "\n",
    "print(\"Initial Parameters (for scaled system):\", initial_params)\n",
    "\n",
    "results = evaluate(data, initial_params) # Pass scaled data and initial guess\n",
    "\n",
    "# --- Output Results ---\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "if results:\n",
    "    print(f\"Optimized Parameters (for scaled system): {results['params']}\")\n",
    "    print(f\"Final Loss (MSE, scaled units): {results['loss']}\")\n",
    "    print(f\"Sensitivities (scaled units): {results['sensitivities']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac24238",
   "metadata": {},
   "source": [
    "# 问题很大，我们首先来验证一下计算是否可行。\n",
    "我们首先验证计算的加速度是否正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b576c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: [1. 2. 3. 4.]\n",
      "Acceleration: [ 3.          4.         -0.08944272 -0.17888544]\n",
      "acc [-0.08944272 -0.17888544]\n"
     ]
    }
   ],
   "source": [
    "# 设置一个数据初始数据\n",
    "from jax import random\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "\n",
    "#初始状态\n",
    "initial = jnp.array([1.0, 2.0 , 3.0, 4.0])\n",
    "\n",
    "#对应加速度\n",
    "def dynamic(state):\n",
    "    x = state[0]\n",
    "    y = state[1]\n",
    "    x_tt = -x/(x**2 + y**2)**(3/2)\n",
    "    y_tt = -y/(x**2 + y**2)**(3/2)\n",
    "    return jnp.array([state[2],state[3],x_tt, y_tt])\n",
    "\n",
    "acceleration = dynamic(initial)\n",
    "\n",
    "print(\"Initial State:\", initial)\n",
    "print(\"Acceleration:\", acceleration)\n",
    "\n",
    "#对应的方程\n",
    "def equation(q: jnp.array, q_t: jnp.array, params: jnp.array) -> jnp.array:\n",
    "\n",
    "    q1, q2 = q[..., 0], q[..., 1]\n",
    "    q1_t, q2_t = q_t[..., 0], q_t[..., 1]\n",
    "\n",
    "    T = params[0] *(q1_t**2 + q2_t**2) # Corrected Kinetic Energy form (factor of 0.5)\n",
    "    V = - params[1] / jnp.sqrt(q1**2 + q2**2) # Potential Energy (add epsilon for stability near origin)\n",
    "\n",
    "    return T - V\n",
    "\n",
    "def compute_acceleration(q, q_t, params):\n",
    "    def lag(q, q_t, params):\n",
    "        return equation(q, q_t, params)\n",
    "    hessian_q_t = jax.hessian(lag, 1)(q, q_t, params)\n",
    "    grad_q = jax.grad(lag, 0)(q, q_t, params)\n",
    "    jacobian_q_q_t = jax.jacobian(jax.grad(lag, 1), 0)(q, q_t, params)\n",
    "    q_tt = jnp.linalg.pinv(hessian_q_t) @ (grad_q - jacobian_q_q_t @ q_t)\n",
    "    return jnp.concatenate([q_t, q_tt])\n",
    "\n",
    "params = jnp.array([0.5, 1.0])\n",
    "\n",
    "print('acc',compute_acceleration(initial[:2],initial[2:], params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31590f78",
   "metadata": {},
   "source": [
    "# 检验完毕，能够正确计算加速度。\n",
    "接着我们开始积分，看看其能否使用自动加速度计算的式子积分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.experimental.ode import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t = jnp.linspace(0, 10, 100)  # 时间点，从 0 到 10，共 100 个点\n",
    "y0 = initial\n",
    "\n",
    "# 使用 odeint 求解\n",
    "y1 = odeint(dynamic, y0, t)\n",
    "y2 = odeint(compute_acceleration, y0, t, params)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(t, y1[:, 0], label='x1(t) (Oscillator 1)', color='blue')\n",
    "plt.plot(t, y2[:, 1], label='x2(t) (Oscillator 2)', color='orange')\n",
    "plt.xlabel('Time (t)')\n",
    "plt.ylabel('Displacement')\n",
    "plt.title('Coupled Harmonic Oscillators')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
