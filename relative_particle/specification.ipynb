{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be08214",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the mathematical function skeleton that represents lagrangian in a physical system, given data on generalized coordinate and generalized velocity.\n",
    "Tips:You may only use no more than 10 parameters,Under limited parameter conditions, you can incorporate nonlinear terms rather than continuously adding new ones.\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config, jit, vmap\n",
    "from pyswarm import pso\n",
    "from jax.scipy.optimize import minimize\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = [jnp.array(1.0)]*MAX_NPARAMS\n",
    "\n",
    "def evaluate(data: dict, params=initial_params) -> float:\n",
    "\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    n_dim = inputs.shape[1] // 2\n",
    "    q = inputs[:, :n_dim]\n",
    "    q_t = inputs[:, n_dim:]\n",
    "    true_accelerations = outputs\n",
    "\n",
    "    @jit\n",
    "    def compute_acceleration(q, q_t, params):\n",
    "\n",
    "        @jit\n",
    "        def lag(q_single, q_t_single, params):\n",
    "            result = equation(q_single, q_t_single, params)\n",
    "            return jnp.sum(result)\n",
    "\n",
    "        hessian_q_t = jax.hessian(lag, 1)(q, q_t, params)\n",
    "        grad_q = jax.grad(lag, 0)(q, q_t, params)\n",
    "        jacobian_q_q_t = jax.jacobian(jax.grad(lag, 1), 0)(q, q_t, params)\n",
    "        q_tt = jnp.linalg.pinv(hessian_q_t) @ (grad_q - jacobian_q_q_t @ q_t)\n",
    "        return q_tt\n",
    "\n",
    "    batch_compute_acceleration = jit(vmap(compute_acceleration, in_axes=(0, 0, None)))\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        predicted_accelerations = batch_compute_acceleration(q, q_t, params)\n",
    "        return jnp.mean(jnp.square(predicted_accelerations - true_accelerations))\n",
    "\n",
    "\n",
    "    def objective(params):\n",
    "        try:\n",
    "            params = jnp.array(params)\n",
    "            loss_value = loss_fn(params)\n",
    "            return float(loss_value)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in objective function: {e}\")\n",
    "\n",
    "            return 1e10\n",
    "\n",
    "\n",
    "    lb = [-10.0] * len(initial_params)\n",
    "    ub = [10.0] * len(initial_params)\n",
    "\n",
    "\n",
    "    optimized_params, optimized_loss = pso(objective, lb, ub, swarmsize=40, maxiter=500)\n",
    "\n",
    "    print(\"pso Optimized parameters:\", optimized_params)\n",
    "    print(\"pso Optimized loss:\", optimized_loss)\n",
    "\n",
    "    loss_partial = jit(loss_fn)\n",
    "    result = minimize(loss_partial, optimized_params, method='BFGS', options={'maxiter': 1000})\n",
    "    optimized_params = result.x\n",
    "    loss = result.fun\n",
    "\n",
    "    if jnp.isnan(loss) or jnp.isinf(loss):\n",
    "        return None\n",
    "    else:\n",
    "        print(optimized_params)\n",
    "        return -loss.item()\n",
    "\n",
    "@jit\n",
    "def equation(q: jnp.array, q_t: jnp.array, params: jnp.array):\n",
    "    \"\"\" Mathematical function for lagrangian in a one-dimensional physical system\n",
    "    Args:\n",
    "        q (jnp.array): observation of current generalized coordinate.\n",
    "        q_t (jnp.array): observation of generalized velocity.\n",
    "        params (jnp.array): List of numeric constants or parameters to be optimized.\n",
    "    Returns:\n",
    "        jnp.array: lagrangian as the result of applying the mathematical function to the inputs.\n",
    "    \"\"\"\n",
    "    # the input\n",
    "    q = q[...,0]\n",
    "    q_t = q_t[...,0]\n",
    "\n",
    "    # the energy\n",
    "    T = params[0]*jnp.power(1-q_t**2,-0.5)\n",
    "\n",
    "    # the potential\n",
    "    V = -params[3]*q + params[2]\n",
    "\n",
    "    return T-V\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db258fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config, grad, jit, vmap\n",
    "from pyswarm import pso\n",
    "from jax.scipy.optimize import minimize\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "def evaluate(data: dict, params=initial_params) -> dict:\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    n_dim = inputs.shape[1] // 2\n",
    "    q, q_t = inputs[:, :n_dim], inputs[:, n_dim:]\n",
    "    true_accelerations = outputs\n",
    "\n",
    "    @jit\n",
    "    def compute_acceleration(q, q_t, params):\n",
    "        def lag(q, q_t, params):\n",
    "            return equation(q, q_t, params)\n",
    "        hessian_q_t = jax.hessian(lag, 1)(q, q_t, params)\n",
    "        grad_q = jax.grad(lag, 0)(q, q_t, params)\n",
    "        jacobian_q_q_t = jax.jacobian(jax.grad(lag, 1), 0)(q, q_t, params)\n",
    "        return jnp.linalg.pinv(hessian_q_t) @ (grad_q - jacobian_q_q_t @ q_t)\n",
    "\n",
    "    batch_compute_acceleration = jit(vmap(compute_acceleration, (0, 0, None)))\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        pred = batch_compute_acceleration(q, q_t, params)\n",
    "        return jnp.mean(jnp.square(pred - true_accelerations))\n",
    "\n",
    "    def run_optimization(objective_fn, initial_guess):\n",
    "        if initial_guess.size > MAX_NPARAMS:\n",
    "            result = minimize(objective_fn, initial_guess,\n",
    "                            method='BFGS', options={'maxiter': 500})\n",
    "            return result.x\n",
    "        else:\n",
    "            def pso_wrapper(x):\n",
    "                return objective_fn(jnp.array(x))\n",
    "            \n",
    "            lb = [-1.0]*initial_guess.size\n",
    "            ub = [10.0]*initial_guess.size\n",
    "            \n",
    "            pso_params, _ = pso(pso_wrapper, lb, ub, \n",
    "                            swarmsize=30, maxiter=200)\n",
    "            \n",
    "            result = minimize(objective_fn, jnp.array(pso_params),\n",
    "                            method='BFGS', options={'maxiter': 500})\n",
    "            return result.x\n",
    "\n",
    "    def calculate_sensitivities(opt_params, base_loss):\n",
    "        mask = 1 - jnp.eye(MAX_NPARAMS)\n",
    "        \n",
    "        @jit\n",
    "        def batch_loss(params_matrix):\n",
    "            return vmap(loss_fn)(params_matrix * mask)\n",
    "\n",
    "        def sensitivity_objective(flat_params):\n",
    "            matrix_params = flat_params.reshape(MAX_NPARAMS, MAX_NPARAMS)\n",
    "            return jnp.sum(batch_loss(matrix_params))\n",
    "\n",
    "        try:\n",
    "            initial_flat = (opt_params * mask).flatten()\n",
    "            optimized_flat = run_optimization(sensitivity_objective, initial_flat)\n",
    "            optimized_matrix = optimized_flat.reshape(MAX_NPARAMS, MAX_NPARAMS)\n",
    "            losses = batch_loss(optimized_matrix)\n",
    "            return jnp.log2(losses / base_loss)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Sensitivity analysis error: {str(e)}\")\n",
    "            return jnp.zeros(MAX_NPARAMS)\n",
    "\n",
    "    try:\n",
    "        optimized_params = run_optimization(loss_fn, params)\n",
    "        final_loss = loss_fn(optimized_params)\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not jnp.isfinite(final_loss):\n",
    "        return None\n",
    "    \n",
    "    sensitivities = calculate_sensitivities(optimized_params, final_loss)\n",
    "    sensitivity_dict = {f\"sensitive of params[{i}]\": round(float(sensitivities[i]), 4) \n",
    "                       for i in range(len(sensitivities))}\n",
    "\n",
    "    return {\n",
    "        'params': optimized_params.tolist(),\n",
    "        'loss': - float(final_loss),\n",
    "        'sensitivities': sensitivity_dict\n",
    "    }\n",
    "\n",
    "@jit\n",
    "def equation(q: jnp.array, q_t: jnp.array, params: jnp.array):\n",
    "    \"\"\" Mathematical function for lagrangian in a one-dimensional physical system\n",
    "    Args:\n",
    "        q (jnp.array): observation of current generalized coordinate.\n",
    "        q_t (jnp.array): observation of generalized velocity.\n",
    "        params (jnp.array): List of numeric constants or parameters to be optimized.\n",
    "    Returns:\n",
    "        jnp.array: lagrangian as the result of applying the mathematical function to the inputs.\n",
    "    \"\"\"\n",
    "    # the input\n",
    "    q = q[...,0]\n",
    "    q_t = q_t[...,0]\n",
    "\n",
    "    # the energy\n",
    "    T = params[0]*jnp.power(1-q_t**2,-0.5)\n",
    "\n",
    "    # the potential\n",
    "    V = -params[3]*q + params[2]\n",
    "\n",
    "    return T-V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e0832e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping search: Swarm best objective change less than 1e-08\n",
      "{'params': [0.7727671435875787, 0.9889311251799003, 6.853301227843082, 7.5731173436548405, 5.214923455301921, 1.305482912520252, 5.780432597713505, 0.4861455058073598, 3.049734631454361, 2.537328650951322], 'loss': -3.6944959872168676e-14, 'sensitivities': {'sensitive of params[0]': 46.8886, 'sensitive of params[1]': 0.0, 'sensitive of params[2]': 0.0, 'sensitive of params[3]': 46.8886, 'sensitive of params[4]': 0.0, 'sensitive of params[5]': 0.0, 'sensitive of params[6]': 0.0, 'sensitive of params[7]': 0.0, 'sensitive of params[8]': 0.0, 'sensitive of params[9]': 0.0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ninputs, outputs = data['inputs'], data['outputs']\\nn_dim = inputs.shape[1] // 2\\nq = inputs[:, :n_dim]\\nq_t = inputs[:, n_dim:]\\ntrue_accelerations = outputs\\nprint(q[:5],q_t[:5],true_accelerations[:5])\\nprint(g)\\n\\n\\ndd = qdotdot(q, q_t, g)[1]\\nloss = jnp.mean((dd - true_accelerations)**2)  # 计算损失\\nprint('Loss:', loss)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "from jax.experimental.ode import odeint\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "\n",
    "def qdotdot(q, q_t, conditionals):\n",
    "    g = conditionals\n",
    "    \n",
    "    q_tt = (\n",
    "        g * (1 - q_t**2)**(5./2) / \n",
    "        (1 + 2 * q_t**2)\n",
    "    )\n",
    "    \n",
    "    return q_t, q_tt\n",
    "\n",
    "\n",
    "# 读取 CSV 文件并转换为 NumPy 数组\n",
    "data0 = pd.read_csv('./relative_particle.csv')#\n",
    "tae = data0.to_numpy()\n",
    "\n",
    "# 使用 JAX 的数组操作替换 PyTorch 的操作\n",
    "state = jnp.array(tae[:, :2], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "g = jnp.array(tae[:, 2], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "true_q_ddot = jnp.array(tae[:, 3:4], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "# 将数据存储在字典中\n",
    "data = {\n",
    "    'inputs': state,\n",
    "    'outputs': true_q_ddot,  # 真实的加速度\n",
    "}\n",
    "\n",
    "f = evaluate(data)\n",
    "print(f)\n",
    "\n",
    "'''\n",
    "inputs, outputs = data['inputs'], data['outputs']\n",
    "n_dim = inputs.shape[1] // 2\n",
    "q = inputs[:, :n_dim]\n",
    "q_t = inputs[:, n_dim:]\n",
    "true_accelerations = outputs\n",
    "print(q[:5],q_t[:5],true_accelerations[:5])\n",
    "print(g)\n",
    "\n",
    "\n",
    "dd = qdotdot(q, q_t, g)[1]\n",
    "loss = jnp.mean((dd - true_accelerations)**2)  # 计算损失\n",
    "print('Loss:', loss)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936998a9",
   "metadata": {},
   "source": [
    "# 下面这个优化虽然好，但是太慢，还存在的一个问题是参数编号需要重新对上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc354ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the mathematical function skeleton that represents acceleration in a damped nonlinear oscillator system with driving force, given data on time, position, and velocity. \n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap, config\n",
    "import pyswarms as ps\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "config.update(\"jax_platform_name\", \"cpu\")\n",
    "import numpy as np\n",
    "import jax.random as random\n",
    "from jaxopt import LBFGS\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def evaluate(data: dict, params=initial_params) -> dict:\n",
    "\n",
    "    master_key = random.PRNGKey(0)  \n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    n_dim = inputs.shape[1] // 2\n",
    "    q = inputs[:, :n_dim]\n",
    "    q_t = inputs[:, n_dim:]\n",
    "    true_accelerations = outputs\n",
    "\n",
    "    @jit\n",
    "    def compute_acceleration(q, q_t, params):\n",
    "\n",
    "        @jit\n",
    "        def lag(q_single, q_t_single, params):\n",
    "            result = equation(q_single, q_t_single, params)\n",
    "            return jnp.sum(result)\n",
    "\n",
    "        hessian_q_t = jax.hessian(lag, 1)(q, q_t, params)\n",
    "        grad_q = jax.grad(lag, 0)(q, q_t, params)\n",
    "        jacobian_q_q_t = jax.jacobian(jax.grad(lag, 1), 0)(q, q_t, params)\n",
    "        q_tt = jnp.linalg.pinv(hessian_q_t) @ (grad_q - jacobian_q_q_t @ q_t)\n",
    "        return q_tt\n",
    "\n",
    "    batch_compute_acceleration = jit(vmap(compute_acceleration, in_axes=(0, 0, None)))\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        predicted_accelerations = batch_compute_acceleration(q, q_t, params)\n",
    "        return jnp.mean(jnp.square(predicted_accelerations - true_accelerations))\n",
    "    \n",
    "    def run_optimization(objective_fn, initial_guess, key, num_pso_runs=3, pso_iters=100, swarmsize=50, gg = False):\n",
    "        print(f\"Initial guess size: {initial_guess.size}, MAX_NPARAMS: {MAX_NPARAMS}\")\n",
    "\n",
    "        grads   = jax.grad(equation,2)(q[1],q_t[1],params)           # (MAX_NPARAMS,)           \n",
    "        active  = jnp.where(jnp.abs(grads) > 0)[0]        # e.g. [0,1,2,3,4,5,6,7]\n",
    "        n_params = active.shape[0]                        # 只保留活跃参数\n",
    "        print(n_params)\n",
    "        \n",
    "        solver = LBFGS(objective_fn, maxiter=100, tol=1e-8)\n",
    "\n",
    "        if  gg == True:\n",
    "            result =  solver.run(initial_guess)\n",
    "            return result.params\n",
    "        else:\n",
    "            @jit\n",
    "            def pso_objective_wrapper(particles_matrix):\n",
    "                return vmap(objective_fn)(particles_matrix)\n",
    "\n",
    "            min_bound_np = np.full(n_params, -10.0, dtype=np.float64)\n",
    "            max_bound_np = np.full(n_params, 10.0, dtype=np.float64)\n",
    "            bounds = (min_bound_np, max_bound_np)\n",
    "\n",
    "            options = {'c1': 1.49445, 'c2': 1.49445, 'w': 0.729}\n",
    "\n",
    "            best_pso_params = None\n",
    "            best_pso_loss = jnp.inf\n",
    "\n",
    "            current_key = key # Use the passed-in key\n",
    "\n",
    "            print(f\"Running pyswarms PSO {num_pso_runs} times with unique initial swarms...\")\n",
    "            for i in range(num_pso_runs):\n",
    "                current_key, subkey = random.split(current_key)\n",
    "                min_bound_jnp = jnp.full(n_params, -10.0, dtype=jnp.float64)\n",
    "                max_bound_jnp = jnp.full(n_params, 10.0, dtype=jnp.float64)\n",
    "                init_pos_jax = random.uniform(subkey,\n",
    "                                              shape=(swarmsize, n_params),\n",
    "                                              dtype=jnp.float64,\n",
    "                                              minval=min_bound_jnp,\n",
    "                                              maxval=max_bound_jnp)\n",
    "                # Convert to NumPy array for pyswarms\n",
    "                init_pos_np = np.array(init_pos_jax)\n",
    "\n",
    "                print(f\"  PSO Run {i+1}/{num_pso_runs} (using JAX key split for init_pos)\")\n",
    "                optimizer = ps.single.GlobalBestPSO(n_particles=swarmsize,\n",
    "                                                     dimensions=n_params,\n",
    "                                                     options=options,\n",
    "                                                     bounds=bounds,\n",
    "                                                     # Pass the generated initial positions\n",
    "                                                     init_pos=init_pos_np) # <-- Pass init_pos here\n",
    "\n",
    "                # Perform optimization (pyswarms will use the provided init_pos)\n",
    "                current_pso_loss, current_pso_params = optimizer.optimize(\n",
    "                    pso_objective_wrapper,\n",
    "                    iters=pso_iters,\n",
    "                    verbose=False\n",
    "                )\n",
    "                current_pso_params = jnp.array(current_pso_params, dtype=jnp.float64)\n",
    "\n",
    "                print(f\"    Run {i+1} completed. Loss: {current_pso_loss}\")\n",
    "                if current_pso_loss < best_pso_loss:\n",
    "                    best_pso_loss = current_pso_loss\n",
    "                    best_pso_params = current_pso_params\n",
    "                    print(f\"    New best PSO loss found: {best_pso_loss}\")\n",
    "\n",
    "            # ... (rest of the function: handling no solution, BFGS refinement) ...\n",
    "            if best_pso_params is None:\n",
    "                 print(\"Warning: PSO did not find any valid solution after multiple runs. Using initial guess for BFGS.\")\n",
    "                 best_pso_params = initial_guess # Fallback\n",
    "\n",
    "            print(f\"\\nBest PSO loss after {num_pso_runs} runs: {best_pso_loss}\")\n",
    "            print(\"Refining best PSO result with L- BFGS...\")\n",
    "\n",
    "            result =  solver.run(best_pso_params)\n",
    "            \n",
    "            return result.params\n",
    "\n",
    "    \n",
    "    def calculate_sensitivities(opt_params: jnp.ndarray, base_loss: float) -> jnp.ndarray:\n",
    "        n_active = opt_params.size\n",
    "        masks = 1.0 - jnp.eye(n_active, dtype=opt_params.dtype)\n",
    "        @jit\n",
    "        def solve_one(mask_i):\n",
    "            init = opt_params * mask_i\n",
    "            def sub_loss(p):\n",
    "                return loss_fn(p * mask_i)\n",
    "            solver = LBFGS(fun=sub_loss, maxiter=100, tol=1e-8)\n",
    "            out    = solver.run(init)         \n",
    "            loss_i = loss_fn(out.params * mask_i)       \n",
    "            return jnp.log2(loss_i / base_loss)\n",
    "\n",
    "        sensitivities = vmap(solve_one)(masks)\n",
    "        return sensitivities\n",
    "\n",
    "    try:\n",
    "        opt_key, sensi_key = random.split(master_key)\n",
    "        optimized_params = run_optimization(loss_fn, params, key=opt_key)\n",
    "        final_loss = loss_fn(optimized_params)\n",
    "        print(f\"Final loss after L-BFGS: {final_loss}\")\n",
    "        if optimized_params is None:\n",
    "             print(\"Optimization failed to produce parameters.\")\n",
    "             return None\n",
    "\n",
    "        final_loss = loss_fn(optimized_params)\n",
    "        print(f\"Final optimized loss: {final_loss}\")\n",
    "    except Exception as e:\n",
    "        # ... (error handling remains the same) ...\n",
    "        print(f\"Optimization or final loss calculation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "    if not jnp.isfinite(final_loss):\n",
    "        print(\"Final loss is not finite.\")\n",
    "        return None\n",
    "\n",
    "    # Pass a key to sensitivity analysis if it also needs randomness\n",
    "    # For now, assuming calculate_sensitivities doesn't need a separate key\n",
    "    sensitivities = calculate_sensitivities(optimized_params, final_loss)\n",
    "    sensitivity_dict = {f\"sensitive of params[{i}]\": round(float(sensitivities[i]), 4) \n",
    "                       for i in range(len(sensitivities))}\n",
    "\n",
    "\n",
    "    return {\n",
    "        'params': optimized_params,\n",
    "        'loss': final_loss,\n",
    "        'sensitivities': sensitivity_dict\n",
    "    }\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
