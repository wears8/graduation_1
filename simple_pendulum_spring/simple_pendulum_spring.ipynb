{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41bfb7ff",
   "metadata": {},
   "source": [
    "## 生成哈密顿量的txt文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64daed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the mathematical function skeleton that represents acceleration in a system, given data on time, position, and velocity.\n",
    "A large positive sensitivity value for param[i] means that removing it significantly hurts the function's performance,\n",
    "You should select those formulas with high sensitivity of parameter and remove formulas with low sensitivity of parameter, then add one or two new formula.\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config, grad, jit, vmap\n",
    "from pyswarm import pso\n",
    "from jax.scipy.optimize import minimize\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "#@evaluate.run\n",
    "def evaluate(data: dict, params=initial_params) -> dict:\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    #n_dim = inputs.shape[1] // 2\n",
    "    #q, p = inputs[:, :n_dim], inputs[:, n_dim:]\n",
    "    q, p = inputs[:, 0], inputs[:,1]\n",
    "    true_value = outputs\n",
    "    \n",
    "    @jit\n",
    "    def compute_dynamics(q, p, params):\n",
    "        def hamiltonian(q, p, params):\n",
    "            return equation(q, p, params)  \n",
    "        q_dot = jax.grad(hamiltonian, 1)(q, p, params)  \n",
    "        p_dot = -jax.grad(hamiltonian, 0)(q, p, params)  \n",
    "        return jnp.array([q_dot, p_dot])     #jnp.concatenate([q_dot, p_dot])\n",
    "\n",
    "    batch_compute_dynamics = jit(vmap(compute_dynamics, (0, 0, None)))\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        pred = batch_compute_dynamics(q, p, params)\n",
    "        return jnp.mean(jnp.square(pred - true_value))\n",
    "\n",
    "    '''def run_optimization(objective_fn, initial_guess):\n",
    "        if initial_guess.size > MAX_NPARAMS:\n",
    "            result = minimize(objective_fn, initial_guess,\n",
    "                            method='BFGS', options={'maxiter': 500})\n",
    "            return result.x\n",
    "        else:\n",
    "            def pso_wrapper(x):\n",
    "                return objective_fn(jnp.array(x))   \n",
    "\n",
    "            lb = [-10.0]*initial_guess.size\n",
    "            ub = [10.0]*initial_guess.size\n",
    "            pso_params, _ = pso(pso_wrapper, lb, ub, \n",
    "                            swarmsize=100, maxiter=300,omega=0.729, phip=1.49445, phig=1.49445)\n",
    "            \n",
    "            result = minimize(objective_fn, jnp.array(pso_params),\n",
    "                            method='BFGS', options={'maxiter': 500})\n",
    "            return result.x'''\n",
    "\n",
    "    def calculate_sensitivities(opt_params, base_loss):\n",
    "        mask = 1 - jnp.eye(MAX_NPARAMS)\n",
    "        \n",
    "        @jit      \n",
    "        def batch_loss(params_matrix):\n",
    "            return vmap(loss_fn)(params_matrix * mask)\n",
    "\n",
    "        def sensitivity_objective(flat_params):\n",
    "            matrix_params = flat_params.reshape(MAX_NPARAMS, MAX_NPARAMS)\n",
    "            return jnp.sum(batch_loss(matrix_params))\n",
    "\n",
    "        try:\n",
    "            initial_flat = (opt_params * mask).flatten()\n",
    "            optimized_flat = run_optimization(sensitivity_objective, initial_flat)\n",
    "            optimized_matrix = optimized_flat.reshape(MAX_NPARAMS, MAX_NPARAMS)\n",
    "            losses = batch_loss(optimized_matrix)\n",
    "            relative_loss = jnp.log2(losses / base_loss)\n",
    "            return jnp.round(relative_loss,4)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Sensitivity analysis error: {str(e)}\")\n",
    "            return jnp.zeros(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "    try:\n",
    "        optimized_params = run_optimization(loss_fn, params)\n",
    "        final_loss = loss_fn(optimized_params)\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not jnp.isfinite(final_loss):\n",
    "        return None\n",
    "\n",
    "    sensitivities = calculate_sensitivities(optimized_params, final_loss)\n",
    "    sensitivity_dict = {f\"sensitive of params[{i}]\": float(sensitivities[i])\n",
    "                       for i in range(len(sensitivities))}\n",
    "\n",
    "    return {\n",
    "        'params': optimized_params,\n",
    "        'loss': -final_loss.item(),\n",
    "        'sensitivities': sensitivity_dict\n",
    "    }\n",
    "\n",
    "#@equation.evolve\n",
    "@jit\n",
    "def equation(q: jnp.array, p: jnp.array, params: jnp.array) -> jnp.array:\n",
    "    #q = q[...,0]\n",
    "    #p = p[...,0]\n",
    "     \n",
    "    #T = params[1] * jnp.square(p)\n",
    "    #V =  params[3] * jnp.square(q) +params[0]*q +params[2]                   #懂了，本质上是我在调用equation时，传入的数据本就只有1组，\n",
    "\n",
    "    #T = params[1] * jnp.dot(p,p)                                             #并没有区别，因为p就只有一个，q只有一个。jnp.sum(q)=q。\n",
    "    #V = params[3] * jnp.dot(q,q) + params[0]*jnp.sum(q) + params[2]\n",
    "    T = params[1] * jnp.square(p) \n",
    "    V = params[3] * jnp.cos(q)+params[0]\n",
    "\n",
    "    return T + V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "393aa9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00 -4.15778787e+01]\n",
      " [-8.66419284e-03 -4.15774283e+01]\n",
      " [-1.73281980e-02 -4.15760771e+01]\n",
      " ...\n",
      " [-1.08776333e+00  3.30105593e+01]\n",
      " [-1.08087271e+00  3.31423153e+01]\n",
      " [-1.07395339e+00  3.32730311e+01]]\n",
      "[[  0.78539816   0.        ]\n",
      " [  0.78538733  -0.10397031]\n",
      " [  0.78535483  -0.20793838]\n",
      " ...\n",
      " [ -0.5960815  -13.05316   ]\n",
      " [ -0.59879171 -12.97047246]\n",
      " [ -0.60148549 -12.8874407 ]]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Optimization or final loss calculation failed: iteration over a 0-d array\n",
      "最终损失值 (MSE): None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\19464\\AppData\\Local\\Temp\\ipykernel_16488\\3614080711.py\", line 173, in evaluate\n",
      "    optimized_params = run_optimization(loss_fn, params, key=random.PRNGKey(0), maxiter=500)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\19464\\AppData\\Local\\Temp\\ipykernel_16488\\3614080711.py\", line 135, in run_optimization\n",
      "    sol = solver.run(initial_guess)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\anacoda\\envs\\pytorch\\Lib\\site-packages\\jaxopt\\_src\\base.py\", line 358, in run\n",
      "    return run(init_params, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\anacoda\\envs\\pytorch\\Lib\\site-packages\\jaxopt\\_src\\implicit_diff.py\", line 251, in wrapped_solver_fun\n",
      "    return make_custom_vjp_solver_fun(solver_fun, keys)(*args, *vals)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\anacoda\\envs\\pytorch\\Lib\\site-packages\\jax\\_src\\traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\anacoda\\envs\\pytorch\\Lib\\site-packages\\jax\\_src\\custom_derivatives.py\", line 681, in __call__\n",
      "    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\anacoda\\envs\\pytorch\\Lib\\site-packages\\jax\\_src\\custom_derivatives.py\", line 911, in bind\n",
      "    return self._true_bind(*args, **params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\anacoda\\envs\\pytorch\\Lib\\site-packages\\jax\\_src\\core.py\", line 520, in _true_bind\n",
      "    return self.bind_with_trace(prev_trace, args, params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\anacoda\\envs\\pytorch\\Lib\\site-packages\\jax\\_src\\custom_derivatives.py\", line 915, in bind_with_trace\n",
      "    return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\anacoda\\envs\\pytorch\\Lib\\site-packages\\jax\\_src\\core.py\", line 1045, in process_custom_vjp_call\n",
      "    return fun.call_wrapped(*tracers)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\anacoda\\envs\\pytorch\\Lib\\site-packages\\jax\\_src\\linear_util.py\", line 210, in call_wrapped\n",
      "    return self.f_transformed(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\anacoda\\envs\\pytorch\\Lib\\site-packages\\jax\\_src\\custom_derivatives.py\", line 87, in _flatten_fun_nokwargs\n",
      "    ans = f(*py_args)\n",
      "          ^^^^^^^^^^^\n",
      "  File \"c:\\anacoda\\envs\\pytorch\\Lib\\site-packages\\jax\\_src\\linear_util.py\", line 370, in _get_result_paths_thunk\n",
      "    ans = _fun(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\anacoda\\envs\\pytorch\\Lib\\site-packages\\jaxopt\\_src\\implicit_diff.py\", line 207, in solver_fun_flat\n",
      "    return solver_fun(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\anacoda\\envs\\pytorch\\Lib\\site-packages\\jaxopt\\_src\\base.py\", line 300, in _run\n",
      "    state = self.init_state(init_params, *args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\anacoda\\envs\\pytorch\\Lib\\site-packages\\jaxopt\\_src\\lbfgs.py\", line 284, in init_state\n",
      "    (value, aux), grad = self._value_and_grad_with_aux(init_params, *args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\anacoda\\envs\\pytorch\\Lib\\site-packages\\jaxopt\\_src\\base.py\", line 62, in value_and_grad_with_aux\n",
      "    v, g = value_and_grad(*a, **kw)\n",
      "    ^^^^\n",
      "  File \"c:\\anacoda\\envs\\pytorch\\Lib\\site-packages\\jax\\_src\\array.py\", line 426, in __iter__\n",
      "    raise TypeError(\"iteration over a 0-d array\")  # same as numpy error\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: iteration over a 0-d array\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 读取 CSV 文件并转换为 NumPy 数组\n",
    "data0 = pd.read_csv('./pendulum_hamilton_data.csv')\n",
    "#data0 = pd.read_csv('./hamiltonian_spring_mass_energy_data.csv')\n",
    "tae = data0.to_numpy()\n",
    "\n",
    "# 使用 JAX 的数组操作替换 PyTorch 的操作\n",
    "state = jnp.array(tae[:, 0:2], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "true_q_ddot = jnp.array(tae[:, 2:-1], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "energy = jnp.array(tae[:, -1], dtype=jnp.float64)\n",
    "print(true_q_ddot)\n",
    "print(state)\n",
    "# 将数据存储在字典中\n",
    "data = {\n",
    "    'inputs': state,\n",
    "    'outputs': true_q_ddot,  # 真实的加速度\n",
    "    'energy': energy\n",
    "}\n",
    "\n",
    "\n",
    "print(initial_params)\n",
    "# 评估并优化参数\n",
    "final_loss = evaluate(data, initial_params)\n",
    "print(\"最终损失值 (MSE):\", final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e3ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the mathematical function skeleton that represents acceleration in a damped nonlinear oscillator system with driving force, given data on time, position, and velocity. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap, config\n",
    "from scipy.optimize import minimize\n",
    "import pyswarms as ps\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import numpy as np\n",
    "import jax.random as random\n",
    "from jaxopt import LBFGS\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def evaluate(data: dict, params=initial_params) -> dict:\n",
    "\n",
    "    master_key = random.PRNGKey(0)\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    n_dim = inputs.shape[1] // 2\n",
    "    q, p = inputs[:, :n_dim], inputs[:, n_dim:]\n",
    "    true_value = outputs\n",
    "    \n",
    "    @jit\n",
    "    def compute_dynamics(q, p, params):\n",
    "        def hamiltonian(q, p, params):\n",
    "            return equation(q, p, params)  \n",
    "        q_dot = jax.grad(hamiltonian, 1)(q, p, params)  \n",
    "        p_dot = -jax.grad(hamiltonian, 0)(q, p, params)  \n",
    "        return jnp.concatenate([q_dot, p_dot])\n",
    "\n",
    "    batch_compute_dynamics = jit(vmap(compute_dynamics, (0, 0, None)))\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        pred = batch_compute_dynamics(q, p, params)\n",
    "        return jnp.mean(jnp.square(pred - true_value))\n",
    "\n",
    "    '''def run_optimization(objective_fn, initial_guess, key, num_pso_runs=5, pso_iters=300, swarmsize=100):\n",
    "        print(f\"Initial guess size: {initial_guess.size}, MAX_NPARAMS: {MAX_NPARAMS}\")\n",
    "        n_params = initial_guess.size\n",
    "\n",
    "        if n_params > MAX_NPARAMS:\n",
    "            # ... (BFGS only part remains the same) ...\n",
    "            print(\"Using BFGS directly due to large number of parameters.\")\n",
    "            result = minimize(objective_fn, initial_guess,\n",
    "                              method='BFGS', options={'maxiter': 500})\n",
    "            if not result.success:\n",
    "                print(f\"BFGS optimization failed: {result.message}\")\n",
    "                return initial_guess # Fallback\n",
    "            return result.x\n",
    "        else:\n",
    "            # --- Use pyswarms with JAX-controlled initial positions ---\n",
    "            @jit\n",
    "            def pso_objective_wrapper(particles_matrix):\n",
    "                return vmap(objective_fn)(particles_matrix)\n",
    "\n",
    "            min_bound_np = np.full(n_params, -10.0, dtype=np.float64)\n",
    "            max_bound_np = np.full(n_params, 10.0, dtype=np.float64)\n",
    "            bounds = (min_bound_np, max_bound_np)\n",
    "\n",
    "            options = {'c1': 1.49445, 'c2': 1.49445, 'w': 0.729}\n",
    "\n",
    "            best_pso_params = None\n",
    "            best_pso_loss = jnp.inf\n",
    "\n",
    "            current_key = key # Use the passed-in key\n",
    "\n",
    "            print(f\"Running pyswarms PSO {num_pso_runs} times with unique initial swarms...\")\n",
    "            for i in range(num_pso_runs):\n",
    "                # Split the key for this run to ensure unique randomness\n",
    "                current_key, subkey = random.split(current_key)\n",
    "\n",
    "                # Generate initial positions using JAX PRNG within bounds\n",
    "                # Use jnp arrays for min/max bounds in jax.random.uniform\n",
    "                min_bound_jnp = jnp.full(n_params, -10.0, dtype=jnp.float64)\n",
    "                max_bound_jnp = jnp.full(n_params, 10.0, dtype=jnp.float64)\n",
    "                init_pos_jax = random.uniform(subkey,\n",
    "                                              shape=(swarmsize, n_params),\n",
    "                                              dtype=jnp.float64,\n",
    "                                              minval=min_bound_jnp,\n",
    "                                              maxval=max_bound_jnp)\n",
    "                # Convert to NumPy array for pyswarms\n",
    "                init_pos_np = np.array(init_pos_jax)\n",
    "\n",
    "                print(f\"  PSO Run {i+1}/{num_pso_runs} (using JAX key split for init_pos)\")\n",
    "                optimizer = ps.single.GlobalBestPSO(n_particles=swarmsize,\n",
    "                                                     dimensions=n_params,\n",
    "                                                     options=options,\n",
    "                                                     bounds=bounds,\n",
    "                                                     # Pass the generated initial positions\n",
    "                                                     init_pos=init_pos_np) # <-- Pass init_pos here\n",
    "\n",
    "                # Perform optimization (pyswarms will use the provided init_pos)\n",
    "                current_pso_loss, current_pso_params = optimizer.optimize(\n",
    "                    pso_objective_wrapper,\n",
    "                    iters=pso_iters,\n",
    "                    verbose=False\n",
    "                )\n",
    "                current_pso_params = jnp.array(current_pso_params, dtype=jnp.float64)\n",
    "\n",
    "                print(f\"    Run {i+1} completed. Loss: {current_pso_loss}\")\n",
    "                if current_pso_loss < best_pso_loss:\n",
    "                    best_pso_loss = current_pso_loss\n",
    "                    best_pso_params = current_pso_params\n",
    "                    print(f\"    New best PSO loss found: {best_pso_loss}\")\n",
    "\n",
    "            # ... (rest of the function: handling no solution, BFGS refinement) ...\n",
    "            if best_pso_params is None:\n",
    "                 print(\"Warning: PSO did not find any valid solution after multiple runs. Using initial guess for BFGS.\")\n",
    "                 best_pso_params = initial_guess # Fallback\n",
    "\n",
    "            print(f\"\\nBest PSO loss after {num_pso_runs} runs: {best_pso_loss}\")\n",
    "            print(\"Refining best PSO result with BFGS...\")\n",
    "\n",
    "            result = minimize(objective_fn, jnp.array(best_pso_params),\n",
    "                              method='BFGS', options={'maxiter': 500})\n",
    "\n",
    "            if not result.success:\n",
    "                 print(f\"BFGS refinement failed: {result.message}\")\n",
    "                 return best_pso_params # Return PSO best\n",
    "\n",
    "            print(f\"BFGS refinement successful. Final loss: {result.fun}\")\n",
    "            return result.x'''\n",
    "        \n",
    "\n",
    "    # 敏感度分析模块\n",
    "    def calculate_sensitivities(opt_params, base_loss):\n",
    "        mask = 1 - jnp.eye(MAX_NPARAMS)\n",
    "        \n",
    "        @jit      \n",
    "        def batch_loss(params_matrix):\n",
    "            return vmap(loss_fn)(params_matrix * mask)\n",
    "\n",
    "        def sensitivity_objective(flat_params):\n",
    "            matrix_params = flat_params.reshape(MAX_NPARAMS, MAX_NPARAMS)\n",
    "            return jnp.sum(batch_loss(matrix_params))\n",
    "\n",
    "        try:\n",
    "            initial_flat = (opt_params * mask).flatten()\n",
    "            #optimized_flat = run_optimization(sensitivity_objective, initial_flat, key=opt_key, num_pso_runs=5)\n",
    "            optimized_flat =run_optimization(sensitivity_objective, initial_flat, maxiter=500)\n",
    "            optimized_matrix = optimized_flat.reshape(MAX_NPARAMS, MAX_NPARAMS)\n",
    "            losses = batch_loss(optimized_matrix)\n",
    "            relative_loss = jnp.log2(losses / base_loss)\n",
    "            return relative_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Sensitivity analysis error: {str(e)}\")\n",
    "            return jnp.zeros(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "    # 主流程\n",
    "    # Main execution flow\n",
    "    try:\n",
    "        # Split the master key for the main optimization run\n",
    "        #opt_key, sensi_key = random.split(master_key) # Keep keys separate if needed later\n",
    "\n",
    "        #optimized_params = run_optimization(loss_fn, params, key=opt_key, num_pso_runs=5) # Pass the key\n",
    "        optimized_params = run_optimization(loss_fn, params,  maxiter=500)\n",
    "        final_loss = loss_fn(optimized_params)\n",
    "        print(f\"Final loss after L-BFGS: {final_loss}\")\n",
    "        if optimized_params is None:\n",
    "             print(\"Optimization failed to produce parameters.\")\n",
    "             return None\n",
    "\n",
    "        final_loss = loss_fn(optimized_params)\n",
    "        print(f\"Final optimized loss: {final_loss}\")\n",
    "    except Exception as e:\n",
    "        # ... (error handling remains the same) ...\n",
    "        print(f\"Optimization or final loss calculation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "    if not jnp.isfinite(final_loss):\n",
    "        print(\"Final loss is not finite.\")\n",
    "        return None\n",
    "\n",
    "    # Pass a key to sensitivity analysis if it also needs randomness\n",
    "    # For now, assuming calculate_sensitivities doesn't need a separate key\n",
    "    sensitivities = calculate_sensitivities(optimized_params, final_loss)\n",
    "    sensitivity_dict = {f\"sensitive of params[{i}]\": round(float(sensitivities[i]), 4) \n",
    "                       for i in range(len(sensitivities))}\n",
    "\n",
    "\n",
    "    return {\n",
    "        'params': optimized_params,\n",
    "        'loss': final_loss,\n",
    "        'sensitivities': sensitivity_dict\n",
    "    }\n",
    "\n",
    "\n",
    "@jit\n",
    "def equation(q: jnp.array, p: jnp.array, params: jnp.array) -> jnp.array:\n",
    "    q = q[...,0]\n",
    "    p = p[...,0]\n",
    "\n",
    "    #T = params[1] * jnp.square(p)\n",
    "    #V =  params[3] * jnp.square(q) +params[0]*q +params[2]\n",
    "\n",
    "    T = params[1] * jnp.square(p) \n",
    "    V = params[3] * jnp.cos(params[2]*q)+params[0]\n",
    "\n",
    "    return T + V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0d109448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00 -4.15778787e+01]\n",
      " [-8.66419284e-03 -4.15774283e+01]\n",
      " [-1.73281980e-02 -4.15760771e+01]\n",
      " ...\n",
      " [-1.08776333e+00  3.30105593e+01]\n",
      " [-1.08087271e+00  3.31423153e+01]\n",
      " [-1.07395339e+00  3.32730311e+01]]\n",
      "[[  0.78539816   0.        ]\n",
      " [  0.78538733  -0.10397031]\n",
      " [  0.78535483  -0.20793838]\n",
      " ...\n",
      " [ -0.5960815  -13.05316   ]\n",
      " [ -0.59879171 -12.97047246]\n",
      " [ -0.60148549 -12.8874407 ]]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Final loss after L-BFGS: 449.67660514045866\n",
      "Final optimized loss: 449.67660514045866\n",
      "最终损失值 (MSE): {'params': Array([1.00000000e+00, 4.16666667e-02, 8.22249605e-10, 7.29000131e-01,\n",
      "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
      "       1.00000000e+00, 1.00000000e+00], dtype=float64), 'loss': Array(449.67660514, dtype=float64), 'sensitivities': {'sensitive of params[0]': 0.0, 'sensitive of params[1]': 0.0023, 'sensitive of params[2]': 0.0, 'sensitive of params[3]': 0.0, 'sensitive of params[4]': 0.0, 'sensitive of params[5]': 0.0, 'sensitive of params[6]': 0.0, 'sensitive of params[7]': 0.0, 'sensitive of params[8]': 0.0, 'sensitive of params[9]': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# 读取 CSV 文件并转换为 NumPy 数组\n",
    "data0 = pd.read_csv('./pendulum_hamilton_data.csv')\n",
    "#data0 = pd.read_csv('./hamiltonian_spring_mass_energy_data.csv')\n",
    "tae = data0.to_numpy()\n",
    "\n",
    "# 使用 JAX 的数组操作替换 PyTorch 的操作\n",
    "state = jnp.array(tae[:, 0:2], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "true_q_ddot = jnp.array(tae[:, 2:-1], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "energy = jnp.array(tae[:, -1], dtype=jnp.float64)\n",
    "print(true_q_ddot)\n",
    "print(state)\n",
    "# 将数据存储在字典中\n",
    "data = {\n",
    "    'inputs': state,\n",
    "    'outputs': true_q_ddot,  # 真实的加速度\n",
    "    'energy': energy\n",
    "}\n",
    "\n",
    "\n",
    "print(initial_params)\n",
    "# 评估并优化参数\n",
    "final_loss = evaluate(data, initial_params)\n",
    "print(\"最终损失值 (MSE):\", final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f414eafc",
   "metadata": {},
   "source": [
    "# 又遇到了优化困难的问题\n",
    "### 尝试了JAX自带的L-BFGS，垃圾的一批：好几百的损失。\n",
    "### 尝试差分进化+bfgs："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233fde1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
