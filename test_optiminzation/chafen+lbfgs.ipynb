{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a898290",
   "metadata": {},
   "source": [
    "# 测试以下DF+LBFGS优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40079f4b",
   "metadata": {},
   "source": [
    "### 以oscillator为例子\n",
    "会出现敏感度计算失误的问题,而且速度不如PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3db4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the mathematical function skeleton that represents acceleration in a damped nonlinear oscillator system with driving force, given data on time, position, and velocity. \n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit, vmap, config\n",
    "from scipy.optimize import minimize\n",
    "from pyswarm import pso\n",
    "from pyswarms.utils.plotters import plot_cost_history\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import jax.random as random\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def evaluate(data: dict, params=initial_params) -> dict:\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    t, x, v = inputs[:,0], inputs[:,1], inputs[:,2]\n",
    "    a = outputs\n",
    "    print(t.shape, x.shape, v.shape, a.shape)\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        pred = equation(t, x, v, params)\n",
    "        true_accelerations = a\n",
    "        return jnp.mean(jnp.square(pred - true_accelerations))\n",
    "        \n",
    "    def run_optimization(objective_fn, initial_guess):\n",
    "        \n",
    "        solver = LBFGS(fun=objective_fn, maxiter=100, tol=1e-8)\n",
    "        if initial_guess.size > MAX_NPARAMS:\n",
    "            result =  solver.run(initial_guess)\n",
    "            return result.params\n",
    "        else:\n",
    "            bounds = [(-10.0, 10.0)] * MAX_NPARAMS\n",
    "            # JIT 编译目标函数\n",
    "            jit_loss = jax.jit(objective_fn)\n",
    "            # NumPy wrapper 供 SciPy 调用\n",
    "            def loss_numpy(x: np.ndarray) -> float:\n",
    "                return float(jit_loss(jnp.array(x, dtype=jnp.float64)))\n",
    "            # 1) 差分进化全局搜索\n",
    "            result_de = differential_evolution(\n",
    "                loss_numpy,\n",
    "                bounds=bounds,\n",
    "                maxiter=300,\n",
    "                popsize=15,\n",
    "                tol=0.01,\n",
    "                disp=True\n",
    "            )\n",
    "            x0 = jnp.array(result_de.x, dtype=jnp.float64)\n",
    "            print(f\"[DE] 初始点 loss = {result_de.fun:.6f}\")\n",
    "            # 2) L-BFGS 局部精炼\n",
    "            sol = solver.run(x0)\n",
    "            return sol.params\n",
    "    \n",
    "    def calculate_sensitivities(loss_fn, opt_params, base_loss,  tol=1e-20):\n",
    "        # 1) 自动识别“活跃”参数索引\n",
    "        grads   = jax.grad(loss_fn)(opt_params)             # (MAX_NPARAMS,)\n",
    "        active  = jnp.where(jnp.abs(grads) > tol)[0]        # e.g. [0,1,2,3,4,5,6,7]\n",
    "        n_active = active.shape[0]\n",
    "\n",
    "        # 2) 构造掩码矩阵 M，shape = (n_active, MAX_NPARAMS)\n",
    "        #    每一行 M[i] 都是全 1，只有 active[i] 这一列为 0\n",
    "        eye      = jnp.eye(opt_params.size)\n",
    "        masks    = 1.0 - eye[active]                         # (n_active, MAX_NPARAMS)\n",
    "\n",
    "        # 3) 生成每个子问题的初始猜测：masked_params = masks * opt_params\n",
    "        init_batch = masks * opt_params                      # (n_active, MAX_NPARAMS)\n",
    "\n",
    "        # 4) 定义一个“单点 L-BFGS 解算器”，并向量化\n",
    "        solver    = LBFGS(fun=loss_fn, maxiter=100, tol=1e-8)\n",
    "\n",
    "        @jax.jit\n",
    "        def solve_one(p0):\n",
    "            sol = solver.run(p0)\n",
    "            return sol.params                              # shape = (MAX_NPARAMS,)\n",
    "\n",
    "        # 5) 并行跑 n_active 次 L-BFGS\n",
    "        sol_batch = vmap(solve_one)(init_batch)             # (n_active, MAX_NPARAMS)\n",
    "\n",
    "        # 6) 计算每个子解对应的损失\n",
    "        loss_batch = vmap(loss_fn)(sol_batch)               # (n_active,)\n",
    "\n",
    "        # 7) 输出相对增益 log2(loss_i / base_loss)\n",
    "        sens = jnp.log2(loss_batch / base_loss)       # (n_active,)\n",
    "\n",
    "        return  sens\n",
    "\n",
    "\n",
    "    # 主流程\n",
    "    # Main execution flow\n",
    "    try:\n",
    "        optimized_params = run_optimization(loss_fn, params)\n",
    "        final_loss = loss_fn(optimized_params)\n",
    "        print(f\"Final loss: {final_loss}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not jnp.isfinite(final_loss):\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'params': optimized_params,\n",
    "        'loss': final_loss,\n",
    "        'sensitivities': calculate_sensitivities(loss_fn,optimized_params, final_loss)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "@jit\n",
    "def equation(t: jnp.array, x: jnp.array, v: jnp.array, params: jnp.array) -> jnp.array:\n",
    "    \"\"\" Mathematical function for acceleration in a damped nonlinear oscillator\n",
    "\n",
    "    Args:\n",
    "        t: A jax array representing time.\n",
    "        x: A jax array representing observations of current position.\n",
    "        v: A jax array representing observations of velocity.\n",
    "        params: Jax array of numeric constants or parameters to be optimized\n",
    "\n",
    "    Return:\n",
    "        A jax array representing acceleration as the result of applying the mathematical function to the inputs.\n",
    "    \"\"\"\n",
    "    dv = params[0] * jnp.sin(params[1]*t) + params[2] * x*v + params[3] * v**3 + params[4]*x * jnp.exp(params[5]*x) + x**2 * params[6] #+params[7]#+ params[8]*jnp.cos(params[9]*t) + params[10]*x*v**2\n",
    "    return dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e636e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# 读取 CSV 文件并转换为 NumPy 数组\n",
    "data0 = pd.read_csv('./train_os2.csv')#\n",
    "tae = data0.to_numpy()\n",
    "\n",
    "# 使用 JAX 的数组操作替换 PyTorch 的操作\n",
    "state = jnp.array(tae[:, :3], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "energy = jnp.array(tae[:,-1], dtype=jnp.float64)\n",
    "print(state.shape)\n",
    "print(energy)\n",
    "# 将数据存储在字典中\n",
    "data = {\n",
    "    'inputs': state,\n",
    "    'outputs': energy\n",
    "}\n",
    "\n",
    "\n",
    "evaluate(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c0751",
   "metadata": {},
   "source": [
    "# 测试原算法，优化掩膜情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0afdef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the mathematical function skeleton that represents acceleration in a damped nonlinear oscillator system with driving force, given data on time, position, and velocity. \n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit, vmap, config\n",
    "from scipy.optimize import minimize\n",
    "from pyswarm import pso\n",
    "from pyswarms.utils.plotters import plot_cost_history\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import jax.random as random\n",
    "from scipy.optimize import differential_evolution\n",
    "from jaxopt import LBFGS\n",
    "import pyswarms as ps\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def evaluate(data: dict, params=initial_params) -> dict:\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    t, x, v = inputs[:,0], inputs[:,1], inputs[:,2]\n",
    "    a = outputs\n",
    "    print(t.shape, x.shape, v.shape, a.shape)\n",
    "    master_key = random.PRNGKey(0)\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        pred = equation(t, x, v, params)\n",
    "        true_accelerations = a\n",
    "        return jnp.mean(jnp.square(pred - true_accelerations))\n",
    "\n",
    "    '''def run_optimization(objective_fn, initial_guess):\n",
    "        if initial_guess.size > MAX_NPARAMS:\n",
    "            result = minimize(objective_fn, initial_guess,\n",
    "                            method='BFGS', options={'maxiter': 100})\n",
    "            return result.x\n",
    "        else:\n",
    "            def pso_wrapper(x):\n",
    "                return objective_fn(jnp.array(x))\n",
    "            \n",
    "            lb = [-10.0]*MAX_NPARAMS\n",
    "            ub = [10.0]*MAX_NPARAMS\n",
    "            \n",
    "            pso_params, _ = pso(pso_wrapper, lb, ub, \n",
    "                            swarmsize=100, maxiter=300,omega=0.729, phip=1.49445, phig=1.4944,minstep=1e-6, debug=False)\n",
    "            \n",
    "            result = minimize(objective_fn, jnp.array(pso_params),\n",
    "                            method='BFGS', options={'maxiter': 500})\n",
    "            return result.x'''\n",
    "    def run_optimization(objective_fn, initial_guess, key, num_pso_runs=5, pso_iters=300, swarmsize=100):\n",
    "        print(f\"Initial guess size: {initial_guess.size}, MAX_NPARAMS: {MAX_NPARAMS}\")\n",
    "        #n_params = initial_guess.size\n",
    "        grads   = jax.grad(loss_fn)(params)           # (MAX_NPARAMS,)           \n",
    "        active  = jnp.where(jnp.abs(grads) > 0)[0]        # e.g. [0,1,2,3,4,5,6,7]\n",
    "        n_params = active.shape[0]                        # 只保留活跃参数\n",
    "        print(n_params)\n",
    "\n",
    "        grads2   = jax.grad(loss_fn)(initial_guess)           # (MAX_NPARAMS,)           \n",
    "        active2  = jnp.where(jnp.abs(grads2) > 0)[0]        # e.g. [0,1,2,3,4,5,6,7]\n",
    "\n",
    "\n",
    "        if  len(active2) < n_params:\n",
    "            # ... (BFGS only part remains the same) ...\n",
    "            print(\"Using BFGS directly due to large number of parameters.\")\n",
    "            result = minimize(objective_fn, initial_guess,\n",
    "                              method='BFGS', options={'maxiter': 500})\n",
    "            if not result.success:\n",
    "                print(f\"BFGS optimization failed: {result.message}\")\n",
    "                return initial_guess # Fallback\n",
    "            return result.x\n",
    "        else:\n",
    "            # --- Use pyswarms with JAX-controlled initial positions ---\n",
    "            @jit\n",
    "            def pso_objective_wrapper(particles_matrix):\n",
    "                return vmap(objective_fn)(particles_matrix)\n",
    "\n",
    "            min_bound_np = np.full(n_params, -10.0, dtype=np.float64)\n",
    "            max_bound_np = np.full(n_params, 10.0, dtype=np.float64)\n",
    "            bounds = (min_bound_np, max_bound_np)\n",
    "\n",
    "            options = {'c1': 1.49445, 'c2': 1.49445, 'w': 0.729}\n",
    "\n",
    "            best_pso_params = None\n",
    "            best_pso_loss = jnp.inf\n",
    "\n",
    "            current_key = key # Use the passed-in key\n",
    "\n",
    "            print(f\"Running pyswarms PSO {num_pso_runs} times with unique initial swarms...\")\n",
    "            for i in range(num_pso_runs):\n",
    "                # Split the key for this run to ensure unique randomness\n",
    "                current_key, subkey = random.split(current_key)\n",
    "\n",
    "                # Generate initial positions using JAX PRNG within bounds\n",
    "                # Use jnp arrays for min/max bounds in jax.random.uniform\n",
    "                min_bound_jnp = jnp.full(n_params, -10.0, dtype=jnp.float64)\n",
    "                max_bound_jnp = jnp.full(n_params, 10.0, dtype=jnp.float64)\n",
    "                init_pos_jax = random.uniform(subkey,\n",
    "                                              shape=(swarmsize, n_params),\n",
    "                                              dtype=jnp.float64,\n",
    "                                              minval=min_bound_jnp,\n",
    "                                              maxval=max_bound_jnp)\n",
    "                # Convert to NumPy array for pyswarms\n",
    "                init_pos_np = np.array(init_pos_jax)\n",
    "\n",
    "                print(f\"  PSO Run {i+1}/{num_pso_runs} (using JAX key split for init_pos)\")\n",
    "                optimizer = ps.single.GlobalBestPSO(n_particles=swarmsize,\n",
    "                                                     dimensions=n_params,\n",
    "                                                     options=options,\n",
    "                                                     bounds=bounds,\n",
    "                                                     # Pass the generated initial positions\n",
    "                                                     init_pos=init_pos_np) # <-- Pass init_pos here\n",
    "\n",
    "                # Perform optimization (pyswarms will use the provided init_pos)\n",
    "                current_pso_loss, current_pso_params = optimizer.optimize(\n",
    "                    pso_objective_wrapper,\n",
    "                    iters=pso_iters,\n",
    "                    verbose=False\n",
    "                )\n",
    "                current_pso_params = jnp.array(current_pso_params, dtype=jnp.float64)\n",
    "\n",
    "                print(f\"    Run {i+1} completed. Loss: {current_pso_loss}\")\n",
    "                if current_pso_loss < best_pso_loss:\n",
    "                    best_pso_loss = current_pso_loss\n",
    "                    best_pso_params = current_pso_params\n",
    "                    print(f\"    New best PSO loss found: {best_pso_loss}\")\n",
    "\n",
    "            # ... (rest of the function: handling no solution, BFGS refinement) ...\n",
    "            if best_pso_params is None:\n",
    "                 print(\"Warning: PSO did not find any valid solution after multiple runs. Using initial guess for BFGS.\")\n",
    "                 best_pso_params = initial_guess # Fallback\n",
    "\n",
    "            print(f\"\\nBest PSO loss after {num_pso_runs} runs: {best_pso_loss}\")\n",
    "            print(\"Refining best PSO result with BFGS...\")\n",
    "\n",
    "            result = minimize(objective_fn, jnp.array(best_pso_params),\n",
    "                              method='BFGS', options={'maxiter': 500})\n",
    "\n",
    "            if not result.success:\n",
    "                 print(f\"BFGS refinement failed: {result.message}\")\n",
    "                 return best_pso_params # Return PSO best\n",
    "\n",
    "            print(f\"BFGS refinement successful. Final loss: {result.fun}\")\n",
    "            return result.x\n",
    "        \n",
    "    \n",
    "    # 敏感度分析模块\n",
    "    def calculate_sensitivities(opt_params, base_loss):\n",
    "        n_active = opt_params.size\n",
    "        mask    = 1.0 - jnp.eye(n_active) \n",
    "        print(mask)\n",
    "\n",
    "        @jit      \n",
    "        def batch_loss(params_matrix):\n",
    "            return vmap(loss_fn)(params_matrix * mask)\n",
    "\n",
    "        def sensitivity_objective(flat_params):\n",
    "            matrix_params = flat_params.reshape(n_active,n_active)\n",
    "            return jnp.sum(batch_loss(matrix_params))\n",
    "\n",
    "        try:\n",
    "            initial_flat = (opt_params * mask).flatten()\n",
    "            #print(f\"Initial flat params: {initial_flat}\")\n",
    "            optimized_flat = run_optimization(sensitivity_objective, initial_flat,key=opt_key)\n",
    "            #print(f\"Optimized flat params: {optimized_flat}\")   \n",
    "            optimized_matrix = optimized_flat.reshape(n_active,n_active)\n",
    "            losses = batch_loss(optimized_matrix)\n",
    "            relative_loss = jnp.log2(losses / base_loss)\n",
    "            return relative_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Sensitivity analysis error: {str(e)}\")\n",
    "            return jnp.zeros(MAX_NPARAMS, dtype=jnp.float64)\n",
    "    \n",
    "\n",
    "    # 主流程\n",
    "    # Main execution flow\n",
    "    try:\n",
    "        opt_key, sensi_key = random.split(master_key)\n",
    "        optimized_params = run_optimization(loss_fn, params, key=opt_key, num_pso_runs=5)\n",
    "        final_loss = loss_fn(optimized_params)\n",
    "        print(f\"optimized_params: {optimized_params}\")\n",
    "        print(f\"Final loss: {final_loss}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not jnp.isfinite(final_loss):\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'params': optimized_params,\n",
    "        'loss': final_loss,\n",
    "        'sensitivities': calculate_sensitivities(optimized_params, final_loss)\n",
    "    }\n",
    "\n",
    "\n",
    "@jit\n",
    "def equation(t: jnp.array, x: jnp.array, v: jnp.array, params: jnp.array) -> jnp.array:\n",
    "    \"\"\" Mathematical function for acceleration in a damped nonlinear oscillator\n",
    "\n",
    "    Args:\n",
    "        t: A jax array representing time.\n",
    "        x: A jax array representing observations of current position.\n",
    "        v: A jax array representing observations of velocity.\n",
    "        params: Jax array of numeric constants or parameters to be optimized\n",
    "\n",
    "    Return:\n",
    "        A jax array representing acceleration as the result of applying the mathematical function to the inputs.\n",
    "    \"\"\"\n",
    "    dv = params[0] * jnp.sin(params[1]*t) + params[2] * x*v + params[3] * v**3 + params[4]*x * jnp.exp(params[5]*x) + x**2 * params[6] +params[7]#+ params[8]*jnp.cos(params[9]*t) + params[10]*x*v**2\n",
    "    return dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c54da4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (10000,) (10000,) (10000,)\n",
      "Initial guess size: 10, MAX_NPARAMS: 10\n",
      "8\n",
      "Running pyswarms PSO 5 times with unique initial swarms...\n",
      "  PSO Run 1/5 (using JAX key split for init_pos)\n",
      "    Run 1 completed. Loss: 2.9134140145145685e-10\n",
      "    New best PSO loss found: 2.9134140145145685e-10\n",
      "  PSO Run 2/5 (using JAX key split for init_pos)\n",
      "    Run 2 completed. Loss: 4.642441072646406e-07\n",
      "  PSO Run 3/5 (using JAX key split for init_pos)\n",
      "    Run 3 completed. Loss: 2.3525600629497307e-08\n",
      "  PSO Run 4/5 (using JAX key split for init_pos)\n",
      "    Run 4 completed. Loss: 7.569607370053503e-07\n",
      "  PSO Run 5/5 (using JAX key split for init_pos)\n",
      "    Run 5 completed. Loss: 1.2836027745966776e-08\n",
      "\n",
      "Best PSO loss after 5 runs: 2.9134140145145685e-10\n",
      "Refining best PSO result with BFGS...\n",
      "BFGS refinement successful. Final loss: 2.102168303052898e-10\n",
      "optimized_params: [-2.99999907e-01 -9.99999371e-01 -1.00007000e+00 -4.99981116e-01\n",
      " -5.00009628e+00  4.99856920e-01 -6.87576387e-04 -5.84263834e-07]\n",
      "Final loss: 2.102168303052898e-10\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 0.]]\n",
      "Initial guess size: 64, MAX_NPARAMS: 10\n",
      "8\n",
      "Using BFGS directly due to large number of parameters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': array([-2.99999907e-01, -9.99999371e-01, -1.00007000e+00, -4.99981116e-01,\n",
       "        -5.00009628e+00,  4.99856920e-01, -6.87576387e-04, -5.84263834e-07]),\n",
       " 'loss': Array(2.1021683e-10, dtype=float64),\n",
       " 'sensitivities': Array([27.55610039, 27.55610039, 24.56785994, 25.55799113, 32.04142612,\n",
       "        18.58773696, -3.42138648,  1.15061881], dtype=float64)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# 读取 CSV 文件并转换为 NumPy 数组\n",
    "data0 = pd.read_csv('../func_grad/test_ood.csv')#\n",
    "tae = data0.to_numpy()\n",
    "\n",
    "# 使用 JAX 的数组操作替换 PyTorch 的操作\n",
    "state = jnp.array(tae[:, :3], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "energy = jnp.array(tae[:,-1], dtype=jnp.float64)\n",
    "\n",
    "# 将数据存储在字典中\n",
    "data = {\n",
    "    'inputs': state,\n",
    "    'outputs': energy\n",
    "}\n",
    "\n",
    "\n",
    "evaluate(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f176923c",
   "metadata": {},
   "source": [
    "## 测试单摆例子，看看能不能正常优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332702a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit, vmap, config\n",
    "from scipy.optimize import minimize\n",
    "from pyswarm import pso\n",
    "from pyswarms.utils.plotters import plot_cost_history\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import jax.random as random\n",
    "from scipy.optimize import differential_evolution\n",
    "from jaxopt import LBFGS\n",
    "import pyswarms as ps\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def evaluate(data: dict, params=initial_params) -> dict:\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    q, p = inputs[:, 0], inputs[:,1]\n",
    "    true_value = outputs\n",
    "    master_key = random.PRNGKey(0)\n",
    "    \n",
    "    @jit\n",
    "    def compute_dynamics(q, p, params):\n",
    "        def hamiltonian(q, p, params):\n",
    "            return equation(q, p, params)  \n",
    "        q_dot = jax.grad(hamiltonian, 1)(q, p, params)  \n",
    "        p_dot = -jax.grad(hamiltonian, 0)(q, p, params)  \n",
    "        return jnp.array([q_dot, p_dot])     #jnp.concatenate([q_dot, p_dot])\n",
    "\n",
    "    batch_compute_dynamics = jit(vmap(compute_dynamics, (0, 0, None)))\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        pred = batch_compute_dynamics(q, p, params)\n",
    "        return jnp.mean(jnp.square(pred - true_value))\n",
    "\n",
    "    def run_optimization(objective_fn, initial_guess, key, num_pso_runs=5, pso_iters=300, swarmsize=100, gg = False):\n",
    "        print(f\"Initial guess size: {initial_guess.size}, MAX_NPARAMS: {MAX_NPARAMS}\")\n",
    "        #n_params = initial_guess.size\n",
    "        grads   = jax.grad(equation,2)(q[1],p[1],params)           # (MAX_NPARAMS,)           \n",
    "        active  = jnp.where(jnp.abs(grads) > 0)[0]        # e.g. [0,1,2,3,4,5,6,7]\n",
    "        n_params = active.shape[0]                        # 只保留活跃参数\n",
    "        print(n_params)\n",
    "\n",
    "        \n",
    "        '''grads2   = jax.grad(loss_fn)(initial_guess)\n",
    "        print(grads2)          \n",
    "        active2  = jnp.where(jnp.abs(grads2) > 0)[0]        # e.g. [0,1,2,3,4,5,6,7]\n",
    "        print(active2)'''\n",
    "\n",
    "\n",
    "        if  gg == True:\n",
    "            # ... (BFGS only part remains the same) ...\n",
    "            print(\"Using BFGS directly due to large number of parameters.\")\n",
    "            result = minimize(objective_fn, initial_guess,\n",
    "                              method='BFGS', options={'maxiter': 500})\n",
    "            if not result.success:\n",
    "                print(f\"BFGS optimization failed: {result.message}\")\n",
    "                return initial_guess # Fallback\n",
    "            return result.x\n",
    "        else:\n",
    "            # --- Use pyswarms with JAX-controlled initial positions ---\n",
    "            @jit\n",
    "            def pso_objective_wrapper(particles_matrix):\n",
    "                return vmap(objective_fn)(particles_matrix)\n",
    "\n",
    "            min_bound_np = np.full(n_params, -100.0, dtype=np.float64)\n",
    "            max_bound_np = np.full(n_params, 100.0, dtype=np.float64)\n",
    "            bounds = (min_bound_np, max_bound_np)\n",
    "\n",
    "            options = {'c1': 1.49445, 'c2': 1.49445, 'w': 0.729}\n",
    "\n",
    "            best_pso_params = None\n",
    "            best_pso_loss = jnp.inf\n",
    "\n",
    "            current_key = key # Use the passed-in key\n",
    "\n",
    "            print(f\"Running pyswarms PSO {num_pso_runs} times with unique initial swarms...\")\n",
    "            for i in range(num_pso_runs):\n",
    "                # Split the key for this run to ensure unique randomness\n",
    "                current_key, subkey = random.split(current_key)\n",
    "\n",
    "                # Generate initial positions using JAX PRNG within bounds\n",
    "                # Use jnp arrays for min/max bounds in jax.random.uniform\n",
    "                min_bound_jnp = jnp.full(n_params, -10.0, dtype=jnp.float64)\n",
    "                max_bound_jnp = jnp.full(n_params, 10.0, dtype=jnp.float64)\n",
    "                init_pos_jax = random.uniform(subkey,\n",
    "                                              shape=(swarmsize, n_params),\n",
    "                                              dtype=jnp.float64,\n",
    "                                              minval=min_bound_jnp,\n",
    "                                              maxval=max_bound_jnp)\n",
    "                # Convert to NumPy array for pyswarms\n",
    "                init_pos_np = np.array(init_pos_jax)\n",
    "\n",
    "                print(f\"  PSO Run {i+1}/{num_pso_runs} (using JAX key split for init_pos)\")\n",
    "                optimizer = ps.single.GlobalBestPSO(n_particles=swarmsize,\n",
    "                                                     dimensions=n_params,\n",
    "                                                     options=options,\n",
    "                                                     bounds=bounds,\n",
    "                                                     # Pass the generated initial positions\n",
    "                                                     init_pos=init_pos_np) # <-- Pass init_pos here\n",
    "\n",
    "                # Perform optimization (pyswarms will use the provided init_pos)\n",
    "                current_pso_loss, current_pso_params = optimizer.optimize(\n",
    "                    pso_objective_wrapper,\n",
    "                    iters=pso_iters,\n",
    "                    verbose=False\n",
    "                )\n",
    "                current_pso_params = jnp.array(current_pso_params, dtype=jnp.float64)\n",
    "\n",
    "                print(f\"    Run {i+1} completed. Loss: {current_pso_loss}\")\n",
    "                if current_pso_loss < best_pso_loss:\n",
    "                    best_pso_loss = current_pso_loss\n",
    "                    best_pso_params = current_pso_params\n",
    "                    print(f\"    New best PSO loss found: {best_pso_loss}\")\n",
    "\n",
    "            # ... (rest of the function: handling no solution, BFGS refinement) ...\n",
    "            if best_pso_params is None:\n",
    "                 print(\"Warning: PSO did not find any valid solution after multiple runs. Using initial guess for BFGS.\")\n",
    "                 best_pso_params = initial_guess # Fallback\n",
    "\n",
    "            print(f\"\\nBest PSO loss after {num_pso_runs} runs: {best_pso_loss}\")\n",
    "            print(\"Refining best PSO result with BFGS...\")\n",
    "\n",
    "            result = minimize(objective_fn, jnp.array(best_pso_params),\n",
    "                              method='BFGS', options={'maxiter': 500})\n",
    "\n",
    "            if not result.success:\n",
    "                 print(f\"BFGS refinement failed: {result.message}\")\n",
    "                 return best_pso_params # Return PSO best\n",
    "\n",
    "            print(f\"BFGS refinement successful. Final loss: {result.fun}\")\n",
    "            return result.x\n",
    "        \n",
    "    \n",
    "    # 敏感度分析模块\n",
    "    def calculate_sensitivities(opt_params, base_loss):\n",
    "        n_active = opt_params.size\n",
    "        mask    = 1.0 - jnp.eye(n_active) \n",
    "        print(mask)\n",
    "        print(f\"opt_params: {opt_params}\")\n",
    "\n",
    "        @jit      \n",
    "        def batch_loss(params_matrix):\n",
    "            return vmap(loss_fn)(params_matrix * mask)\n",
    "\n",
    "        def sensitivity_objective(flat_params):\n",
    "            matrix_params = flat_params.reshape(n_active,n_active)\n",
    "            return jnp.sum(batch_loss(matrix_params))\n",
    "\n",
    "        try:\n",
    "            initial_flat = (opt_params * mask).flatten()\n",
    "            print(f\"Initial flat params: {initial_flat}\")\n",
    "            optimized_flat = run_optimization(sensitivity_objective, initial_flat,key=opt_key, gg = True)\n",
    "            print(f\"Optimized flat params: {optimized_flat}\")   \n",
    "            optimized_matrix = optimized_flat.reshape(n_active,n_active)\n",
    "            losses = batch_loss(optimized_matrix)\n",
    "            relative_loss = jnp.log2(losses / base_loss)\n",
    "            return relative_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Sensitivity analysis error: {str(e)}\")\n",
    "            return jnp.zeros(MAX_NPARAMS, dtype=jnp.float64)\n",
    "    \n",
    "\n",
    "    # 主流程\n",
    "    # Main execution flow\n",
    "    try:\n",
    "        opt_key, sensi_key = random.split(master_key)\n",
    "        optimized_params = run_optimization(loss_fn, params, key=opt_key, num_pso_runs=5)\n",
    "        final_loss = loss_fn(optimized_params)\n",
    "        print(f\"optimized_params: {optimized_params}\")\n",
    "        print(f\"Final loss: {final_loss}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not jnp.isfinite(final_loss):\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'params': optimized_params,\n",
    "        'loss': final_loss,\n",
    "        'sensitivities': calculate_sensitivities(optimized_params, final_loss)\n",
    "    }\n",
    "\n",
    "\n",
    "@jit\n",
    "def equation(q: jnp.array, p: jnp.array, params: jnp.array) -> jnp.array:\n",
    "\n",
    "    T = params[0] * jnp.square(p) \n",
    "    V = params[2] * jnp.cos(params[1]*q) + params[3] + params[4]*q + params[5]\n",
    "\n",
    "    return T + V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7c688b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Initial guess size: 10, MAX_NPARAMS: 10\n",
      "6\n",
      "Failed to instantiate Solution: Any cannot be instantiated\n",
      "Optimization failed: Any cannot be instantiated\n",
      "最终损失值 (MSE): None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 读取 CSV 文件并转换为 NumPy 数组\n",
    "data0 = pd.read_csv('../simple_pendulum_spring/pendulum_hamilton_data.csv')\n",
    "#data0 = pd.read_csv('./hamiltonian_spring_mass_energy_data.csv')\n",
    "tae = data0.to_numpy()\n",
    "\n",
    "# 使用 JAX 的数组操作替换 PyTorch 的操作\n",
    "state = jnp.array(tae[:, 0:2], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "true_q_ddot = jnp.array(tae[:, 2:-1], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "energy = jnp.array(tae[:, -1], dtype=jnp.float64)\n",
    "#print(true_q_ddot)\n",
    "#print(state)\n",
    "# 将数据存储在字典中\n",
    "data = {\n",
    "    'inputs': state,\n",
    "    'outputs': true_q_ddot,  # 真实的加速度\n",
    "    'energy': energy\n",
    "}\n",
    "\n",
    "\n",
    "print(initial_params)\n",
    "# 评估并优化参数\n",
    "final_loss = evaluate(data, initial_params)\n",
    "print(\"最终损失值 (MSE):\", final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087666e5",
   "metadata": {},
   "source": [
    "# 测试使用L-BFGS的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525db9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit, vmap, config\n",
    "from scipy.optimize import minimize\n",
    "from pyswarm import pso\n",
    "from pyswarms.utils.plotters import plot_cost_history\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import jax.random as random\n",
    "from jaxopt import LBFGS\n",
    "import pyswarms as ps\n",
    "import evosax\n",
    "from evosax.algorithms import DifferentialEvolution\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def evaluate(data: dict, params=initial_params) -> dict:\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    q, p = inputs[:, 0], inputs[:,1]\n",
    "    true_value = outputs\n",
    "    master_key = random.PRNGKey(0)\n",
    "    \n",
    "    @jit\n",
    "    def compute_dynamics(q, p, params):\n",
    "        def hamiltonian(q, p, params):\n",
    "            return equation(q, p, params)  \n",
    "        q_dot = jax.grad(hamiltonian, 1)(q, p, params)  \n",
    "        p_dot = -jax.grad(hamiltonian, 0)(q, p, params)  \n",
    "        return jnp.array([q_dot, p_dot])     #jnp.concatenate([q_dot, p_dot])\n",
    "\n",
    "    batch_compute_dynamics = jit(vmap(compute_dynamics, (0, 0, None)))\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        pred = batch_compute_dynamics(q, p, params)\n",
    "        return jnp.mean(jnp.square(pred - true_value))\n",
    "\n",
    "    '''def run_optimization(objective_fn, initial_guess, key, num_pso_runs=5, pso_iters=300, swarmsize=100, gg = False):\n",
    "        print(f\"Initial guess size: {initial_guess.size}, MAX_NPARAMS: {MAX_NPARAMS}\")\n",
    "\n",
    "        grads   = jax.grad(equation,2)(q[1],p[1],params)           # (MAX_NPARAMS,)           \n",
    "        active  = jnp.where(jnp.abs(grads) > 0)[0]        # e.g. [0,1,2,3,4,5,6,7]\n",
    "        n_params = active.shape[0]                        # 只保留活跃参数\n",
    "        print(n_params)\n",
    "        \n",
    "        solver = LBFGS(objective_fn, maxiter=100, tol=1e-8)\n",
    "\n",
    "        if  gg == True:\n",
    "            result =  solver.run(initial_guess)\n",
    "            return result.params\n",
    "        else:\n",
    "            # --- Use pyswarms with JAX-controlled initial positions ---\n",
    "            @jit\n",
    "            def pso_objective_wrapper(particles_matrix):\n",
    "                return vmap(objective_fn)(particles_matrix)\n",
    "\n",
    "            min_bound_np = np.full(n_params, -100.0, dtype=np.float64)\n",
    "            max_bound_np = np.full(n_params, 100.0, dtype=np.float64)\n",
    "            bounds = (min_bound_np, max_bound_np)\n",
    "\n",
    "            options = {'c1': 1.49445, 'c2': 1.49445, 'w': 0.729}\n",
    "\n",
    "            best_pso_params = None\n",
    "            best_pso_loss = jnp.inf\n",
    "\n",
    "            current_key = key # Use the passed-in key\n",
    "\n",
    "            print(f\"Running pyswarms PSO {num_pso_runs} times with unique initial swarms...\")\n",
    "            for i in range(num_pso_runs):\n",
    "                # Split the key for this run to ensure unique randomness\n",
    "                current_key, subkey = random.split(current_key)\n",
    "\n",
    "                # Generate initial positions using JAX PRNG within bounds\n",
    "                # Use jnp arrays for min/max bounds in jax.random.uniform\n",
    "                min_bound_jnp = jnp.full(n_params, -10.0, dtype=jnp.float64)\n",
    "                max_bound_jnp = jnp.full(n_params, 10.0, dtype=jnp.float64)\n",
    "                init_pos_jax = random.uniform(subkey,\n",
    "                                              shape=(swarmsize, n_params),\n",
    "                                              dtype=jnp.float64,\n",
    "                                              minval=min_bound_jnp,\n",
    "                                              maxval=max_bound_jnp)\n",
    "                # Convert to NumPy array for pyswarms\n",
    "                init_pos_np = np.array(init_pos_jax)\n",
    "\n",
    "                print(f\"  PSO Run {i+1}/{num_pso_runs} (using JAX key split for init_pos)\")\n",
    "                optimizer = ps.single.GlobalBestPSO(n_particles=swarmsize,\n",
    "                                                     dimensions=n_params,\n",
    "                                                     options=options,\n",
    "                                                     bounds=bounds,\n",
    "                                                     # Pass the generated initial positions\n",
    "                                                     init_pos=init_pos_np) # <-- Pass init_pos here\n",
    "\n",
    "                # Perform optimization (pyswarms will use the provided init_pos)\n",
    "                current_pso_loss, current_pso_params = optimizer.optimize(\n",
    "                    pso_objective_wrapper,\n",
    "                    iters=pso_iters,\n",
    "                    verbose=False\n",
    "                )\n",
    "                current_pso_params = jnp.array(current_pso_params, dtype=jnp.float64)\n",
    "\n",
    "                print(f\"    Run {i+1} completed. Loss: {current_pso_loss}\")\n",
    "                if current_pso_loss < best_pso_loss:\n",
    "                    best_pso_loss = current_pso_loss\n",
    "                    best_pso_params = current_pso_params\n",
    "                    print(f\"    New best PSO loss found: {best_pso_loss}\")\n",
    "\n",
    "            # ... (rest of the function: handling no solution, BFGS refinement) ...\n",
    "            if best_pso_params is None:\n",
    "                 print(\"Warning: PSO did not find any valid solution after multiple runs. Using initial guess for BFGS.\")\n",
    "                 best_pso_params = initial_guess # Fallback\n",
    "\n",
    "            print(f\"\\nBest PSO loss after {num_pso_runs} runs: {best_pso_loss}\")\n",
    "            print(\"Refining best PSO result with L- BFGS...\")\n",
    "\n",
    "            result =  solver.run(best_pso_params)\n",
    "            \n",
    "            return result.params'''\n",
    "    \n",
    "    # 敏感度分析模块\n",
    "    '''def calculate_sensitivities(opt_params, base_loss):\n",
    "        n_active = opt_params.size\n",
    "        mask    = 1.0 - jnp.eye(n_active) \n",
    "        print(mask)\n",
    "        print(f\"opt_params: {opt_params}\")\n",
    "\n",
    "        @jit      \n",
    "        def batch_loss(params_matrix):\n",
    "            return vmap(loss_fn)(params_matrix * mask)\n",
    "\n",
    "        def sensitivity_objective(flat_params):\n",
    "            matrix_params = flat_params.reshape(n_active,n_active)\n",
    "            return jnp.sum(batch_loss(matrix_params))\n",
    "\n",
    "        try:\n",
    "            initial_flat = (opt_params * mask).flatten()\n",
    "            print(f\"Initial flat params: {initial_flat}\")\n",
    "            optimized_flat = run_optimization(sensitivity_objective, initial_flat,key=opt_key, gg = True)\n",
    "            print(f\"Optimized flat params: {optimized_flat}\")   \n",
    "            optimized_matrix = optimized_flat.reshape(n_active,n_active)\n",
    "            losses = batch_loss(optimized_matrix)\n",
    "            relative_loss = jnp.log2(losses / base_loss)\n",
    "            return relative_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Sensitivity analysis error: {str(e)}\")\n",
    "            return jnp.zeros(MAX_NPARAMS, dtype=jnp.float64)'''\n",
    "    def calculate_sensitivities(opt_params: jnp.ndarray, base_loss: float, loss_fn) -> jnp.ndarray:\n",
    "        n_active = opt_params.size\n",
    "        masks = 1.0 - jnp.eye(n_active, dtype=opt_params.dtype)\n",
    "        @jit\n",
    "        def solve_one(mask_i):\n",
    "            init = opt_params * mask_i\n",
    "            def sub_loss(p):\n",
    "                return loss_fn(p * mask_i)\n",
    "            solver = LBFGS(fun=sub_loss, maxiter=100, tol=1e-8)\n",
    "            out    = solver.run(init)         \n",
    "            loss_i = loss_fn(out.params * mask_i)       \n",
    "            return jnp.log2(loss_i / base_loss)\n",
    "\n",
    "        sensitivities = vmap(solve_one)(masks)\n",
    "        return sensitivities\n",
    "\n",
    "\n",
    "    # 主流程\n",
    "    # Main execution flow\n",
    "    try:\n",
    "        opt_key, sensi_key = random.split(master_key)\n",
    "        optimized_params = run_optimization(loss_fn, params, key=opt_key)\n",
    "        final_loss = loss_fn(optimized_params)\n",
    "        print(f\"optimized_params: {optimized_params}\")\n",
    "        print(f\"Final loss: {final_loss}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not jnp.isfinite(final_loss):\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'params': optimized_params,\n",
    "        'loss': final_loss,\n",
    "        'sensitivities': calculate_sensitivities(optimized_params, final_loss,loss_fn)\n",
    "    }\n",
    "\n",
    "\n",
    "@jit\n",
    "def equation(q: jnp.array, p: jnp.array, params: jnp.array) -> jnp.array:\n",
    "\n",
    "    T = params[0] * jnp.square(p) \n",
    "    V = params[1] * jnp.cos(params[2]*q) + params[3] + params[4]*q + params[5]\n",
    "\n",
    "    return T + V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5046edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evosax.types import Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e779569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
