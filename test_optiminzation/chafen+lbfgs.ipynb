{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a898290",
   "metadata": {},
   "source": [
    "# 测试以下DF+LBFGS优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40079f4b",
   "metadata": {},
   "source": [
    "### 以oscillator为例子\n",
    "会出现敏感度计算失误的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f3db4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the mathematical function skeleton that represents acceleration in a damped nonlinear oscillator system with driving force, given data on time, position, and velocity. \n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit, vmap, config\n",
    "from scipy.optimize import minimize\n",
    "from pyswarm import pso\n",
    "from pyswarms.utils.plotters import plot_cost_history\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import jax.random as random\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def evaluate(data: dict, params=initial_params) -> dict:\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    t, x, v = inputs[:,0], inputs[:,1], inputs[:,2]\n",
    "    a = outputs\n",
    "    print(t.shape, x.shape, v.shape, a.shape)\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        pred = equation(t, x, v, params)\n",
    "        true_accelerations = a\n",
    "        return jnp.mean(jnp.square(pred - true_accelerations))\n",
    "        \n",
    "    def run_optimization(objective_fn, initial_guess):\n",
    "        \n",
    "        solver = LBFGS(fun=objective_fn, maxiter=100, tol=1e-8)\n",
    "        if initial_guess.size > MAX_NPARAMS:\n",
    "            result =  solver.run(initial_guess)\n",
    "            return result.params\n",
    "        else:\n",
    "            bounds = [(-10.0, 10.0)] * MAX_NPARAMS\n",
    "            # JIT 编译目标函数\n",
    "            jit_loss = jax.jit(objective_fn)\n",
    "            # NumPy wrapper 供 SciPy 调用\n",
    "            def loss_numpy(x: np.ndarray) -> float:\n",
    "                return float(jit_loss(jnp.array(x, dtype=jnp.float64)))\n",
    "            # 1) 差分进化全局搜索\n",
    "            result_de = differential_evolution(\n",
    "                loss_numpy,\n",
    "                bounds=bounds,\n",
    "                maxiter=300,\n",
    "                popsize=15,\n",
    "                tol=0.01,\n",
    "                disp=True\n",
    "            )\n",
    "            x0 = jnp.array(result_de.x, dtype=jnp.float64)\n",
    "            print(f\"[DE] 初始点 loss = {result_de.fun:.6f}\")\n",
    "            # 2) L-BFGS 局部精炼\n",
    "            sol = solver.run(x0)\n",
    "            return sol.params\n",
    "    \n",
    "    def calculate_sensitivities(loss_fn, opt_params, base_loss,  tol=1e-20):\n",
    "        # 1) 自动识别“活跃”参数索引\n",
    "        grads   = jax.grad(loss_fn)(opt_params)             # (MAX_NPARAMS,)\n",
    "        active  = jnp.where(jnp.abs(grads) > tol)[0]        # e.g. [0,1,2,3,4,5,6,7]\n",
    "        n_active = active.shape[0]\n",
    "\n",
    "        # 2) 构造掩码矩阵 M，shape = (n_active, MAX_NPARAMS)\n",
    "        #    每一行 M[i] 都是全 1，只有 active[i] 这一列为 0\n",
    "        eye      = jnp.eye(opt_params.size)\n",
    "        masks    = 1.0 - eye[active]                         # (n_active, MAX_NPARAMS)\n",
    "\n",
    "        # 3) 生成每个子问题的初始猜测：masked_params = masks * opt_params\n",
    "        init_batch = masks * opt_params                      # (n_active, MAX_NPARAMS)\n",
    "\n",
    "        # 4) 定义一个“单点 L-BFGS 解算器”，并向量化\n",
    "        solver    = LBFGS(fun=loss_fn, maxiter=100, tol=1e-8)\n",
    "\n",
    "        @jax.jit\n",
    "        def solve_one(p0):\n",
    "            sol = solver.run(p0)\n",
    "            return sol.params                              # shape = (MAX_NPARAMS,)\n",
    "\n",
    "        # 5) 并行跑 n_active 次 L-BFGS\n",
    "        sol_batch = vmap(solve_one)(init_batch)             # (n_active, MAX_NPARAMS)\n",
    "\n",
    "        # 6) 计算每个子解对应的损失\n",
    "        loss_batch = vmap(loss_fn)(sol_batch)               # (n_active,)\n",
    "\n",
    "        # 7) 输出相对增益 log2(loss_i / base_loss)\n",
    "        sens = jnp.log2(loss_batch / base_loss)       # (n_active,)\n",
    "\n",
    "        return  sens\n",
    "\n",
    "\n",
    "    # 主流程\n",
    "    # Main execution flow\n",
    "    try:\n",
    "        optimized_params = run_optimization(loss_fn, params)\n",
    "        final_loss = loss_fn(optimized_params)\n",
    "        print(f\"Final loss: {final_loss}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not jnp.isfinite(final_loss):\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'params': optimized_params,\n",
    "        'loss': final_loss,\n",
    "        'sensitivities': calculate_sensitivities(loss_fn,optimized_params, final_loss)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "@jit\n",
    "def equation(t: jnp.array, x: jnp.array, v: jnp.array, params: jnp.array) -> jnp.array:\n",
    "    \"\"\" Mathematical function for acceleration in a damped nonlinear oscillator\n",
    "\n",
    "    Args:\n",
    "        t: A jax array representing time.\n",
    "        x: A jax array representing observations of current position.\n",
    "        v: A jax array representing observations of velocity.\n",
    "        params: Jax array of numeric constants or parameters to be optimized\n",
    "\n",
    "    Return:\n",
    "        A jax array representing acceleration as the result of applying the mathematical function to the inputs.\n",
    "    \"\"\"\n",
    "    dv = params[0] * jnp.sin(params[1]*t) + params[2] * x*v + params[3] * v**3 + params[4]*x * jnp.exp(params[5]*x) + x**2 * params[6] #+params[7]#+ params[8]*jnp.cos(params[9]*t) + params[10]*x*v**2\n",
    "    return dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e636e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n",
      "[0.66038351 0.6594464  0.65849712 ... 0.24477721 0.24244884 0.24011387]\n",
      "(10000,) (10000,) (10000,) (10000,)\n",
      "differential_evolution step 1: f(x)= 0.09313057349369772\n",
      "differential_evolution step 2: f(x)= 0.05858682986039899\n",
      "differential_evolution step 3: f(x)= 0.040736511032269385\n",
      "differential_evolution step 4: f(x)= 0.040736511032269385\n",
      "differential_evolution step 5: f(x)= 0.040736511032269385\n",
      "differential_evolution step 6: f(x)= 0.040736511032269385\n",
      "differential_evolution step 7: f(x)= 0.040673256457241444\n",
      "differential_evolution step 8: f(x)= 0.03513630338988301\n",
      "differential_evolution step 9: f(x)= 0.03513630338988301\n",
      "differential_evolution step 10: f(x)= 0.032930633049247315\n",
      "differential_evolution step 11: f(x)= 0.032930633049247315\n",
      "differential_evolution step 12: f(x)= 0.032930633049247315\n",
      "differential_evolution step 13: f(x)= 0.015069052149403366\n",
      "differential_evolution step 14: f(x)= 0.015069052149403366\n",
      "differential_evolution step 15: f(x)= 0.015069052149403366\n",
      "differential_evolution step 16: f(x)= 0.015069052149403366\n",
      "differential_evolution step 17: f(x)= 0.015069052149403366\n",
      "differential_evolution step 18: f(x)= 0.015069052149403366\n",
      "differential_evolution step 19: f(x)= 0.015069052149403366\n",
      "differential_evolution step 20: f(x)= 0.011549401320328034\n",
      "differential_evolution step 21: f(x)= 0.011549401320328034\n",
      "differential_evolution step 22: f(x)= 0.011549401320328034\n",
      "differential_evolution step 23: f(x)= 0.011549401320328034\n",
      "differential_evolution step 24: f(x)= 0.011549401320328034\n",
      "differential_evolution step 25: f(x)= 0.011549401320328034\n",
      "differential_evolution step 26: f(x)= 0.007312551635830411\n",
      "differential_evolution step 27: f(x)= 0.007312551635830411\n",
      "differential_evolution step 28: f(x)= 0.007312551635830411\n",
      "differential_evolution step 29: f(x)= 0.007312551635830411\n",
      "differential_evolution step 30: f(x)= 0.007312551635830411\n",
      "differential_evolution step 31: f(x)= 0.007312551635830411\n",
      "differential_evolution step 32: f(x)= 0.007312551635830411\n",
      "differential_evolution step 33: f(x)= 0.007312551635830411\n",
      "differential_evolution step 34: f(x)= 0.007312551635830411\n",
      "differential_evolution step 35: f(x)= 0.007312551635830411\n",
      "differential_evolution step 36: f(x)= 0.005647557569008\n",
      "differential_evolution step 37: f(x)= 0.00552670712377724\n",
      "differential_evolution step 38: f(x)= 0.00552670712377724\n",
      "differential_evolution step 39: f(x)= 0.00552670712377724\n",
      "differential_evolution step 40: f(x)= 0.00552670712377724\n",
      "differential_evolution step 41: f(x)= 0.00552670712377724\n",
      "differential_evolution step 42: f(x)= 0.00552670712377724\n",
      "differential_evolution step 43: f(x)= 0.00552670712377724\n",
      "differential_evolution step 44: f(x)= 0.00552670712377724\n",
      "differential_evolution step 45: f(x)= 0.00552670712377724\n",
      "differential_evolution step 46: f(x)= 0.00552670712377724\n",
      "differential_evolution step 47: f(x)= 0.00552670712377724\n",
      "differential_evolution step 48: f(x)= 0.00552670712377724\n",
      "differential_evolution step 49: f(x)= 0.00552670712377724\n",
      "differential_evolution step 50: f(x)= 0.00552670712377724\n",
      "differential_evolution step 51: f(x)= 0.004598286002855752\n",
      "differential_evolution step 52: f(x)= 0.004598286002855752\n",
      "differential_evolution step 53: f(x)= 0.0037535984376616746\n",
      "differential_evolution step 54: f(x)= 0.0037535984376616746\n",
      "differential_evolution step 55: f(x)= 0.0037535984376616746\n",
      "differential_evolution step 56: f(x)= 0.002081704775626777\n",
      "differential_evolution step 57: f(x)= 0.002081704775626777\n",
      "differential_evolution step 58: f(x)= 0.002081704775626777\n",
      "differential_evolution step 59: f(x)= 0.0005620430483310448\n",
      "differential_evolution step 60: f(x)= 0.0005620430483310448\n",
      "differential_evolution step 61: f(x)= 0.0005341291521386063\n",
      "differential_evolution step 62: f(x)= 0.0005341291521386063\n",
      "differential_evolution step 63: f(x)= 0.0002848635403723906\n",
      "differential_evolution step 64: f(x)= 0.0002848635403723906\n",
      "differential_evolution step 65: f(x)= 0.0002848635403723906\n",
      "differential_evolution step 66: f(x)= 0.0002836674007551963\n",
      "differential_evolution step 67: f(x)= 0.00013712794203692112\n",
      "differential_evolution step 68: f(x)= 5.919973546969271e-05\n",
      "differential_evolution step 69: f(x)= 5.919973546969271e-05\n",
      "differential_evolution step 70: f(x)= 5.919973546969271e-05\n",
      "differential_evolution step 71: f(x)= 5.919973546969271e-05\n",
      "differential_evolution step 72: f(x)= 5.919973546969271e-05\n",
      "differential_evolution step 73: f(x)= 5.919973546969271e-05\n",
      "differential_evolution step 74: f(x)= 5.919973546969271e-05\n",
      "differential_evolution step 75: f(x)= 2.3327174864471463e-05\n",
      "differential_evolution step 76: f(x)= 2.3327174864471463e-05\n",
      "differential_evolution step 77: f(x)= 2.3327174864471463e-05\n",
      "differential_evolution step 78: f(x)= 2.3327174864471463e-05\n",
      "differential_evolution step 79: f(x)= 2.3327174864471463e-05\n",
      "differential_evolution step 80: f(x)= 2.3327174864471463e-05\n",
      "differential_evolution step 81: f(x)= 2.1501741802360782e-05\n",
      "differential_evolution step 82: f(x)= 7.814100138506702e-06\n",
      "differential_evolution step 83: f(x)= 6.9612871786355005e-06\n",
      "differential_evolution step 84: f(x)= 6.674573649837534e-06\n",
      "differential_evolution step 85: f(x)= 4.9251181218319e-06\n",
      "differential_evolution step 86: f(x)= 3.838150344938892e-06\n",
      "differential_evolution step 87: f(x)= 3.838150344938892e-06\n",
      "differential_evolution step 88: f(x)= 3.838150344938892e-06\n",
      "differential_evolution step 89: f(x)= 3.838150344938892e-06\n",
      "differential_evolution step 90: f(x)= 3.629399223347034e-06\n",
      "differential_evolution step 91: f(x)= 1.5980591209102211e-06\n",
      "differential_evolution step 92: f(x)= 1.5980591209102211e-06\n",
      "differential_evolution step 93: f(x)= 1.5980591209102211e-06\n",
      "differential_evolution step 94: f(x)= 1.5980591209102211e-06\n",
      "differential_evolution step 95: f(x)= 4.1876146715231394e-07\n",
      "differential_evolution step 96: f(x)= 4.1876146715231394e-07\n",
      "differential_evolution step 97: f(x)= 4.1876146715231394e-07\n",
      "differential_evolution step 98: f(x)= 3.629486107748956e-07\n",
      "differential_evolution step 99: f(x)= 3.629486107748956e-07\n",
      "differential_evolution step 100: f(x)= 3.629486107748956e-07\n",
      "differential_evolution step 101: f(x)= 3.629486107748956e-07\n",
      "differential_evolution step 102: f(x)= 3.629486107748956e-07\n",
      "differential_evolution step 103: f(x)= 3.629486107748956e-07\n",
      "differential_evolution step 104: f(x)= 3.629486107748956e-07\n",
      "differential_evolution step 105: f(x)= 3.629486107748956e-07\n",
      "differential_evolution step 106: f(x)= 3.2312053490398943e-07\n",
      "differential_evolution step 107: f(x)= 3.2312053490398943e-07\n",
      "differential_evolution step 108: f(x)= 1.4080542827988784e-07\n",
      "differential_evolution step 109: f(x)= 1.4080542827988784e-07\n",
      "differential_evolution step 110: f(x)= 1.4080542827988784e-07\n",
      "differential_evolution step 111: f(x)= 1.4080542827988784e-07\n",
      "differential_evolution step 112: f(x)= 1.4080542827988784e-07\n",
      "differential_evolution step 113: f(x)= 1.4080542827988784e-07\n",
      "differential_evolution step 114: f(x)= 1.4080542827988784e-07\n",
      "differential_evolution step 115: f(x)= 1.1233429591995384e-07\n",
      "differential_evolution step 116: f(x)= 1.058414764908726e-07\n",
      "differential_evolution step 117: f(x)= 1.058414764908726e-07\n",
      "differential_evolution step 118: f(x)= 5.7046382044325495e-08\n",
      "differential_evolution step 119: f(x)= 5.7046382044325495e-08\n",
      "differential_evolution step 120: f(x)= 5.7046382044325495e-08\n",
      "differential_evolution step 121: f(x)= 5.199031497881742e-08\n",
      "differential_evolution step 122: f(x)= 4.725640006518062e-08\n",
      "differential_evolution step 123: f(x)= 2.3842196872926647e-08\n",
      "differential_evolution step 124: f(x)= 2.3842196872926647e-08\n",
      "differential_evolution step 125: f(x)= 2.3842196872926647e-08\n",
      "differential_evolution step 126: f(x)= 2.3842196872926647e-08\n",
      "differential_evolution step 127: f(x)= 1.5566762900496926e-08\n",
      "differential_evolution step 128: f(x)= 1.5566762900496926e-08\n",
      "differential_evolution step 129: f(x)= 1.5566762900496926e-08\n",
      "differential_evolution step 130: f(x)= 1.2552675470524862e-08\n",
      "differential_evolution step 131: f(x)= 1.2552675470524862e-08\n",
      "differential_evolution step 132: f(x)= 1.2552675470524862e-08\n",
      "differential_evolution step 133: f(x)= 1.2552675470524862e-08\n",
      "differential_evolution step 134: f(x)= 1.2552675470524862e-08\n",
      "differential_evolution step 135: f(x)= 1.2552675470524862e-08\n",
      "differential_evolution step 136: f(x)= 1.2552675470524862e-08\n",
      "differential_evolution step 137: f(x)= 1.2552675470524862e-08\n",
      "differential_evolution step 138: f(x)= 7.313639692845035e-09\n",
      "differential_evolution step 139: f(x)= 7.313639692845035e-09\n",
      "differential_evolution step 140: f(x)= 5.27330107201192e-09\n",
      "differential_evolution step 141: f(x)= 3.5081032541187797e-09\n",
      "differential_evolution step 142: f(x)= 1.7259204133249519e-09\n",
      "differential_evolution step 143: f(x)= 1.7259204133249519e-09\n",
      "differential_evolution step 144: f(x)= 1.7259204133249519e-09\n",
      "differential_evolution step 145: f(x)= 1.7259204133249519e-09\n",
      "differential_evolution step 146: f(x)= 1.7259204133249519e-09\n",
      "differential_evolution step 147: f(x)= 1.7259204133249519e-09\n",
      "differential_evolution step 148: f(x)= 1.548898786598233e-09\n",
      "differential_evolution step 149: f(x)= 1.1740324767105964e-09\n",
      "differential_evolution step 150: f(x)= 1.1740324767105964e-09\n",
      "differential_evolution step 151: f(x)= 1.1740324767105964e-09\n",
      "differential_evolution step 152: f(x)= 1.1740324767105964e-09\n",
      "differential_evolution step 153: f(x)= 8.738746620647264e-10\n",
      "differential_evolution step 154: f(x)= 8.738746620647264e-10\n",
      "differential_evolution step 155: f(x)= 8.290168994484577e-10\n",
      "differential_evolution step 156: f(x)= 8.290168994484577e-10\n",
      "differential_evolution step 157: f(x)= 8.290168994484577e-10\n",
      "differential_evolution step 158: f(x)= 8.290168994484577e-10\n",
      "differential_evolution step 159: f(x)= 8.290168994484577e-10\n",
      "differential_evolution step 160: f(x)= 8.290168994484577e-10\n",
      "differential_evolution step 161: f(x)= 8.189723983076812e-10\n",
      "differential_evolution step 162: f(x)= 7.701684586972346e-10\n",
      "differential_evolution step 163: f(x)= 7.046718835385612e-10\n",
      "differential_evolution step 164: f(x)= 7.046718835385612e-10\n",
      "differential_evolution step 165: f(x)= 7.046718835385612e-10\n",
      "differential_evolution step 166: f(x)= 6.28765240552305e-10\n",
      "differential_evolution step 167: f(x)= 5.627939591685764e-10\n",
      "differential_evolution step 168: f(x)= 4.874773417222586e-10\n",
      "differential_evolution step 169: f(x)= 4.874773417222586e-10\n",
      "differential_evolution step 170: f(x)= 4.874773417222586e-10\n",
      "differential_evolution step 171: f(x)= 4.874773417222586e-10\n",
      "differential_evolution step 172: f(x)= 4.378137624647128e-10\n",
      "differential_evolution step 173: f(x)= 4.378137624647128e-10\n",
      "differential_evolution step 174: f(x)= 4.378137624647128e-10\n",
      "differential_evolution step 175: f(x)= 4.349728033954941e-10\n",
      "differential_evolution step 176: f(x)= 4.349728033954941e-10\n",
      "differential_evolution step 177: f(x)= 4.1563051625153115e-10\n",
      "differential_evolution step 178: f(x)= 4.1427839949804204e-10\n",
      "differential_evolution step 179: f(x)= 4.0931691650185755e-10\n",
      "differential_evolution step 180: f(x)= 4.0931691650185755e-10\n",
      "differential_evolution step 181: f(x)= 4.0931691650185755e-10\n",
      "differential_evolution step 182: f(x)= 4.0931691650185755e-10\n",
      "differential_evolution step 183: f(x)= 4.06030497831464e-10\n",
      "differential_evolution step 184: f(x)= 4.06030497831464e-10\n",
      "differential_evolution step 185: f(x)= 4.06030497831464e-10\n",
      "differential_evolution step 186: f(x)= 4.044743437574175e-10\n",
      "differential_evolution step 187: f(x)= 4.044743437574175e-10\n",
      "differential_evolution step 188: f(x)= 4.044743437574175e-10\n",
      "differential_evolution step 189: f(x)= 4.044743437574175e-10\n",
      "differential_evolution step 190: f(x)= 4.044743437574175e-10\n",
      "differential_evolution step 191: f(x)= 4.044743437574175e-10\n",
      "differential_evolution step 192: f(x)= 4.010508880254222e-10\n",
      "differential_evolution step 193: f(x)= 4.010508880254222e-10\n",
      "differential_evolution step 194: f(x)= 4.0098882933220234e-10\n",
      "differential_evolution step 195: f(x)= 4.0098882933220234e-10\n",
      "differential_evolution step 196: f(x)= 4.0062593030408e-10\n",
      "differential_evolution step 197: f(x)= 4.0062593030408e-10\n",
      "differential_evolution step 198: f(x)= 4.0062593030408e-10\n",
      "differential_evolution step 199: f(x)= 4.0062593030408e-10\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "[DE] 初始点 loss = 0.000000\n",
      "Final loss: 4.001403169712133e-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': Array([ 0.29999887,  1.00000036, -0.99994083, -0.50003713, -5.00015655,\n",
       "        -0.49650175, -4.98746969, -7.07820057, -4.6952015 ,  0.90605955],      dtype=float64),\n",
       " 'loss': Array(4.00140317e-10, dtype=float64),\n",
       " 'sensitivities': Array([-5.32266674e-05,  2.62484073e+01,  1.41231323e-03,  5.70507663e-04,\n",
       "         2.42037339e+01,  2.37875258e+00,  3.95585604e+00], dtype=float64)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# 读取 CSV 文件并转换为 NumPy 数组\n",
    "data0 = pd.read_csv('./train_os2.csv')#\n",
    "tae = data0.to_numpy()\n",
    "\n",
    "# 使用 JAX 的数组操作替换 PyTorch 的操作\n",
    "state = jnp.array(tae[:, :3], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "energy = jnp.array(tae[:,-1], dtype=jnp.float64)\n",
    "print(state.shape)\n",
    "print(energy)\n",
    "# 将数据存储在字典中\n",
    "data = {\n",
    "    'inputs': state,\n",
    "    'outputs': energy\n",
    "}\n",
    "\n",
    "\n",
    "evaluate(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c0751",
   "metadata": {},
   "source": [
    "# 测试原算法，优化掩膜情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afdef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the mathematical function skeleton that represents acceleration in a damped nonlinear oscillator system with driving force, given data on time, position, and velocity. \n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit, vmap, config\n",
    "from scipy.optimize import minimize\n",
    "from pyswarm import pso\n",
    "from pyswarms.utils.plotters import plot_cost_history\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import jax.random as random\n",
    "from scipy.optimize import differential_evolution\n",
    "from jaxopt import LBFGS\n",
    "import pyswarms as ps\n",
    "\n",
    "MAX_NPARAMS = 10\n",
    "initial_params = jnp.ones(MAX_NPARAMS, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def evaluate(data: dict, params=initial_params) -> dict:\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    t, x, v = inputs[:,0], inputs[:,1], inputs[:,2]\n",
    "    a = outputs\n",
    "    print(t.shape, x.shape, v.shape, a.shape)\n",
    "    master_key = random.PRNGKey(0)\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        pred = equation(t, x, v, params)\n",
    "        true_accelerations = a\n",
    "        return jnp.mean(jnp.square(pred - true_accelerations))\n",
    "\n",
    "    '''def run_optimization(objective_fn, initial_guess):\n",
    "        if initial_guess.size > MAX_NPARAMS:\n",
    "            result = minimize(objective_fn, initial_guess,\n",
    "                            method='BFGS', options={'maxiter': 100})\n",
    "            return result.x\n",
    "        else:\n",
    "            def pso_wrapper(x):\n",
    "                return objective_fn(jnp.array(x))\n",
    "            \n",
    "            lb = [-10.0]*MAX_NPARAMS\n",
    "            ub = [10.0]*MAX_NPARAMS\n",
    "            \n",
    "            pso_params, _ = pso(pso_wrapper, lb, ub, \n",
    "                            swarmsize=100, maxiter=300,omega=0.729, phip=1.49445, phig=1.4944,minstep=1e-6, debug=False)\n",
    "            \n",
    "            result = minimize(objective_fn, jnp.array(pso_params),\n",
    "                            method='BFGS', options={'maxiter': 500})\n",
    "            return result.x'''\n",
    "    def run_optimization(objective_fn, initial_guess, key, num_pso_runs=5, pso_iters=300, swarmsize=100):\n",
    "        print(f\"Initial guess size: {initial_guess.size}, MAX_NPARAMS: {MAX_NPARAMS}\")\n",
    "        n_params = initial_guess.size\n",
    "\n",
    "        if n_params > MAX_NPARAMS:\n",
    "            # ... (BFGS only part remains the same) ...\n",
    "            print(\"Using BFGS directly due to large number of parameters.\")\n",
    "            result = minimize(objective_fn, initial_guess,\n",
    "                              method='BFGS', options={'maxiter': 500})\n",
    "            if not result.success:\n",
    "                print(f\"BFGS optimization failed: {result.message}\")\n",
    "                return initial_guess # Fallback\n",
    "            return result.x\n",
    "        else:\n",
    "            # --- Use pyswarms with JAX-controlled initial positions ---\n",
    "            @jit\n",
    "            def pso_objective_wrapper(particles_matrix):\n",
    "                return vmap(objective_fn)(particles_matrix)\n",
    "\n",
    "            min_bound_np = np.full(n_params, -10.0, dtype=np.float64)\n",
    "            max_bound_np = np.full(n_params, 10.0, dtype=np.float64)\n",
    "            bounds = (min_bound_np, max_bound_np)\n",
    "\n",
    "            options = {'c1': 1.49445, 'c2': 1.49445, 'w': 0.729}\n",
    "\n",
    "            best_pso_params = None\n",
    "            best_pso_loss = jnp.inf\n",
    "\n",
    "            current_key = key # Use the passed-in key\n",
    "\n",
    "            print(f\"Running pyswarms PSO {num_pso_runs} times with unique initial swarms...\")\n",
    "            for i in range(num_pso_runs):\n",
    "                # Split the key for this run to ensure unique randomness\n",
    "                current_key, subkey = random.split(current_key)\n",
    "\n",
    "                # Generate initial positions using JAX PRNG within bounds\n",
    "                # Use jnp arrays for min/max bounds in jax.random.uniform\n",
    "                min_bound_jnp = jnp.full(n_params, -10.0, dtype=jnp.float64)\n",
    "                max_bound_jnp = jnp.full(n_params, 10.0, dtype=jnp.float64)\n",
    "                init_pos_jax = random.uniform(subkey,\n",
    "                                              shape=(swarmsize, n_params),\n",
    "                                              dtype=jnp.float64,\n",
    "                                              minval=min_bound_jnp,\n",
    "                                              maxval=max_bound_jnp)\n",
    "                # Convert to NumPy array for pyswarms\n",
    "                init_pos_np = np.array(init_pos_jax)\n",
    "\n",
    "                print(f\"  PSO Run {i+1}/{num_pso_runs} (using JAX key split for init_pos)\")\n",
    "                optimizer = ps.single.GlobalBestPSO(n_particles=swarmsize,\n",
    "                                                     dimensions=n_params,\n",
    "                                                     options=options,\n",
    "                                                     bounds=bounds,\n",
    "                                                     # Pass the generated initial positions\n",
    "                                                     init_pos=init_pos_np) # <-- Pass init_pos here\n",
    "\n",
    "                # Perform optimization (pyswarms will use the provided init_pos)\n",
    "                current_pso_loss, current_pso_params = optimizer.optimize(\n",
    "                    pso_objective_wrapper,\n",
    "                    iters=pso_iters,\n",
    "                    verbose=False\n",
    "                )\n",
    "                current_pso_params = jnp.array(current_pso_params, dtype=jnp.float64)\n",
    "\n",
    "                print(f\"    Run {i+1} completed. Loss: {current_pso_loss}\")\n",
    "                if current_pso_loss < best_pso_loss:\n",
    "                    best_pso_loss = current_pso_loss\n",
    "                    best_pso_params = current_pso_params\n",
    "                    print(f\"    New best PSO loss found: {best_pso_loss}\")\n",
    "\n",
    "            # ... (rest of the function: handling no solution, BFGS refinement) ...\n",
    "            if best_pso_params is None:\n",
    "                 print(\"Warning: PSO did not find any valid solution after multiple runs. Using initial guess for BFGS.\")\n",
    "                 best_pso_params = initial_guess # Fallback\n",
    "\n",
    "            print(f\"\\nBest PSO loss after {num_pso_runs} runs: {best_pso_loss}\")\n",
    "            print(\"Refining best PSO result with BFGS...\")\n",
    "\n",
    "            result = minimize(objective_fn, jnp.array(best_pso_params),\n",
    "                              method='BFGS', options={'maxiter': 500})\n",
    "\n",
    "            if not result.success:\n",
    "                 print(f\"BFGS refinement failed: {result.message}\")\n",
    "                 return best_pso_params # Return PSO best\n",
    "\n",
    "            print(f\"BFGS refinement successful. Final loss: {result.fun}\")\n",
    "            return result.x\n",
    "        \n",
    "    \n",
    "    # 敏感度分析模块\n",
    "    def calculate_sensitivities(opt_params, base_loss):\n",
    "        grads   = jax.grad(loss_fn)(params)           # (MAX_NPARAMS,)\n",
    "        print(grads)                                      #能够准确识别梯度            \n",
    "        active  = jnp.where(jnp.abs(grads) > 0)[0]        # e.g. [0,1,2,3,4,5,6,7]\n",
    "        opt_params = opt_params[active]                     # 只保留活跃参数\n",
    "        n_active = active.shape[0]\n",
    "        mask    = 1.0 - jnp.eye(n_active) \n",
    "        print(mask )\n",
    "\n",
    "        @jit      \n",
    "        def batch_loss(params_matrix):\n",
    "            return vmap(loss_fn)(params_matrix * mask)\n",
    "\n",
    "        def sensitivity_objective(flat_params):\n",
    "            matrix_params = flat_params.reshape(n_active,n_active)\n",
    "            return jnp.sum(batch_loss(matrix_params))\n",
    "\n",
    "        try:\n",
    "            initial_flat = (opt_params * mask).flatten()\n",
    "            optimized_flat = run_optimization(sensitivity_objective, initial_flat,key=opt_key)   \n",
    "            optimized_matrix = optimized_flat.reshape(n_active,n_active)\n",
    "            losses = batch_loss(optimized_matrix)\n",
    "            relative_loss = jnp.log2(losses / base_loss)\n",
    "            return relative_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Sensitivity analysis error: {str(e)}\")\n",
    "            return jnp.zeros(MAX_NPARAMS, dtype=jnp.float64)\n",
    "    \n",
    "\n",
    "    # 主流程\n",
    "    # Main execution flow\n",
    "    try:\n",
    "        opt_key, sensi_key = random.split(master_key)\n",
    "        optimized_params = run_optimization(loss_fn, params, key=opt_key, num_pso_runs=5)\n",
    "        final_loss = loss_fn(optimized_params)\n",
    "        print(f\"Final loss: {final_loss}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not jnp.isfinite(final_loss):\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'params': optimized_params,\n",
    "        'loss': final_loss,\n",
    "        'sensitivities': calculate_sensitivities(optimized_params, final_loss)\n",
    "    }\n",
    "\n",
    "\n",
    "@jit\n",
    "def equation(t: jnp.array, x: jnp.array, v: jnp.array, params: jnp.array) -> jnp.array:\n",
    "    \"\"\" Mathematical function for acceleration in a damped nonlinear oscillator\n",
    "\n",
    "    Args:\n",
    "        t: A jax array representing time.\n",
    "        x: A jax array representing observations of current position.\n",
    "        v: A jax array representing observations of velocity.\n",
    "        params: Jax array of numeric constants or parameters to be optimized\n",
    "\n",
    "    Return:\n",
    "        A jax array representing acceleration as the result of applying the mathematical function to the inputs.\n",
    "    \"\"\"\n",
    "    dv = params[0] * jnp.sin(params[1]*t) + params[2] * x*v + params[3] * v**3 + params[4]*x * jnp.exp(params[5]*x) + x**2 * params[6] +params[7]#+ params[8]*jnp.cos(params[9]*t) + params[10]*x*v**2\n",
    "    return dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c54da4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (10000,) (10000,) (10000,)\n",
      "Initial guess size: 10, MAX_NPARAMS: 10\n",
      "Running pyswarms PSO 5 times with unique initial swarms...\n",
      "  PSO Run 1/5 (using JAX key split for init_pos)\n",
      "    Run 1 completed. Loss: 1.8779801776864734e-08\n",
      "    New best PSO loss found: 1.8779801776864734e-08\n",
      "  PSO Run 2/5 (using JAX key split for init_pos)\n",
      "    Run 2 completed. Loss: 0.0026699999824137534\n",
      "  PSO Run 3/5 (using JAX key split for init_pos)\n",
      "    Run 3 completed. Loss: 0.008855897474132195\n",
      "  PSO Run 4/5 (using JAX key split for init_pos)\n",
      "    Run 4 completed. Loss: 0.005595129434093178\n",
      "  PSO Run 5/5 (using JAX key split for init_pos)\n",
      "    Run 5 completed. Loss: 6.63555642553374e-09\n",
      "    New best PSO loss found: 6.63555642553374e-09\n",
      "\n",
      "Best PSO loss after 5 runs: 6.63555642553374e-09\n",
      "Refining best PSO result with BFGS...\n",
      "BFGS refinement successful. Final loss: 6.5940665863206395e-09\n",
      "Final loss: 6.5940665863206395e-09\n",
      "[ 1.05987578e+00 -1.11681569e+00  1.98393131e-03  7.80097456e-04\n",
      "  1.62920544e-01  2.18002526e-02  1.85535905e-02  1.92464831e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 0.]]\n",
      "Initial guess size: 64, MAX_NPARAMS: 10\n",
      "Using BFGS directly due to large number of parameters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': array([ 2.99983654e-01,  1.00000068e+00, -9.99896612e-01, -5.01206299e-01,\n",
       "        -5.00180918e+00,  4.64317718e-01, -1.79077881e-01,  8.11270132e-06,\n",
       "        -9.25496229e-02,  9.13170731e+00]),\n",
       " 'loss': Array(6.59406659e-09, dtype=float64),\n",
       " 'sensitivities': Array([22.20403872, 22.20403872, 15.03910487, 12.47087436, 24.55283238,\n",
       "         5.61911479, -8.63815805, -0.11304308], dtype=float64)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# 读取 CSV 文件并转换为 NumPy 数组\n",
    "data0 = pd.read_csv('./train_os2.csv')#\n",
    "tae = data0.to_numpy()\n",
    "\n",
    "# 使用 JAX 的数组操作替换 PyTorch 的操作\n",
    "state = jnp.array(tae[:, :3], dtype=jnp.float64)  # 转换为 JAX 数组\n",
    "energy = jnp.array(tae[:,-1], dtype=jnp.float64)\n",
    "\n",
    "# 将数据存储在字典中\n",
    "data = {\n",
    "    'inputs': state,\n",
    "    'outputs': energy\n",
    "}\n",
    "\n",
    "\n",
    "evaluate(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332702a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c688b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
